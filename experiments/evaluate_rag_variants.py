"""
RAG ë³€í˜• ë¹„êµ ê²°ê³¼ì— ëŒ€í•œ RAGAS í‰ê°€ (í”¼ë“œë°± ë°˜ì˜)

RAGAS 3ì¶• í‰ê°€:
1. Faithfulness (ê·¼ê±° ì¶©ì‹¤ë„)
2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)
3. Context Precision (ë¬¸ë§¥ ì •í™•ë„)

Usage:
    python experiments/evaluate_rag_variants.py runs/rag_variants_comparison/comparison_P001_20251216_143022.json
"""
import json
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from experiments.evaluation.ragas_metrics import calculate_ragas_metrics_full


def load_comparison_results(json_path: Path) -> Dict[str, Any]:
    """ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def evaluate_variant(variant_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ë‹¨ì¼ RAG ë³€í˜•ì— ëŒ€í•œ RAGAS í‰ê°€
    
    Args:
        variant_data: ë³€í˜• ì‹¤í—˜ ê²°ê³¼ ë°ì´í„°
    
    Returns:
        í„´ë³„ RAGAS ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    """
    variant_name = variant_data['variant_name']
    turns = variant_data['turns']
    
    print(f"\n{'='*80}")
    print(f"[RAGAS í‰ê°€] {variant_name}")
    print(f"{'='*80}")
    
    turn_scores = []
    
    for turn_data in turns:
        if 'error' in turn_data:
            print(f"  Turn {turn_data['turn_id']}: ìŠ¤í‚µ (ì˜¤ë¥˜ ë°œìƒ)")
            continue
        
        turn_id = turn_data['turn_id']
        question = turn_data['user_query']
        answer = turn_data['answer']
        contexts = turn_data['contexts']
        
        # ë¹ˆ contexts ì²˜ë¦¬
        if not contexts or all(not ctx.strip() for ctx in contexts):
            contexts = ["No context retrieved"]
        
        print(f"  Turn {turn_id}: {question[:50]}...")
        
        # RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° (LLM as a Judge)
        ragas_scores = calculate_ragas_metrics_full(
            question=question,
            answer=answer,
            contexts=contexts,
            ground_truth=None  # ground_truth ì—†ìœ¼ë©´ context_recallì€ ê³„ì‚° ì•ˆ ë¨
        )
        
        if ragas_scores:
            turn_score = {
                'turn_id': turn_id,
                'user_query': question,
                'faithfulness': ragas_scores.get('faithfulness', 0.0),
                'answer_relevancy': ragas_scores.get('answer_relevancy', 0.0),
                'context_precision': ragas_scores.get('context_precision', 0.0),
                'context_relevancy': ragas_scores.get('context_relevancy', 0.0),
                'quality_score': turn_data.get('quality_score', 0.0),
                'iteration_count': turn_data.get('iteration_count', 0),
                'num_docs': turn_data.get('num_docs', 0),
                'elapsed_sec': turn_data.get('elapsed_sec', 0.0),
            }
            
            turn_scores.append(turn_score)
            
            print(f"    âœ“ Faithfulness={turn_score['faithfulness']:.3f}, "
                  f"Relevancy={turn_score['answer_relevancy']:.3f}, "
                  f"Precision={turn_score['context_precision']:.3f}")
        else:
            print(f"    âœ— RAGAS í‰ê°€ ì‹¤íŒ¨")
    
    return turn_scores


def calculate_statistics(scores: List[Dict[str, Any]]) -> Dict[str, Any]:
    """í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨)"""
    if not scores:
        return {}
    
    df = pd.DataFrame(scores)
    
    metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_relevancy']
    
    stats = {
        'count': len(scores),
    }
    
    for metric in metrics:
        if metric in df.columns:
            stats[f'{metric}_mean'] = float(df[metric].mean())
            stats[f'{metric}_std'] = float(df[metric].std())
            stats[f'{metric}_min'] = float(df[metric].min())
            stats[f'{metric}_max'] = float(df[metric].max())
    
    return stats


def compare_variants_statistical(
    variant_scores: Dict[str, List[Dict[str, Any]]]
) -> Dict[str, Any]:
    """ë³€í˜• ê°„ í†µê³„ì  ë¹„êµ (t-test)"""
    from scipy import stats as scipy_stats
    
    comparison_results = {}
    
    variants = list(variant_scores.keys())
    metrics = ['faithfulness', 'answer_relevancy', 'context_precision']
    
    # ìŒë³„ ë¹„êµ
    for i, variant_a in enumerate(variants):
        for variant_b in variants[i+1:]:
            scores_a = variant_scores[variant_a]
            scores_b = variant_scores[variant_b]
            
            if not scores_a or not scores_b:
                continue
            
            df_a = pd.DataFrame(scores_a)
            df_b = pd.DataFrame(scores_b)
            
            pair_key = f"{variant_a}_vs_{variant_b}"
            comparison_results[pair_key] = {}
            
            for metric in metrics:
                if metric not in df_a.columns or metric not in df_b.columns:
                    continue
                
                values_a = df_a[metric].dropna()
                values_b = df_b[metric].dropna()
                
                if len(values_a) < 2 or len(values_b) < 2:
                    continue
                
                # t-test (ì–‘ì¸¡ ê²€ì •)
                t_stat, p_value = scipy_stats.ttest_ind(values_a, values_b)
                
                # íš¨ê³¼ í¬ê¸° (Cohen's d)
                mean_a = values_a.mean()
                mean_b = values_b.mean()
                std_pooled = ((values_a.std() ** 2 + values_b.std() ** 2) / 2) ** 0.5
                cohens_d = (mean_a - mean_b) / std_pooled if std_pooled > 0 else 0
                
                comparison_results[pair_key][metric] = {
                    'mean_a': float(mean_a),
                    'mean_b': float(mean_b),
                    'diff': float(mean_a - mean_b),
                    't_statistic': float(t_stat),
                    'p_value': float(p_value),
                    'cohens_d': float(cohens_d),
                    'significant': p_value < 0.05,
                }
    
    return comparison_results


def main():
    parser = argparse.ArgumentParser(description="RAG ë³€í˜• ë¹„êµ ê²°ê³¼ RAGAS í‰ê°€")
    parser.add_argument('comparison_file', type=str,
                        help='ë¹„êµ ì‹¤í—˜ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    comparison_file = Path(args.comparison_file)
    
    if not comparison_file.exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {comparison_file}")
        return
    
    print("=" * 80)
    print("RAG ë³€í˜• RAGAS í‰ê°€ (LLM as a Judge)")
    print("=" * 80)
    print(f"ì…ë ¥ íŒŒì¼: {comparison_file}")
    print("=" * 80)
    
    # ê²°ê³¼ ë¡œë“œ
    data = load_comparison_results(comparison_file)
    
    patient_id = data.get('patient_id', 'unknown')
    patient_name = data.get('patient_name', '')
    results = data.get('results', {})
    
    print(f"í™˜ì: {patient_name} ({patient_id})")
    print(f"ë³€í˜• ìˆ˜: {len(results)}")
    
    # ê° ë³€í˜• í‰ê°€
    all_scores = {}
    
    for variant_name, variant_data in results.items():
        scores = evaluate_variant(variant_data)
        all_scores[variant_name] = scores
    
    # ============================================================
    # í†µê³„ ê³„ì‚°
    # ============================================================
    print(f"\n\n{'='*80}")
    print("RAGAS ë©”íŠ¸ë¦­ í†µê³„")
    print(f"{'='*80}")
    
    all_stats = {}
    
    for variant_name, scores in all_scores.items():
        stats = calculate_statistics(scores)
        all_stats[variant_name] = stats
        
        if stats:
            print(f"\n[{variant_name}]")
            print(f"  ìƒ˜í”Œ ìˆ˜: {stats['count']}")
            print(f"  Faithfulness:      {stats.get('faithfulness_mean', 0):.3f} Â± {stats.get('faithfulness_std', 0):.3f}")
            print(f"  Answer Relevancy:  {stats.get('answer_relevancy_mean', 0):.3f} Â± {stats.get('answer_relevancy_std', 0):.3f}")
            print(f"  Context Precision: {stats.get('context_precision_mean', 0):.3f} Â± {stats.get('context_precision_std', 0):.3f}")
    
    # ============================================================
    # í†µê³„ì  ë¹„êµ (t-test)
    # ============================================================
    print(f"\n\n{'='*80}")
    print("í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)")
    print(f"{'='*80}")
    
    comparison_stats = compare_variants_statistical(all_scores)
    
    for pair_key, pair_stats in comparison_stats.items():
        print(f"\n[{pair_key}]")
        
        for metric, metric_stats in pair_stats.items():
            sig_marker = "***" if metric_stats['significant'] else ""
            
            print(f"  {metric}:")
            print(f"    Î” = {metric_stats['diff']:+.3f} "
                  f"(p={metric_stats['p_value']:.4f}, "
                  f"d={metric_stats['cohens_d']:.2f}) {sig_marker}")
    
    # ============================================================
    # ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    # ============================================================
    print(f"\n\n{'='*80}")
    print("RAGAS ë©”íŠ¸ë¦­ ë¹„êµ í…Œì´ë¸”")
    print(f"{'='*80}")
    
    # í—¤ë”
    print(f"{'ë³€í˜•':<20} {'Faithfulness':>14} {'Relevancy':>14} {'Precision':>14}")
    print(f"{'-'*80}")
    
    # ê° ë³€í˜• í†µê³„
    for variant_name in results.keys():
        if variant_name not in all_stats:
            continue
        
        stats = all_stats[variant_name]
        
        if stats:
            print(f"{variant_name:<20} "
                  f"{stats.get('faithfulness_mean', 0):>8.3f}Â±{stats.get('faithfulness_std', 0):.3f} "
                  f"{stats.get('answer_relevancy_mean', 0):>8.3f}Â±{stats.get('answer_relevancy_std', 0):.3f} "
                  f"{stats.get('context_precision_mean', 0):>8.3f}Â±{stats.get('context_precision_std', 0):.3f}")
    
    print(f"{'='*80}")
    
    # ============================================================
    # ê²°ê³¼ ì €ì¥
    # ============================================================
    output_dir = comparison_file.parent / "ragas_evaluation"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"ragas_{patient_id}_{timestamp}.json"
    
    output_data = {
        'experiment_type': 'ragas_evaluation',
        'timestamp': datetime.now().isoformat(),
        'source_file': str(comparison_file),
        'patient_id': patient_id,
        'patient_name': patient_name,
        'variant_scores': all_scores,
        'variant_statistics': all_stats,
        'statistical_comparison': comparison_stats,
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… RAGAS í‰ê°€ ê²°ê³¼ ì €ì¥: {output_file}")
    
    # CSV ì €ì¥ (ë…¼ë¬¸/ë³´ê³ ì„œìš©)
    csv_file = output_dir / f"ragas_summary_{patient_id}_{timestamp}.csv"
    
    with open(csv_file, 'w', encoding='utf-8-sig') as f:
        f.write("Variant,Faithfulness_Mean,Faithfulness_Std,Relevancy_Mean,Relevancy_Std,Precision_Mean,Precision_Std\n")
        
        for variant_name in results.keys():
            if variant_name not in all_stats:
                continue
            
            stats = all_stats[variant_name]
            
            if stats:
                f.write(f"{variant_name},"
                       f"{stats.get('faithfulness_mean', 0):.4f},"
                       f"{stats.get('faithfulness_std', 0):.4f},"
                       f"{stats.get('answer_relevancy_mean', 0):.4f},"
                       f"{stats.get('answer_relevancy_std', 0):.4f},"
                       f"{stats.get('context_precision_mean', 0):.4f},"
                       f"{stats.get('context_precision_std', 0):.4f}\n")
    
    print(f"   CSV ìš”ì•½: {csv_file}")
    
    print("\ní‰ê°€ ì™„ë£Œ! ğŸ‰")
    print(f"ì´ {len(all_scores)}ê°œ ë³€í˜• í‰ê°€ ì™„ë£Œ")


if __name__ == "__main__":
    main()

