# ì¬ì„¤ê³„ ì „ëµ Quick Start ê°€ì´ë“œ

**ëª©ì **: ì˜¤ëŠ˜ ë‹¹ì¥ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì‹¤ì „ ê°€ì´ë“œ  
**ì†Œìš” ì‹œê°„**: 1-2ì£¼ë¡œ MVP êµ¬ì¶• ê°€ëŠ¥

---

## ğŸ¯ í•µì‹¬ ì§ˆë¬¸ê³¼ ë‹µë³€

### Q1: ì§€ê¸ˆ ë‹¹ì¥ ë¬´ì—‡ë¶€í„° ì‹œì‘í•´ì•¼ í•˜ë‚˜ìš”?

**A: ë°ì´í„° ë ˆì´ì–´ë¶€í„° ì‹œì‘í•˜ì„¸ìš” (Bottom-Up ì ‘ê·¼)**

```
Day 1-2: í˜„ì¬ ë°ì´í„° ë¶„ì„
  â”œâ”€ ì²­í¬ í¬ê¸° ë¶„í¬ í™•ì¸
  â”œâ”€ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ê°€ëŠ¥ì„± ê²€í† 
  â””â”€ Retrieval ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •

Day 3-5: ì²­í‚¹ ì „ëµ ê°œì„ 
  â”œâ”€ 280 í† í°ìœ¼ë¡œ ì¬ì²­í‚¹
  â”œâ”€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
  â””â”€ ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶•

Day 6-7: ì„±ëŠ¥ ì¸¡ì •
  â”œâ”€ Recall@5, MRR ì¸¡ì •
  â””â”€ ê°œì„  íš¨ê³¼ í™•ì¸ (ëª©í‘œ: +10-20%p)
```

### Q2: í˜„ì¬ ì‹œìŠ¤í…œì„ ì–´ë–»ê²Œ ê°œì„ í• ê¹Œìš”?

**A: 3ë‹¨ê³„ ì ì§„ì  ê°œì„  ì „ëµ**

```
Phase 1 (1ì£¼): ë°ì´í„° ìµœì í™”
  â†’ ì²­í‚¹ + ì„ë² ë”© ê°œì„ 
  â†’ ì˜ˆìƒ íš¨ê³¼: Recall +15%p

Phase 2 (1ì£¼): Retrieval ê°•í™”
  â†’ Hybrid + Dual Index
  â†’ ì˜ˆìƒ íš¨ê³¼: MRR +10%p

Phase 3 (1ì£¼): Self-Refine ì¶”ê°€
  â†’ Quality Check + Rewrite
  â†’ ì˜ˆìƒ íš¨ê³¼: Judge Score +1.5ì 
```

### Q3: Ablation ì—°êµ¬ëŠ” ì–¸ì œ ì‹œì‘í•˜ë‚˜ìš”?

**A: ê° Phase ì™„ë£Œ í›„ ì¦‰ì‹œ ì¸¡ì •**

```
Phase 1 ì™„ë£Œ â†’ Data Ablation (D1-D5)
Phase 2 ì™„ë£Œ â†’ Retrieval Ablation (R1-R4)
Phase 3 ì™„ë£Œ â†’ Generation Ablation (G1-G4)
```

---

## ğŸ“‹ 1ì£¼ì°¨ ì‹¤í–‰ ê³„íš

### Day 1: í˜„ì¬ ìƒíƒœ ë¶„ì„

#### 1.1 ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```python
# scripts/analyze_current_data.py
"""í˜„ì¬ ì²­í‚¹ ìƒíƒœ ë¶„ì„"""

import json
from pathlib import Path
from collections import Counter
import matplotlib.pyplot as plt

def analyze_chunks(meta_path: str):
    """ì²­í¬ ë©”íƒ€ë°ì´í„° ë¶„ì„"""
    chunks = []
    with open(meta_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))
    
    # í† í° ìˆ˜ ë¶„í¬
    token_counts = []
    for chunk in chunks:
        text = chunk.get('text', '')
        token_count = len(text.split()) * 1.3  # ëŒ€ëµì  í† í° ìˆ˜
        token_counts.append(token_count)
    
    # í†µê³„
    print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"í‰ê·  í† í° ìˆ˜: {sum(token_counts) / len(token_counts):.1f}")
    print(f"ì¤‘ì•™ê°’: {sorted(token_counts)[len(token_counts)//2]:.1f}")
    print(f"ìµœì†Œ/ìµœëŒ€: {min(token_counts):.1f} / {max(token_counts):.1f}")
    
    # ë¶„í¬ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    plt.hist(token_counts, bins=50, edgecolor='black')
    plt.xlabel('Token Count')
    plt.ylabel('Frequency')
    plt.title('Current Chunk Size Distribution')
    plt.axvline(x=900, color='r', linestyle='--', label='Current target (900)')
    plt.axvline(x=280, color='g', linestyle='--', label='Proposed target (280)')
    plt.legend()
    plt.savefig('chunk_distribution.png', dpi=300)
    print("\nâœ… ë¶„í¬ ì°¨íŠ¸ ì €ì¥: chunk_distribution.png")
    
    return token_counts

if __name__ == '__main__':
    # í˜„ì¬ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    meta_path = 'data/index/train_qa/train_questions.meta.jsonl'
    
    if Path(meta_path).exists():
        analyze_chunks(meta_path)
    else:
        print(f"âš ï¸  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {meta_path}")
        print("ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
```

**ì‹¤í–‰**:
```bash
python scripts/analyze_current_data.py
```

#### 1.2 ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •

```python
# scripts/measure_baseline.py
"""í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •"""

from agent.graph import run_agent
from evaluation.retrieval_metrics import recall_at_k, mrr
import json

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (val setì—ì„œ ìƒ˜í”Œë§)
TEST_CASES = [
    {
        'query': 'What are the side effects of metformin?',
        'relevant_docs': ['doc_123', 'doc_456', 'doc_789']  # Ground truth
    },
    # ... ìµœì†Œ 20-30ê°œ
]

def measure_baseline():
    """ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •"""
    results = []
    
    for case in TEST_CASES:
        # í˜„ì¬ ì‹œìŠ¤í…œìœ¼ë¡œ ê²€ìƒ‰
        state = run_agent(
            user_text=case['query'],
            mode='ai_agent',
            return_state=True
        )
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ID ì¶”ì¶œ
        retrieved_ids = [doc['doc_id'] for doc in state['retrieved_docs']]
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'query': case['query'],
            'recall@5': recall_at_k(retrieved_ids, case['relevant_docs'], k=5),
            'recall@10': recall_at_k(retrieved_ids, case['relevant_docs'], k=10),
            'mrr': mrr(retrieved_ids, case['relevant_docs']),
            'quality_score': state.get('quality_score', 0.0)
        }
        
        results.append(metrics)
        print(f"âœ“ {case['query'][:50]}... â†’ R@5={metrics['recall@5']:.2f}")
    
    # ì§‘ê³„
    avg_metrics = {
        'recall@5': sum(r['recall@5'] for r in results) / len(results),
        'recall@10': sum(r['recall@10'] for r in results) / len(results),
        'mrr': sum(r['mrr'] for r in results) / len(results),
        'quality_score': sum(r['quality_score'] for r in results) / len(results),
    }
    
    print(f"\n{'='*60}")
    print("ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (í˜„ì¬ ì‹œìŠ¤í…œ)")
    print(f"{'='*60}")
    for metric, value in avg_metrics.items():
        print(f"{metric:20s}: {value:.4f}")
    
    # ì €ì¥
    with open('baseline_results.json', 'w') as f:
        json.dump({
            'avg_metrics': avg_metrics,
            'detailed_results': results
        }, f, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: baseline_results.json")
    
    return avg_metrics

if __name__ == '__main__':
    measure_baseline()
```

**ì‹¤í–‰**:
```bash
python scripts/measure_baseline.py
```

**ì˜ˆìƒ ê²°ê³¼**:
```
ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (í˜„ì¬ ì‹œìŠ¤í…œ)
============================================================
recall@5            : 0.6500
recall@10           : 0.7800
mrr                 : 0.5200
quality_score       : 7.2000
```

### Day 2-3: ì²­í‚¹ ì „ëµ ê°œì„ 

#### 2.1 ìƒˆë¡œìš´ ì²­í‚¹ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

```bash
# 1. êµ¬í˜„ íŒŒì¼ ë³µì‚¬ (IMPLEMENTATION_EXAMPLES.mdì—ì„œ)
mkdir -p data_pipeline
# data_pipeline/chunker.py ì‘ì„±

# 2. ë¬¸ì„œ ì¬ì²­í‚¹
python scripts/rechunk_corpus.py \
  --input data/corpus/train_source \
  --output data/corpus_v2/train_source \
  --strategy type_aware \
  --target_size 280
```

```python
# scripts/rechunk_corpus.py
"""ì½”í¼ìŠ¤ ì¬ì²­í‚¹ ìŠ¤í¬ë¦½íŠ¸"""

import argparse
from pathlib import Path
import json
from data_pipeline.chunker import TypeAwareChunker

def rechunk_corpus(input_dir: str, output_dir: str, strategy: str, target_size: int):
    """ì½”í¼ìŠ¤ ì¬ì²­í‚¹"""
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì›ë³¸ ë¬¸ì„œ ë¡œë“œ
    documents = []
    for jsonl_file in input_path.glob('**/*.jsonl'):
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                documents.append(json.loads(line))
    
    print(f"[Rechunking] ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ì²­í‚¹
    if strategy == 'type_aware':
        chunker = TypeAwareChunker()
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    chunks = chunker.chunk_corpus(documents)
    
    # ì €ì¥
    output_file = output_path / 'chunks.jsonl'
    with open(output_file, 'w', encoding='utf-8') as f:
        for chunk in chunks:
            chunk_data = {
                'chunk_id': chunk.chunk_id,
                'doc_id': chunk.doc_id,
                'doc_type': chunk.doc_type,
                'text': chunk.text,
                'span_start': chunk.span_start,
                'span_end': chunk.span_end,
                'token_count': chunk.token_count,
                'metadata': chunk.metadata
            }
            f.write(json.dumps(chunk_data, ensure_ascii=False) + '\n')
    
    print(f"âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', required=True)
    parser.add_argument('--output', required=True)
    parser.add_argument('--strategy', default='type_aware')
    parser.add_argument('--target_size', type=int, default=280)
    
    args = parser.parse_args()
    rechunk_corpus(args.input, args.output, args.strategy, args.target_size)
```

#### 2.2 ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶•

```bash
python scripts/build_dual_index.py \
  --chunks data/corpus_v2/train_source/chunks.jsonl \
  --output data/index_v2/train_source \
  --embedding_model text-embedding-3-large
```

```python
# scripts/build_dual_index.py
"""ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸"""

import argparse
import json
from pathlib import Path
from data_pipeline.indexer import DualIndexBuilder
from data_pipeline.chunker import Chunk

def build_dual_index(chunks_path: str, output_dir: str, embedding_model: str):
    """ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶•"""
    # ì²­í¬ ë¡œë“œ
    chunks = []
    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            chunk = Chunk(
                text=data['text'],
                chunk_id=data['chunk_id'],
                doc_id=data['doc_id'],
                doc_type=data['doc_type'],
                span_start=data['span_start'],
                span_end=data['span_end'],
                metadata=data['metadata']
            )
            chunks.append(chunk)
    
    print(f"[Build Index] ë¡œë“œëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # ì¸ë±ìŠ¤ ìƒì„±
    builder = DualIndexBuilder(embedding_model=embedding_model)
    builder.build(chunks, output_dir=output_dir)
    
    print(f"âœ… ë“€ì–¼ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {output_dir}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--chunks', required=True)
    parser.add_argument('--output', required=True)
    parser.add_argument('--embedding_model', default='text-embedding-3-large')
    
    args = parser.parse_args()
    build_dual_index(args.chunks, args.output, args.embedding_model)
```

### Day 4-5: ì„±ëŠ¥ ì¬ì¸¡ì •

```python
# scripts/compare_performance.py
"""ê°œì„  ì „í›„ ì„±ëŠ¥ ë¹„êµ"""

import json
from scripts.measure_baseline import measure_baseline, TEST_CASES
from retrieval.dual_retriever import DualIndexRetriever

def measure_improved():
    """ê°œì„ ëœ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •"""
    # ìƒˆë¡œìš´ retriever ì‚¬ìš©
    retriever = DualIndexRetriever(
        index_dir='data/index_v2/train_source',
        embedding_model='text-embedding-3-large'
    )
    
    results = []
    
    for case in TEST_CASES:
        # ê²€ìƒ‰
        docs = retriever.search(
            query=case['query'],
            k_fine=12,
            k_coarse=5,
            route='both'
        )
        
        retrieved_ids = [doc['doc_id'] for doc in docs]
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        from evaluation.retrieval_metrics import recall_at_k, mrr
        metrics = {
            'query': case['query'],
            'recall@5': recall_at_k(retrieved_ids, case['relevant_docs'], k=5),
            'recall@10': recall_at_k(retrieved_ids, case['relevant_docs'], k=10),
            'mrr': mrr(retrieved_ids, case['relevant_docs']),
        }
        
        results.append(metrics)
    
    # ì§‘ê³„
    avg_metrics = {
        'recall@5': sum(r['recall@5'] for r in results) / len(results),
        'recall@10': sum(r['recall@10'] for r in results) / len(results),
        'mrr': sum(r['mrr'] for r in results) / len(results),
    }
    
    return avg_metrics

def compare():
    """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
    # ë² ì´ìŠ¤ë¼ì¸ ë¡œë“œ
    with open('baseline_results.json', 'r') as f:
        baseline = json.load(f)['avg_metrics']
    
    # ê°œì„  ë²„ì „ ì¸¡ì •
    improved = measure_improved()
    
    # ë¹„êµ ì¶œë ¥
    print(f"\n{'='*70}")
    print("ì„±ëŠ¥ ë¹„êµ: Baseline vs Improved")
    print(f"{'='*70}")
    print(f"{'Metric':<20} {'Baseline':>12} {'Improved':>12} {'Î”':>12} {'Î”%':>10}")
    print(f"{'-'*70}")
    
    for metric in ['recall@5', 'recall@10', 'mrr']:
        base_val = baseline[metric]
        impr_val = improved[metric]
        delta = impr_val - base_val
        delta_pct = (delta / base_val) * 100 if base_val > 0 else 0
        
        print(f"{metric:<20} {base_val:>12.4f} {impr_val:>12.4f} "
              f"{delta:>+12.4f} {delta_pct:>+9.1f}%")
    
    # ì €ì¥
    with open('comparison_results.json', 'w') as f:
        json.dump({
            'baseline': baseline,
            'improved': improved,
            'delta': {k: improved[k] - baseline[k] for k in improved.keys()}
        }, f, indent=2)
    
    print(f"\nâœ… ë¹„êµ ê²°ê³¼ ì €ì¥: comparison_results.json")

if __name__ == '__main__':
    compare()
```

**ì‹¤í–‰**:
```bash
python scripts/compare_performance.py
```

**ì˜ˆìƒ ê²°ê³¼**:
```
ì„±ëŠ¥ ë¹„êµ: Baseline vs Improved
======================================================================
Metric               Baseline     Improved           Î”        Î”%
----------------------------------------------------------------------
recall@5               0.6500       0.7800      +0.1300    +20.0%
recall@10              0.7800       0.8500      +0.0700     +9.0%
mrr                    0.5200       0.6500      +0.1300    +25.0%
```

### Day 6-7: Data Ablation ì‹¤í—˜

```python
# experiments/data_ablation.py
"""ë°ì´í„° ë ˆì´ì–´ ablation ì‹¤í—˜"""

from config.feature_flags import FeatureFlags
from scripts.measure_baseline import TEST_CASES
from evaluation.retrieval_metrics import recall_at_k, mrr
import json

# ì‹¤í—˜ ì„¤ì •
EXPERIMENTS = {
    'D1_baseline': {
        'chunk_size': 900,
        'chunking_strategy': 'uniform',
        'metadata_richness': 'minimal',
        'index_strategy': 'single'
    },
    'D2_fine_chunking': {
        'chunk_size': 280,
        'chunking_strategy': 'uniform',
        'metadata_richness': 'minimal',
        'index_strategy': 'single'
    },
    'D3_metadata': {
        'chunk_size': 280,
        'chunking_strategy': 'type_aware',
        'metadata_richness': 'full',
        'index_strategy': 'single'
    },
    'D4_dual_index': {
        'chunk_size': 280,
        'chunking_strategy': 'type_aware',
        'metadata_richness': 'full',
        'index_strategy': 'dual'
    },
}

def run_experiment(exp_id: str, config: dict):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print(f"ì‹¤í—˜: {exp_id}")
    print(f"{'='*60}")
    
    # TODO: ê° ì„¤ì •ì— ë§ëŠ” retriever ë¡œë“œ
    # retriever = load_retriever(config)
    
    # ì„±ëŠ¥ ì¸¡ì •
    # results = measure_performance(retriever, TEST_CASES)
    
    # ì„ì‹œ: ì‹œë®¬ë ˆì´ì…˜
    results = {
        'recall@5': 0.65 + (0.15 * (list(EXPERIMENTS.keys()).index(exp_id) / len(EXPERIMENTS))),
        'mrr': 0.52 + (0.13 * (list(EXPERIMENTS.keys()).index(exp_id) / len(EXPERIMENTS)))
    }
    
    print(f"  Recall@5: {results['recall@5']:.4f}")
    print(f"  MRR:      {results['mrr']:.4f}")
    
    return results

def run_all_experiments():
    """ì „ì²´ ablation ì‹¤í—˜ ì‹¤í–‰"""
    all_results = {}
    
    for exp_id, config in EXPERIMENTS.items():
        results = run_experiment(exp_id, config)
        all_results[exp_id] = results
    
    # ë¹„êµ í…Œì´ë¸”
    print(f"\n{'='*70}")
    print("Data Ablation ê²°ê³¼")
    print(f"{'='*70}")
    print(f"{'Experiment':<25} {'Recall@5':>12} {'MRR':>12} {'Î” R@5':>12}")
    print(f"{'-'*70}")
    
    baseline_recall = all_results['D1_baseline']['recall@5']
    
    for exp_id, results in all_results.items():
        delta = results['recall@5'] - baseline_recall
        print(f"{exp_id:<25} {results['recall@5']:>12.4f} {results['mrr']:>12.4f} "
              f"{delta:>+12.4f}")
    
    # ì €ì¥
    with open('data_ablation_results.json', 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: data_ablation_results.json")

if __name__ == '__main__':
    run_all_experiments()
```

**ì‹¤í–‰**:
```bash
python experiments/data_ablation.py
```

---

## ğŸ“Š 2ì£¼ì°¨ ì‹¤í–‰ ê³„íš

### Day 8-10: Self-Refine êµ¬í˜„

```python
# agent/nodes/refine_v2.py
"""ê°œì„ ëœ Self-Refine ë…¸ë“œ"""

from agent.state import AgentState
from core.llm_client import LLMClient

def refine_node(state: AgentState) -> AgentState:
    """
    Self-Refine ë…¸ë“œ
    
    1. í˜„ì¬ ë‹µë³€ í’ˆì§ˆ í‰ê°€
    2. í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì¿¼ë¦¬ ì¬ì‘ì„±
    3. ì¬ê²€ìƒ‰ ë° ì¬ìƒì„±
    """
    llm_client = LLMClient()
    
    # í’ˆì§ˆ í‰ê°€
    quality_score = evaluate_quality(state['answer'], state['retrieved_docs'], llm_client)
    state['quality_score'] = quality_score
    
    # ì„ê³„ê°’ í™•ì¸
    threshold = state.get('quality_threshold', 0.5)
    max_iterations = state.get('max_refine_iterations', 2)
    current_iteration = state.get('iteration_count', 0)
    
    if quality_score < threshold and current_iteration < max_iterations:
        # ì¬ì‘ì„± í•„ìš”
        state['needs_retrieval'] = True
        state['iteration_count'] = current_iteration + 1
        
        # ì¿¼ë¦¬ ì¬ì‘ì„±
        rewritten_query = rewrite_query(
            original_query=state['user_text'],
            current_answer=state['answer'],
            quality_score=quality_score,
            llm_client=llm_client
        )
        state['query_for_retrieval'] = rewritten_query
        
        print(f"[Refine] í’ˆì§ˆ ë‚®ìŒ ({quality_score:.2f}), ì¬ê²€ìƒ‰ í•„ìš”")
        print(f"[Refine] ì¬ì‘ì„± ì¿¼ë¦¬: {rewritten_query}")
    else:
        state['needs_retrieval'] = False
        print(f"[Refine] í’ˆì§ˆ ì¶©ë¶„ ({quality_score:.2f}), ì™„ë£Œ")
    
    return state

def evaluate_quality(answer: str, docs: list, llm_client: LLMClient) -> float:
    """LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
    prompt = f"""
    Rate the quality of this answer on a scale of 0-1.
    Consider: relevance, accuracy, completeness, clarity.
    
    Answer: {answer}
    
    Context documents: {[d['text'][:200] for d in docs[:3]]}
    
    Return only a number between 0 and 1.
    """
    
    response = llm_client.generate(prompt, temperature=0.0, max_tokens=10)
    
    try:
        score = float(response.strip())
        return max(0.0, min(1.0, score))
    except:
        return 0.5  # ê¸°ë³¸ê°’

def rewrite_query(original_query: str, current_answer: str, quality_score: float, llm_client: LLMClient) -> str:
    """ì¿¼ë¦¬ ì¬ì‘ì„±"""
    prompt = f"""
    The current answer has low quality ({quality_score:.2f}).
    Rewrite the search query to find better information.
    
    Original query: {original_query}
    Current answer: {current_answer}
    
    Rewritten query:
    """
    
    rewritten = llm_client.generate(prompt, temperature=0.3, max_tokens=100)
    return rewritten.strip()
```

### Day 11-12: Generation Ablation ì‹¤í—˜

```python
# experiments/generation_ablation.py
"""Generation ë ˆì´ì–´ ablation ì‹¤í—˜"""

EXPERIMENTS = {
    'G1_no_refine': {
        'self_refine_enabled': False
    },
    'G2_heuristic': {
        'self_refine_enabled': True,
        'llm_based_quality': False
    },
    'G3_llm_quality': {
        'self_refine_enabled': True,
        'llm_based_quality': True,
        'dynamic_query_rewrite': False
    },
    'G4_full': {
        'self_refine_enabled': True,
        'llm_based_quality': True,
        'dynamic_query_rewrite': True
    },
}

# ì‹¤í–‰ ë¡œì§ì€ data_ablation.pyì™€ ìœ ì‚¬
```

### Day 13-14: ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ë° ë…¼ë¬¸ ì‘ì„±

```python
# experiments/final_comparison.py
"""ìµœì¢… ì‹œìŠ¤í…œ ë¹„êµ"""

SYSTEMS = {
    'Baseline (LLM Only)': {...},
    'Basic RAG': {...},
    'RAG + Self-Refine': {...},
    'Full System': {...},
}

# ì „ì²´ ë©”íŠ¸ë¦­ ì¸¡ì • ë° ë¹„êµ í…Œì´ë¸” ìƒì„±
```

---

## ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: Data Layer

- [ ] **Day 1**: í˜„ì¬ ë°ì´í„° ë¶„ì„ ì™„ë£Œ
  - [ ] ì²­í¬ í¬ê¸° ë¶„í¬ í™•ì¸
  - [ ] ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (Recall@5, MRR)

- [ ] **Day 2-3**: ì²­í‚¹ ê°œì„ 
  - [ ] TypeAwareChunker êµ¬í˜„
  - [ ] ì½”í¼ìŠ¤ ì¬ì²­í‚¹ (280 tokens)
  - [ ] ë©”íƒ€ë°ì´í„° ì¶”ê°€

- [ ] **Day 4-5**: ì¸ë±ìŠ¤ êµ¬ì¶•
  - [ ] ë“€ì–¼ ì¸ë±ìŠ¤ ìƒì„±
  - [ ] ì„±ëŠ¥ ì¬ì¸¡ì •
  - [ ] ê°œì„  íš¨ê³¼ í™•ì¸ (ëª©í‘œ: Recall@5 +15%p)

- [ ] **Day 6-7**: Data Ablation
  - [ ] D1-D4 ì‹¤í—˜ ì‹¤í–‰
  - [ ] ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### Week 2: Self-Refine & Integration

- [ ] **Day 8-10**: Self-Refine êµ¬í˜„
  - [ ] Quality evaluator êµ¬í˜„
  - [ ] Query rewriter êµ¬í˜„
  - [ ] Refine loop í†µí•©

- [ ] **Day 11-12**: Generation Ablation
  - [ ] G1-G4 ì‹¤í—˜ ì‹¤í–‰
  - [ ] Judge Score ì¸¡ì •

- [ ] **Day 13-14**: ìµœì¢… í†µí•©
  - [ ] ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
  - [ ] ë…¼ë¬¸ìš© í‘œ/ê·¸ë˜í”„ ìƒì„±
  - [ ] ì½”ë“œ ì •ë¦¬ ë° ë¬¸ì„œí™”

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼

### ì •ëŸ‰ì  ëª©í‘œ

| ë©”íŠ¸ë¦­ | Baseline | Week 1 | Week 2 | ëª©í‘œ |
|-------|---------|--------|--------|------|
| **Recall@5** | 0.65 | 0.78 (+20%) | 0.82 (+26%) | > 0.75 |
| **MRR** | 0.52 | 0.65 (+25%) | 0.72 (+38%) | > 0.70 |
| **Judge Score** | 7.2 | 7.5 (+4%) | 8.5 (+18%) | > 8.0 |

### Ablation ë°œê²¬ (ì˜ˆìƒ)

```
Data Layer ê¸°ì—¬ë„:     40-50% (ê°€ì¥ í° ì˜í–¥)
Retrieval ê¸°ì—¬ë„:      10-15%
Self-Refine ê¸°ì—¬ë„:    20-30%
Context Eng ê¸°ì—¬ë„:    5-10%
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­

### 1. API ë¹„ìš© ê´€ë¦¬

```python
# ë¹„ìš© ì¶”ì •
EMBEDDING_COST = 0.00013 / 1K tokens  # text-embedding-3-large
LLM_COST = 0.15 / 1M tokens  # gpt-4o-mini input

# ì˜ˆìƒ ë¹„ìš© (1,000 ë¬¸ì„œ ê¸°ì¤€)
# - ì¬ì„ë² ë”©: ~$2-5
# - Ablation ì‹¤í—˜ (100 ì¿¼ë¦¬): ~$1-3
# ì´ ì˜ˆìƒ: $10-20
```

### 2. ì‹œê°„ ê´€ë¦¬

```
ì¬ì²­í‚¹:       1-2ì‹œê°„
ì¬ì„ë² ë”©:     2-4ì‹œê°„ (1,000 ë¬¸ì„œ ê¸°ì¤€)
ì¸ë±ìŠ¤ êµ¬ì¶•:  10-30ë¶„
ì‹¤í—˜ ì‹¤í–‰:    1-2ì‹œê°„ (per ablation)
```

### 3. ë°ì´í„° ë°±ì—…

```bash
# ì›ë³¸ ë°ì´í„° ë°±ì—…
cp -r data/corpus data/corpus_backup
cp -r data/index data/index_backup

# ì‹¤í—˜ ê²°ê³¼ ë²„ì „ ê´€ë¦¬
git add experiments/results/
git commit -m "Add ablation results"
```

---

## ğŸ’¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q: ì¬ì„ë² ë”©ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ìš”

**A: ë°°ì¹˜ í¬ê¸° ì¦ê°€ ë° ë³‘ë ¬ ì²˜ë¦¬**

```python
# data_pipeline/indexer.py
batch_size = 256  # 128 â†’ 256
# ë˜ëŠ” multiprocessing ì‚¬ìš©
```

### Q: ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

**A: ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬**

```python
# ì „ì²´ë¥¼ í•œ ë²ˆì— ë¡œë“œí•˜ì§€ ë§ê³  ìŠ¤íŠ¸ë¦¬ë°
for chunk_batch in load_chunks_in_batches(batch_size=1000):
    embeddings = embed_batch(chunk_batch)
    save_embeddings(embeddings)
```

### Q: Ablation ì‹¤í—˜ì´ ë„ˆë¬´ ë§ì•„ìš”

**A: ìš°ì„ ìˆœìœ„ ì‹¤í—˜ë§Œ ì‹¤í–‰**

```python
# í•„ìˆ˜ ì‹¤í—˜ë§Œ (ë…¼ë¬¸ìš©)
PRIORITY_EXPERIMENTS = [
    'D1_baseline',
    'D4_dual_index',  # ìµœì¢… ë°ì´í„° ì„¤ì •
    'G1_no_refine',
    'G4_full'  # ìµœì¢… ì‹œìŠ¤í…œ
]
```

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

### 2ì£¼ í›„ (MVP ì™„ì„± ì‹œ)

1. **ë…¼ë¬¸ ì‘ì„± ì‹œì‘**
   - Method ì„¹ì…˜: ì•„í‚¤í…ì²˜ ì„¤ëª…
   - Results ì„¹ì…˜: Ablation ê²°ê³¼ í…Œì´ë¸”

2. **ì¶”ê°€ ì‹¤í—˜ (ì„ íƒ)**
   - Reranker ì¶”ê°€
   - Context compression
   - Multi-turn í…ŒìŠ¤íŠ¸

3. **ì½”ë“œ ì •ë¦¬**
   - README ì‘ì„±
   - ì£¼ì„ ì¶”ê°€
   - í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

---

## ğŸ“ í•™ìŠµ ìë£Œ

### ì¶”ì²œ ë…¼ë¬¸ (ì½ê¸° ìˆœì„œ)

1. **RAG ê¸°ì´ˆ** (1-2ì‹œê°„)
   - Lewis et al. (2020). "Retrieval-Augmented Generation"
   
2. **Self-Refine** (1ì‹œê°„)
   - Madaan et al. (2023). "Self-Refine"
   
3. **ì˜í•™ ë„ë©”ì¸** (2-3ì‹œê°„)
   - Xiong et al. (2024). "Benchmarking RAG for Medicine"

### ì¶”ì²œ ì½”ë“œ ì°¸ê³ 

- **LangGraph íŠœí† ë¦¬ì–¼**: https://langchain-ai.github.io/langgraph/
- **FAISS ê°€ì´ë“œ**: https://github.com/facebookresearch/faiss/wiki
- **Ragas (í‰ê°€)**: https://docs.ragas.io/

---

## âœ… ìµœì¢… ì ê²€

ì‹œì‘ ì „ í™•ì¸:

- [ ] Python 3.9+ ì„¤ì¹˜
- [ ] OpenAI API í‚¤ ì„¤ì •
- [ ] ìµœì†Œ 10GB ë””ìŠ¤í¬ ê³µê°„
- [ ] ìµœì†Œ 8GB RAM
- [ ] Git ì €ì¥ì†Œ ì´ˆê¸°í™”

ì¤€ë¹„ ì™„ë£Œ ì‹œ:

```bash
# í™˜ê²½ ì„¤ì •
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Day 1 ì‹œì‘!
python scripts/analyze_current_data.py
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •**: 2025-12-15  
**ì˜ˆìƒ ì™„ë£Œ ì‹œê°„**: 2ì£¼ (í•˜ë£¨ 4-6ì‹œê°„ ì‘ì—… ê¸°ì¤€)

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€**

