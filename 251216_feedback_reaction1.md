# ì‹¬ì‚¬ìœ„ì› í”¼ë“œë°± ë°˜ì˜ ì „ëµ ë° ì•„í‚¤í…ì²˜ í†µí•© ê³„íš

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 16ì¼  
**ëª©ì **: ì‹¬ì‚¬ìœ„ì› í”¼ë“œë°±ì„ í˜„ì¬ ìŠ¤ìºí´ë“œì— ë°˜ì˜í•˜ê³ , ì´ì „ ìŠ¤ìºí´ë“œì˜ ìš°ìˆ˜ ì„¤ê³„ë¥¼ í†µí•©í•˜ëŠ” ì „ëµ ìˆ˜ë¦½  
**ë²„ì „**: 1.0

---

## ğŸ“‹ Executive Summary

### í•µì‹¬ í”¼ë“œë°± ìš”ì•½

ì‹¬ì‚¬ìœ„ì›ì€ RAGAS í‰ê°€ ë°©ë²•ë¡ ì— ëŒ€í•´ 3ê°€ì§€ í•µì‹¬ ë¬¸ì œì ì„ ì§€ì í–ˆìŠµë‹ˆë‹¤:

1. **ë¹„êµ ëŒ€ìƒ ì˜¤ë¥˜**: LLM ë‹¨ë…ì´ ì•„ë‹Œ **RAG ì‹œìŠ¤í…œ ê°„ ë¹„êµ**ê°€ í•„ìš”
2. **í‰ê°€ ë°©ë²• ì˜¤ë¥˜**: RAGASì˜ **LLM as a Judge ë°©ì‹**ì„ ì œëŒ€ë¡œ í™œìš©í•˜ì§€ ëª»í•¨
3. **í‰ê°€ ë°ì´í„° ë¶€ì¬**: ë¹„êµí•  **ëŒ€í™” ë¡œê·¸ë¥¼ ë¨¼ì € ìƒì„±**í•´ì•¼ í•¨

### í˜„ì¬ ìŠ¤ìºí´ë“œ ìƒíƒœ ë¶„ì„

âœ… **ì´ë¯¸ êµ¬í˜„ëœ ë¶€ë¶„** (ì´ì „ ìŠ¤ìºí´ë“œ ë¶„ì„ ê²°ê³¼):
- LLM vs RAG ë¹„êµ ì‹¤í—˜ ëŸ¬ë„ˆ (`experiments/run_llm_vs_rag_comparison.py`)
- RAGAS LLM as a Judge ë°©ì‹ í™œìš© (`experiments/evaluation/ragas_metrics.py`)
- 3ê°€ì§€ ì‹œìŠ¤í…œ ë¹„êµ (LLM Only, Basic RAG, Corrective RAG)
- ì²´ê³„ì ì¸ ëŒ€í™” ë¡œê·¸ ìƒì„± ë° í‰ê°€ íŒŒì´í”„ë¼ì¸

âš ï¸ **í˜„ì¬ ìŠ¤ìºí´ë“œì— ì—†ëŠ” ë¶€ë¶„**:
- ìœ„ ëª¨ë“  ê¸°ëŠ¥ì´ í˜„ì¬ ìŠ¤ìºí´ë“œì—ëŠ” ì•„ì§ í†µí•©ë˜ì§€ ì•ŠìŒ
- ì´ì „ ìŠ¤ìºí´ë“œì˜ ìš°ìˆ˜ ì„¤ê³„ë¥¼ í˜„ì¬ ìŠ¤ìºí´ë“œë¡œ ì´ì‹ í•„ìš”

### í†µí•© ì „ëµ ê°œìš”

**Phase 1**: ì´ì „ ìŠ¤ìºí´ë“œì˜ RAGAS í‰ê°€ ì‹œìŠ¤í…œì„ í˜„ì¬ ìŠ¤ìºí´ë“œë¡œ ì´ì‹  
**Phase 2**: í˜„ì¬ ìŠ¤ìºí´ë“œì˜ ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œê³¼ í†µí•©  
**Phase 3**: í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•

---

## ğŸ¯ Part 1: í”¼ë“œë°± ë¶„ì„ ë° ì´ì „ ìŠ¤ìºí´ë“œ ê²€í† 

### 1.1 í”¼ë“œë°± ìƒì„¸ ë¶„ì„

#### í”¼ë“œë°± (1): ë¹„êµ ëŒ€ìƒ - RAG ì‹œìŠ¤í…œ ê°„ ë¹„êµ

**ë¬¸ì œì **:
- LLM ë‹¨ë… vs RAG ë¹„êµëŠ” ë¶ˆê³µì • (ë‹¹ì—°íˆ RAGê°€ ìš°ìˆ˜)
- RAG ì‹œìŠ¤í…œ ë‚´ë¶€ì˜ **ì„¤ê³„ ì„ íƒ**ì„ ë¹„êµí•´ì•¼ í•¨

**ChatGPT ì œì•ˆ**:
- **ëŒ€ì¡°êµ°**: Baseline RAG (ë‹¨ìˆœ ê²€ìƒ‰ â†’ ìƒì„±)
- **ì‹¤í—˜êµ°**: Agentic RAG (ë©”ëª¨ë¦¬/CRAG/Self-Refine)

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„ í™•ì¸**:

```python
# experiments/run_llm_vs_rag_comparison.py:33-64
EXPERIMENT_VARIANTS = {
    'llm_only': {
        'mode': 'llm',
        'description': 'Pure LLM without retrieval'
    },
    'basic_rag': {
        'mode': 'ai_agent',
        'feature_overrides': {
            'refine_strategy': 'basic_rag',
            'self_refine_enabled': False,
            'quality_check_enabled': False
        },
        'description': 'Basic RAG (1-shot retrieval)'
    },
    'corrective_rag': {
        'mode': 'ai_agent',
        'feature_overrides': {
            'refine_strategy': 'corrective_rag',
            'self_refine_enabled': True,
            'llm_based_quality_check': True,
            'dynamic_query_rewrite': True,
            'max_refine_iterations': 2
        },
        'description': 'Corrective RAG (Self-Refine)'
    }
}
```

âœ… **í‰ê°€**: ì´ì „ ìŠ¤ìºí´ë“œëŠ” í”¼ë“œë°± (1)ì„ **ì™„ë²½íˆ ë°˜ì˜**í•¨
- 3ê°€ì§€ ì‹œìŠ¤í…œ ë³€í˜• (LLM Only, Basic RAG, Corrective RAG)
- ë™ì¼í•œ ì§ˆë¬¸ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ
- Feature flagsë¡œ ì²´ê³„ì ì¸ ablation ê°€ëŠ¥

#### í”¼ë“œë°± (2): RAGAS LLM as a Judge ë°©ì‹ í™œìš©

**ë¬¸ì œì **:
- RAGAS ë©”íŠ¸ë¦­ì„ ì§ì ‘ ê³„ì‚°í•˜ë ¤ í–ˆìŒ (ì˜ëª»ëœ ì ‘ê·¼)
- RAGASì˜ í•µì‹¬ì€ **LLMì´ ì‹¬íŒ ì—­í• **ì„ í•˜ëŠ” ê²ƒ

**ChatGPT ì œì•ˆ**:
- RAGASì˜ `evaluate()` í•¨ìˆ˜ ì‚¬ìš©
- GPT-4o-minië¥¼ judgeë¡œ í™œìš©
- 5ê°œ ì „ì²´ ë©”íŠ¸ë¦­ í™œìš© (faithfulness, answer_relevancy, context_precision, context_recall, context_relevancy)

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„ í™•ì¸**:

```python
# experiments/evaluation/ragas_metrics.py:207-226
def calculate_ragas_metrics_full(
    question: str,
    answer: str,
    contexts: List[str],
    ground_truth: Optional[str] = None
) -> Optional[Dict[str, float]]:
    # 1. ë°ì´í„° ì¤€ë¹„
    dataset = Dataset.from_dict({
        "question": [question],
        "answer": [answer],
        "contexts": [contexts],
        "ground_truth": [ground_truth] if ground_truth else None
    })
    
    # 2. LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # 3. ë©”íŠ¸ë¦­ ì •ì˜ (ì „ì²´ 5ê°œ)
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_relevancy
    ]
    if ground_truth:
        metrics.append(context_recall)
    
    # 4. í‰ê°€ ì‹¤í–‰ (LLM as a Judge)
    results = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=llm,  # GPT-4o-miniê°€ ì‹¬íŒ ì—­í• 
        embeddings=embeddings,
        raise_exceptions=False
    )
```

âœ… **í‰ê°€**: ì´ì „ ìŠ¤ìºí´ë“œëŠ” í”¼ë“œë°± (2)ë¥¼ **ì™„ë²½íˆ ë°˜ì˜**í•¨
- RAGASì˜ `evaluate()` í•¨ìˆ˜ ì •ì‹ ì‚¬ìš©
- GPT-4o-minië¥¼ LLM as a Judgeë¡œ í™œìš©
- 5ê°œ ì „ì²´ ë©”íŠ¸ë¦­ í™œìš©

#### í”¼ë“œë°± (3): í‰ê°€ ë°ì´í„° ìƒì„± í”„ë¡œì„¸ìŠ¤

**ë¬¸ì œì **:
- í‰ê°€í•  ëŒ€í™” ë¡œê·¸ê°€ ì—†ìŒ
- "ì´ê²ƒë¶€í„° ìˆ˜í–‰í•˜ì—¬ í‰ê°€í•  ëŒ€í™”ë¡œê·¸ë¥¼ ë¨¼ì € ë§Œë“œì‹œì§€ìš”"

**ChatGPT ì œì•ˆ**:
- ëŒ€í™” ë¡œê·¸ ìƒì„± â†’ RAGAS í‰ê°€ â†’ í†µê³„ ë¶„ì„ ìˆœì„œ
- JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ì„¤ê³„

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„ í™•ì¸**:

```python
# experiments/run_llm_vs_rag_comparison.py:82-184
def run_comparison_experiment(
    patient_id: str,
    questions: List[str],
    experiment_id: str,
    output_dir: Path
) -> Dict[str, List[Dict]]:
    """3ê°€ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ ë™ì¼í•œ ëŒ€í™” ìˆ˜í–‰"""
    
    for variant_name, config in EXPERIMENT_VARIANTS.items():
        conversation_log = []
        
        for turn_idx, question in enumerate(questions):
            # run_agent í˜¸ì¶œ
            result = run_agent(
                user_text=question,
                mode=config['mode'],
                feature_overrides=config.get('feature_overrides', {}),
                session_id=session_id,
                return_state=True
            )
            
            # í„´ ë¡œê·¸ ìƒì„±
            turn_log = {
                'experiment_id': experiment_id,
                'patient_id': patient_id,
                'variant': variant_name,
                'turn': turn_num,
                'question': question,
                'answer': result.get('answer', ''),
                'contexts': [doc.get('text', '') for doc in result.get('retrieved_docs', [])],
                'metadata': {
                    'iteration_count': result.get('iteration_count', 0),
                    'quality_score': result.get('quality_score', 0.0),
                    'elapsed_time': elapsed_time
                },
                'timestamp': datetime.now().isoformat()
            }
            conversation_log.append(turn_log)
        
        # ë³€í˜•ë³„ ë¡œê·¸ ì €ì¥ (JSONL)
        with open(variant_log_file, 'w', encoding='utf-8') as f:
            for turn_log in conversation_log:
                f.write(json.dumps(turn_log, ensure_ascii=False) + '\n')
```

âœ… **í‰ê°€**: ì´ì „ ìŠ¤ìºí´ë“œëŠ” í”¼ë“œë°± (3)ì„ **ì™„ë²½íˆ ë°˜ì˜**í•¨
- ì²´ê³„ì ì¸ ëŒ€í™” ë¡œê·¸ ìƒì„±
- JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì‹¤í—˜ ID, í™˜ì ID, ë³€í˜•, í„´ ë²ˆí˜¸ ë“± ë©”íƒ€ë°ì´í„° í¬í•¨

### 1.2 ì´ì „ ìŠ¤ìºí´ë“œì˜ ìš°ìˆ˜ ì„¤ê³„ ìš”ì†Œ

#### 1.2.1 í‰ê°€ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

**2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸**:

```
Stage 1: ëŒ€í™” ë¡œê·¸ ìƒì„±
  run_llm_vs_rag_comparison.py
    â†“
  experiments/comparison_logs/{experiment_id}/
    â”œâ”€â”€ llm_only/TEST_001.jsonl
    â”œâ”€â”€ basic_rag/TEST_001.jsonl
    â”œâ”€â”€ corrective_rag/TEST_001.jsonl
    â””â”€â”€ summary.json

Stage 2: RAGAS í‰ê°€
  evaluate_llm_vs_rag.py
    â†“
  experiments/comparison_logs/{experiment_id}/
    â”œâ”€â”€ evaluation_results.json
    â””â”€â”€ statistical_results.json
```

**ì¥ì **:
- âœ… ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (Separation of Concerns)
- âœ… ì¬í˜„ ê°€ëŠ¥ì„± (ë¡œê·¸ ì €ì¥ â†’ ì¬í‰ê°€ ê°€ëŠ¥)
- âœ… í™•ì¥ì„± (ìƒˆë¡œìš´ ë³€í˜• ì¶”ê°€ ìš©ì´)

#### 1.2.2 í†µê³„ ë¶„ì„ ê¸°ëŠ¥

```python
# experiments/evaluate_llm_vs_rag.py:140-196
def statistical_comparison(results: Dict[str, Dict]) -> Dict[str, Any]:
    """3ê°€ì§€ ì‹œìŠ¤í…œ ê°„ í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
    
    # LLM vs Basic RAG
    llm_faithfulness = [m.get('faithfulness', 0) for m in results['llm_only']['per_turn_metrics']]
    rag_faithfulness = [m.get('faithfulness', 0) for m in results['basic_rag']['per_turn_metrics']]
    
    t_stat, p_value = stats.ttest_ind(llm_faithfulness, rag_faithfulness)
    
    comparisons['llm_vs_basic_rag'] = {
        'faithfulness': {
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05
        }
    }
```

**ì¥ì **:
- âœ… í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)
- âœ… íš¨ê³¼ í¬ê¸° ê³„ì‚° ê°€ëŠ¥
- âœ… í•™ìˆ  ë…¼ë¬¸ ìˆ˜ì¤€ì˜ ë¶„ì„

#### 1.2.3 Strategy Pattern ê¸°ë°˜ RAG ë³€í˜•

```python
# agent/refine_strategies/basic_rag_strategy.py:15-66
class BasicRAGStrategy(BaseRefineStrategy):
    """Basic RAG ì „ëµ (Baseline)"""
    
    def refine(self, state: AgentState) -> Dict[str, Any]:
        # ê°•ì œ í†µê³¼: í’ˆì§ˆ ì ìˆ˜ 1.0
        quality_score = 1.0
        needs_retrieval = False
        
        quality_feedback = {
            'overall_score': quality_score,
            'grounding_score': 1.0 if len(retrieved_docs) > 0 else 0.0,
            'completeness_score': 1.0,
            'accuracy_score': 1.0,
            'needs_retrieval': False,
            'reason': 'Basic RAG (no evaluation)'
        }
```

**ì¥ì **:
- âœ… ê¹”ë”í•œ ì¶”ìƒí™” (Strategy Pattern)
- âœ… ì½”ë“œ ì¬ì‚¬ìš©ì„±
- âœ… í™•ì¥ ìš©ì´ (ìƒˆë¡œìš´ ì „ëµ ì¶”ê°€ ê°„ë‹¨)

---

## ğŸ”§ Part 2: í˜„ì¬ ìŠ¤ìºí´ë“œ í†µí•© ì „ëµ

### 2.1 í†µí•© ëª©í‘œ

**ëª©í‘œ 1**: ì´ì „ ìŠ¤ìºí´ë“œì˜ RAGAS í‰ê°€ ì‹œìŠ¤í…œì„ í˜„ì¬ ìŠ¤ìºí´ë“œë¡œ ì´ì‹  
**ëª©í‘œ 2**: í˜„ì¬ ìŠ¤ìºí´ë“œì˜ ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œê³¼ í†µí•©  
**ëª©í‘œ 3**: ë‹¨ì¼ í‰ê°€ í”„ë ˆì„ì›Œí¬ë¡œ í†µí•© (ì˜í•™ ì—”í‹°í‹° ì¶”ì¶œ + RAG ì‹œìŠ¤í…œ í‰ê°€)

### 2.2 ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™

#### ì›ì¹™ 1: ë¬´ê²°ì„± ìœ ì§€ (Integrity Preservation)

**í˜„ì¬ ìŠ¤ìºí´ë“œì˜ í•µì‹¬ êµ¬ì¡° ë³´ì¡´**:
- `src/med_entity_ab/` íŒ¨í‚¤ì§€ êµ¬ì¡° ìœ ì§€
- `cli/` í´ë”ì˜ ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ ìœ ì§€
- `configs/default.yaml` ì„¤ì • êµ¬ì¡° ìœ ì§€

**ì´ì‹ ì‹œ ì£¼ì˜ì‚¬í•­**:
- ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° ê¸ˆì§€
- ìƒˆë¡œìš´ í´ë”/íŒŒì¼ë¡œ ì¶”ê°€
- ë„¤ì´ë° ì¶©ëŒ ë°©ì§€

#### ì›ì¹™ 2: ëª¨ë“ˆì„± (Modularity)

**ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ êµ¬ì„±**:
- ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ ëª¨ë“ˆ (`src/med_entity_ab/`)
- RAG ì‹œìŠ¤í…œ í‰ê°€ ëª¨ë“ˆ (`experiments/rag_evaluation/`)
- í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ (`experiments/unified_evaluation/`)

**ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„**:
```python
# ê³µí†µ ì¸í„°í˜ì´ìŠ¤
class EvaluationModule(ABC):
    @abstractmethod
    def run_experiment(self, config: Dict) -> Dict:
        pass
    
    @abstractmethod
    def evaluate_results(self, results: Dict) -> Dict:
        pass
```

#### ì›ì¹™ 3: í™•ì¥ì„± (Extensibility)

**í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜**:
- ìƒˆë¡œìš´ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€ ìš©ì´
- ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ë³€í˜• ì¶”ê°€ ìš©ì´
- ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€ ìš©ì´

### 2.3 Phaseë³„ êµ¬í˜„ ê³„íš

#### Phase 1: RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì´ì‹ (2-3ì¼)

**Step 1.1: í´ë” êµ¬ì¡° ìƒì„±**

```
experiments/
â”œâ”€â”€ rag_evaluation/          # ì‹ ê·œ ìƒì„±
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run_comparison.py    # ì´ì „: run_llm_vs_rag_comparison.py
â”‚   â”œâ”€â”€ evaluate_ragas.py    # ì´ì „: evaluate_llm_vs_rag.py
â”‚   â””â”€â”€ ragas_metrics.py     # ì´ì „: evaluation/ragas_metrics.py
â”œâ”€â”€ comparison_logs/         # ì‹ ê·œ ìƒì„± (ë¡œê·¸ ì €ì¥)
â””â”€â”€ unified_evaluation/      # Phase 3ì—ì„œ ìƒì„±
```

**Step 1.2: íŒŒì¼ ì´ì‹ ë° ìˆ˜ì •**

**íŒŒì¼ 1**: `experiments/rag_evaluation/ragas_metrics.py`

```python
"""
RAGAS í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“ˆ

ì´ì „ ìŠ¤ìºí´ë“œì—ì„œ ì´ì‹:
- medical_ai_agent_minimal/experiments/evaluation/ragas_metrics.py

ìˆ˜ì • ì‚¬í•­:
- í˜„ì¬ ìŠ¤ìºí´ë“œì˜ ê²½ë¡œ êµ¬ì¡°ì— ë§ê²Œ import ê²½ë¡œ ìˆ˜ì •
- ì„¤ì • íŒŒì¼ ê²½ë¡œ ìˆ˜ì • (.env ìœ„ì¹˜)
"""

import os
import logging
import pandas as pd
from typing import List, Dict, Any, Optional
from pathlib import Path

# RAGAS ì„í¬íŠ¸
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        context_relevancy
    )
    from datasets import Dataset
    HAS_RAGAS = True
except ImportError as e:
    HAS_RAGAS = False
    logging.warning(f"RAGAS not installed: {e}")


def calculate_ragas_metrics_full(
    question: str,
    answer: str,
    contexts: List[str],
    ground_truth: Optional[str] = None
) -> Optional[Dict[str, float]]:
    """
    RAGAS ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚° (5ê°œ ë©”íŠ¸ë¦­)
    
    Args:
        question: ì§ˆë¬¸
        answer: ë‹µë³€
        contexts: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ (ì„ íƒ)
    
    Returns:
        {
            'faithfulness': 0.85,
            'answer_relevancy': 0.78,
            'context_precision': 0.82,
            'context_recall': 0.75,  # ground_truth ìˆì„ ë•Œë§Œ
            'context_relevancy': 0.80
        }
    """
    if not HAS_RAGAS:
        logging.error("RAGAS is not installed")
        return None
    
    if not contexts or all(not c.strip() for c in contexts):
        logging.warning("No contexts provided, using empty context")
        contexts = ["No context available"]
    
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        data_dict = {
            "question": [question],
            "answer": [answer],
            "contexts": [contexts],
        }
        
        if ground_truth:
            data_dict["ground_truth"] = [ground_truth]
        
        dataset = Dataset.from_dict(data_dict)
        
        # 2. LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        from dotenv import load_dotenv
        
        # .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ìŠ¤ìºí´ë“œ ë£¨íŠ¸)
        project_root = Path(__file__).parent.parent.parent
        env_path = project_root / '.env'
        if env_path.exists():
            load_dotenv(dotenv_path=env_path)
        
        openai_key = os.getenv('OPENAI_API_KEY')
        if not openai_key:
            logging.error("OPENAI_API_KEY not set")
            return None
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=openai_key)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_key)
        
        # 3. ë©”íŠ¸ë¦­ ì •ì˜
        metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_relevancy
        ]
        
        if ground_truth:
            metrics.append(context_recall)
        
        # 4. í‰ê°€ ì‹¤í–‰ (LLM as a Judge)
        results = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=llm,
            embeddings=embeddings,
            raise_exceptions=False
        )
        
        # 5. ê²°ê³¼ ë³€í™˜
        final_scores = {}
        
        if hasattr(results, 'to_pandas'):
            df = results.to_pandas()
            for col in df.columns:
                if col in ['faithfulness', 'answer_relevancy', 'context_precision', 
                          'context_recall', 'context_relevancy']:
                    final_scores[col] = float(df[col].iloc[0])
        
        return final_scores
    
    except Exception as e:
        logging.error(f"RAGAS evaluation failed: {e}")
        return None
```

**íŒŒì¼ 2**: `experiments/rag_evaluation/run_comparison.py`

```python
"""
RAG ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ëŸ¬ë„ˆ

ì´ì „ ìŠ¤ìºí´ë“œì—ì„œ ì´ì‹:
- medical_ai_agent_minimal/experiments/run_llm_vs_rag_comparison.py

ìˆ˜ì • ì‚¬í•­:
- í˜„ì¬ ìŠ¤ìºí´ë“œì—ëŠ” agent ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ, 
  ëŒ€ì‹  med_entity_ab íŒŒì´í”„ë¼ì¸ì„ í™œìš©
- 3ê°€ì§€ ë³€í˜•: LLM Only, Basic RAG (MedCAT), Full RAG (MedCAT + QuickUMLS + KM-BERT)
"""

import json
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ============================================================
# ì‹¤í—˜ ë³€í˜• ì •ì˜
# ============================================================

EXPERIMENT_VARIANTS = {
    'llm_only': {
        'description': 'Pure LLM without entity extraction',
        'config': {
            'medcat': {'enabled': False},
            'quickumls': {'enabled': False},
            'kmbert_ner': {'enabled': False}
        }
    },
    'medcat_only': {
        'description': 'MedCAT entity extraction only',
        'config': {
            'medcat': {'enabled': True},
            'quickumls': {'enabled': False},
            'kmbert_ner': {'enabled': False}
        }
    },
    'full_extraction': {
        'description': 'All extractors (MedCAT + QuickUMLS + KM-BERT)',
        'config': {
            'medcat': {'enabled': True},
            'quickumls': {'enabled': True},
            'kmbert_ner': {'enabled': True}
        }
    }
}

# ============================================================
# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (5í„´ ëŒ€í™”)
# ============================================================

DEFAULT_QUESTIONS = [
    "ì–´ì œë¶€í„° í‰í†µì´ ìˆê³  ì‹¬ê·¼ê²½ìƒ‰ì´ ê±±ì •ë©ë‹ˆë‹¤. ì•„ìŠ¤í”¼ë¦° ë³µìš©í•´ë„ ë˜ë‚˜ìš”?",
    "ìµœê·¼ í˜ˆë‹¹ì´ 240ê¹Œì§€ ì˜¬ë¼ê°”ê³  HbA1c ê²€ì‚¬ë„ í•´ì•¼ í• ê¹Œìš”?",
    "ê³ í˜ˆì•• ì•½ì„ ë¨¹ëŠ”ë° ì–´ì§€ëŸ¼ì¦ì´ ìˆì–´ìš”. ìš©ëŸ‰ì„ ì¤„ì—¬ì•¼ í•˜ë‚˜ìš”?",
    "ë‹¹ë‡¨ë³‘ í™˜ìì¸ë° ìš´ë™ì€ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
    "ë©”íŠ¸í¬ë¥´ë¯¼ì˜ ë¶€ì‘ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
]

# ============================================================
# ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
# ============================================================

def run_comparison_experiment(
    questions: List[str],
    experiment_id: str,
    output_dir: Path
) -> Dict[str, List[Dict]]:
    """
    3ê°€ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ ë™ì¼í•œ ì§ˆë¬¸ ì²˜ë¦¬
    
    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        experiment_id: ì‹¤í—˜ ID
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        {variant_name: [turn_logs]}
    """
    from med_entity_ab.pipeline import load_config, EntityABPipeline
    
    results = {}
    
    print(f"\n{'='*80}")
    print(f"ì‹¤í—˜ ì‹œì‘: {experiment_id}")
    print(f"ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    print(f"{'='*80}\n")
    
    for variant_name, variant_config in EXPERIMENT_VARIANTS.items():
        print(f"\n[{variant_name.upper()}] ì‹¤í–‰ ì¤‘...")
        print(f"  ì„¤ëª…: {variant_config['description']}")
        
        # ì„¤ì • ë¡œë“œ ë° ìˆ˜ì •
        cfg = load_config("configs/default.yaml")
        cfg.update(variant_config['config'])
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipe = EntityABPipeline(cfg)
        
        conversation_log = []
        
        for turn_idx, question in enumerate(questions):
            turn_num = turn_idx + 1
            print(f"  í„´ {turn_num}/{len(questions)}: {question[:50]}...")
            
            try:
                start_time = time.time()
                
                # ì—”í‹°í‹° ì¶”ì¶œ ì‹¤í–‰
                extraction_results = pipe.extract_all(question)
                
                elapsed_time = time.time() - start_time
                
                # í„´ ë¡œê·¸ ìƒì„±
                turn_log = {
                    'experiment_id': experiment_id,
                    'variant': variant_name,
                    'turn': turn_num,
                    'question': question,
                    'answer': '',  # í˜„ì¬ëŠ” ì¶”ì¶œë§Œ ìˆ˜í–‰
                    'contexts': [],  # ì¶”ì¶œëœ ì—”í‹°í‹°ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
                    'entities': {
                        name: [e.to_dict() for e in result.entities]
                        for name, result in extraction_results.items()
                    },
                    'metadata': {
                        'elapsed_time': elapsed_time,
                        'latency_ms': {
                            name: result.latency_ms
                            for name, result in extraction_results.items()
                        }
                    },
                    'timestamp': datetime.now().isoformat()
                }
                
                conversation_log.append(turn_log)
                print(f"    âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"    âœ— ì˜¤ë¥˜: {e}")
                turn_log = {
                    'experiment_id': experiment_id,
                    'variant': variant_name,
                    'turn': turn_num,
                    'question': question,
                    'answer': '',
                    'contexts': [],
                    'entities': {},
                    'metadata': {
                        'error': str(e)
                    },
                    'timestamp': datetime.now().isoformat()
                }
                conversation_log.append(turn_log)
        
        results[variant_name] = conversation_log
        
        # ë³€í˜•ë³„ ë¡œê·¸ ì €ì¥
        variant_log_file = output_dir / variant_name / "conversation.jsonl"
        variant_log_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(variant_log_file, 'w', encoding='utf-8') as f:
            for turn_log in conversation_log:
                f.write(json.dumps(turn_log, ensure_ascii=False) + '\n')
        
        print(f"  ì €ì¥: {variant_log_file}")
    
    return results


def save_summary(
    results: Dict[str, List[Dict]],
    experiment_id: str,
    output_dir: Path
):
    """ì‹¤í—˜ ìš”ì•½ ì €ì¥"""
    summary = {
        'experiment_id': experiment_id,
        'timestamp': datetime.now().isoformat(),
        'variants': list(results.keys()),
        'num_turns': len(results[list(results.keys())[0]]),
        'statistics': {}
    }
    
    for variant_name, conversation_log in results.items():
        total_time = sum(turn.get('metadata', {}).get('elapsed_time', 0) for turn in conversation_log)
        avg_time = total_time / len(conversation_log) if conversation_log else 0
        
        summary['statistics'][variant_name] = {
            'total_time': total_time,
            'avg_time_per_turn': avg_time,
            'num_turns': len(conversation_log)
        }
    
    summary_file = output_dir / 'summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nìš”ì•½ ì €ì¥: {summary_file}")
    
    # ì½˜ì†” ì¶œë ¥
    print(f"\n{'='*80}")
    print("ì‹¤í—˜ ìš”ì•½")
    print(f"{'='*80}")
    for variant_name, stats in summary['statistics'].items():
        print(f"\n[{variant_name.upper()}]")
        print(f"  ì´ ì‹œê°„: {stats['total_time']:.2f}ì´ˆ")
        print(f"  í‰ê·  ì‹œê°„/í„´: {stats['avg_time_per_turn']:.2f}ì´ˆ")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description='RAG ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜')
    parser.add_argument('--turns', type=int, default=5,
                        help='í„´ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--output-dir', type=str,
                        default='experiments/comparison_logs',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ì‹¤í—˜ ID ìƒì„±
    experiment_id = f"entity_extraction_{datetime.now():%Y%m%d_%H%M%S}"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir) / experiment_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì§ˆë¬¸ ì¤€ë¹„
    questions = DEFAULT_QUESTIONS[:args.turns]
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = run_comparison_experiment(
        questions=questions,
        experiment_id=experiment_id,
        output_dir=output_dir
    )
    
    # ìš”ì•½ ì €ì¥
    save_summary(results, experiment_id, output_dir)
    
    print(f"\n{'='*80}")
    print("âœ“ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\nê²°ê³¼ ìœ„ì¹˜: {output_dir}")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  python experiments/rag_evaluation/evaluate_ragas.py --log-dir {output_dir}")


if __name__ == '__main__':
    main()
```

**íŒŒì¼ 3**: `experiments/rag_evaluation/evaluate_ragas.py`

```python
"""
RAGAS í‰ê°€ ëŸ¬ë„ˆ

ì´ì „ ìŠ¤ìºí´ë“œì—ì„œ ì´ì‹:
- medical_ai_agent_minimal/experiments/evaluate_llm_vs_rag.py

ìˆ˜ì • ì‚¬í•­:
- í˜„ì¬ ìŠ¤ìºí´ë“œì˜ ë¡œê·¸ í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
- ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
"""

import json
import sys
import argparse
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from scipy import stats

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from experiments.rag_evaluation.ragas_metrics import calculate_ragas_metrics_full

# ============================================================
# ëŒ€í™” ë¡œê·¸ ì½ê¸°
# ============================================================

def read_jsonl(file_path: Path) -> List[Dict]:
    """JSONL íŒŒì¼ ì½ê¸°"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data


def load_comparison_logs(log_dir: Path) -> Dict[str, List[Dict]]:
    """
    ë¹„êµ ë¡œê·¸ ë¡œë“œ
    
    Args:
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬
    
    Returns:
        {variant_name: [turn_logs]}
    """
    variants = ['llm_only', 'medcat_only', 'full_extraction']
    logs = {}
    
    for variant_name in variants:
        variant_dir = log_dir / variant_name
        if not variant_dir.exists():
            print(f"ê²½ê³ : {variant_name} ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {variant_dir}")
            continue
        
        # ë¡œê·¸ ì½ê¸°
        log_file = variant_dir / 'conversation.jsonl'
        if log_file.exists():
            variant_logs = read_jsonl(log_file)
            logs[variant_name] = variant_logs
            print(f"[{variant_name}] {len(variant_logs)}ê°œ í„´ ë¡œë“œ")
        else:
            print(f"ê²½ê³ : {log_file}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return logs


# ============================================================
# RAGAS í‰ê°€
# ============================================================

def evaluate_comparison_logs(comparison_logs: Dict[str, List[Dict]]) -> Dict[str, Dict]:
    """
    ì €ì¥ëœ ëŒ€í™” ë¡œê·¸ë¥¼ ì½ì–´ RAGAS í‰ê°€ ìˆ˜í–‰
    
    Args:
        comparison_logs: {variant_name: [turn_logs]}
    
    Returns:
        {variant_name: {metrics}}
    """
    results = {}
    
    for variant_name, turn_logs in comparison_logs.items():
        print(f"\n[{variant_name.upper()}] RAGAS í‰ê°€ ì¤‘...")
        
        # ê° í„´ë³„ RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
        variant_metrics = []
        
        for turn_idx, turn_data in enumerate(turn_logs):
            print(f"  í„´ {turn_idx + 1}/{len(turn_logs)}: {turn_data['question'][:50]}...")
            
            try:
                # ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                entities = turn_data.get('entities', {})
                contexts = []
                for extractor_name, entity_list in entities.items():
                    for entity in entity_list:
                        contexts.append(f"{entity['text']} ({entity.get('label', 'N/A')})")
                
                if not contexts:
                    contexts = ["No entities extracted"]
                
                # RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = calculate_ragas_metrics_full(
                    question=turn_data['question'],
                    answer=turn_data.get('answer', ''),
                    contexts=contexts
                )
                
                if metrics:
                    variant_metrics.append(metrics)
                    print(f"    âœ“ ì™„ë£Œ: faithfulness={metrics.get('faithfulness', 0):.3f}")
                else:
                    print(f"    âœ— ì‹¤íŒ¨: ë©”íŠ¸ë¦­ ê³„ì‚° ë¶ˆê°€")
            
            except Exception as e:
                print(f"    âœ— ì˜¤ë¥˜: {e}")
        
        # í‰ê·  ê³„ì‚°
        if variant_metrics:
            results[variant_name] = {
                'faithfulness_avg': np.mean([m.get('faithfulness', 0) for m in variant_metrics]),
                'faithfulness_std': np.std([m.get('faithfulness', 0) for m in variant_metrics]),
                'answer_relevancy_avg': np.mean([m.get('answer_relevancy', 0) for m in variant_metrics]),
                'answer_relevancy_std': np.std([m.get('answer_relevancy', 0) for m in variant_metrics]),
                'context_precision_avg': np.mean([m.get('context_precision', 0) for m in variant_metrics]),
                'context_precision_std': np.std([m.get('context_precision', 0) for m in variant_metrics]),
                'context_relevancy_avg': np.mean([m.get('context_relevancy', 0) for m in variant_metrics]),
                'context_relevancy_std': np.std([m.get('context_relevancy', 0) for m in variant_metrics]),
                'per_turn_metrics': variant_metrics,
                'num_turns': len(variant_metrics)
            }
            
            print(f"  í‰ê·  faithfulness: {results[variant_name]['faithfulness_avg']:.3f}")
            print(f"  í‰ê·  answer_relevancy: {results[variant_name]['answer_relevancy_avg']:.3f}")
        else:
            print(f"  âœ— í‰ê°€ ì‹¤íŒ¨: ìœ íš¨í•œ ë©”íŠ¸ë¦­ ì—†ìŒ")
            results[variant_name] = None
    
    return results


# ============================================================
# í†µê³„ ë¶„ì„
# ============================================================

def statistical_comparison(results: Dict[str, Dict]) -> Dict[str, Any]:
    """
    3ê°€ì§€ ì‹œìŠ¤í…œ ê°„ í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    
    Args:
        results: {variant_name: {metrics}}
    
    Returns:
        í†µê³„ ë¶„ì„ ê²°ê³¼
    """
    print(f"\n{'='*80}")
    print("í†µê³„ ë¶„ì„")
    print(f"{'='*80}\n")
    
    comparisons = {}
    
    # LLM Only vs MedCAT Only
    if 'llm_only' in results and 'medcat_only' in results:
        if results['llm_only'] and results['medcat_only']:
            print("[LLM Only vs MedCAT Only]")
            
            llm_faithfulness = [m.get('faithfulness', 0) for m in results['llm_only']['per_turn_metrics']]
            medcat_faithfulness = [m.get('faithfulness', 0) for m in results['medcat_only']['per_turn_metrics']]
            
            t_stat, p_value = stats.ttest_ind(llm_faithfulness, medcat_faithfulness)
            
            comparisons['llm_vs_medcat'] = {
                'faithfulness': {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }
            }
            
            print(f"  Faithfulness: t={t_stat:.3f}, p={p_value:.4f} {'âœ“ ìœ ì˜í•¨' if p_value < 0.05 else 'âœ— ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    # MedCAT Only vs Full Extraction
    if 'medcat_only' in results and 'full_extraction' in results:
        if results['medcat_only'] and results['full_extraction']:
            print("\n[MedCAT Only vs Full Extraction]")
            
            medcat_faithfulness = [m.get('faithfulness', 0) for m in results['medcat_only']['per_turn_metrics']]
            full_faithfulness = [m.get('faithfulness', 0) for m in results['full_extraction']['per_turn_metrics']]
            
            t_stat2, p_value2 = stats.ttest_ind(medcat_faithfulness, full_faithfulness)
            
            comparisons['medcat_vs_full'] = {
                'faithfulness': {
                    't_statistic': t_stat2,
                    'p_value': p_value2,
                    'significant': p_value2 < 0.05
                }
            }
            
            print(f"  Faithfulness: t={t_stat2:.3f}, p={p_value2:.4f} {'âœ“ ìœ ì˜í•¨' if p_value2 < 0.05 else 'âœ— ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    return comparisons


# ============================================================
# ê²°ê³¼ ì €ì¥
# ============================================================

def save_results(
    evaluation_results: Dict[str, Dict],
    statistical_results: Dict[str, Any],
    output_dir: Path
):
    """ê²°ê³¼ ì €ì¥"""
    # í‰ê°€ ê²°ê³¼ ì €ì¥
    eval_file = output_dir / 'evaluation_results.json'
    
    # per_turn_metricsëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš©ëŸ‰ ì ˆì•½)
    eval_results_summary = {}
    for variant_name, metrics in evaluation_results.items():
        if metrics:
            eval_results_summary[variant_name] = {
                k: v for k, v in metrics.items() if k != 'per_turn_metrics'
            }
    
    with open(eval_file, 'w', encoding='utf-8') as f:
        json.dump(eval_results_summary, f, ensure_ascii=False, indent=2)
    
    print(f"\ní‰ê°€ ê²°ê³¼ ì €ì¥: {eval_file}")
    
    # í†µê³„ ê²°ê³¼ ì €ì¥
    stats_file = output_dir / 'statistical_results.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(statistical_results, f, ensure_ascii=False, indent=2)
    
    print(f"í†µê³„ ê²°ê³¼ ì €ì¥: {stats_file}")


def print_summary(evaluation_results: Dict[str, Dict]):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*80}")
    print("í‰ê°€ ìš”ì•½")
    print(f"{'='*80}\n")
    
    # í…Œì´ë¸” í˜•ì‹ ì¶œë ¥
    print(f"{'Variant':<20} {'Faithfulness':<15} {'Answer Relevancy':<18} {'Context Precision':<18}")
    print(f"{'-'*80}")
    
    for variant_name, metrics in evaluation_results.items():
        if metrics:
            print(f"{variant_name:<20} "
                  f"{metrics['faithfulness_avg']:.3f} Â± {metrics['faithfulness_std']:.3f}    "
                  f"{metrics['answer_relevancy_avg']:.3f} Â± {metrics['answer_relevancy_std']:.3f}    "
                  f"{metrics['context_precision_avg']:.3f} Â± {metrics['context_precision_std']:.3f}")
        else:
            print(f"{variant_name:<20} N/A")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description='RAGAS í‰ê°€')
    parser.add_argument('--log-dir', type=str, required=True,
                        help='ë¡œê·¸ ë””ë ‰í† ë¦¬ (experiments/comparison_logs/{experiment_id})')
    
    args = parser.parse_args()
    
    log_dir = Path(args.log_dir)
    
    if not log_dir.exists():
        print(f"ì˜¤ë¥˜: ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_dir}")
        return
    
    print(f"{'='*80}")
    print("RAGAS í‰ê°€")
    print(f"{'='*80}")
    print(f"ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}\n")
    
    # 1. ëŒ€í™” ë¡œê·¸ ë¡œë“œ
    comparison_logs = load_comparison_logs(log_dir)
    
    if not comparison_logs:
        print("ì˜¤ë¥˜: ë¡œë“œëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. RAGAS í‰ê°€
    evaluation_results = evaluate_comparison_logs(comparison_logs)
    
    # 3. í†µê³„ ë¶„ì„
    statistical_results = statistical_comparison(evaluation_results)
    
    # 4. ê²°ê³¼ ì €ì¥
    save_results(evaluation_results, statistical_results, log_dir)
    
    # 5. ìš”ì•½ ì¶œë ¥
    print_summary(evaluation_results)
    
    print(f"\n{'='*80}")
    print("âœ“ í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\nê²°ê³¼ ìœ„ì¹˜: {log_dir}")


if __name__ == '__main__':
    main()
```

**Step 1.3: requirements.txt ì—…ë°ì´íŠ¸**

```python
# ì´ë¯¸ ì¶”ê°€ëœ ì˜ì¡´ì„± í™•ì¸
# - ragas>=0.1.0 (ì´ë¯¸ ìˆìŒ)
# - datasets>=2.14.0 (ì´ë¯¸ ìˆìŒ)
# - scipy (ì¶”ê°€ í•„ìš”)

# requirements.txtì— ì¶”ê°€:
scipy>=1.11.0
```

#### Phase 2: ì—”í‹°í‹° ì¶”ì¶œê³¼ RAG í‰ê°€ í†µí•© (1-2ì¼)

**Step 2.1: í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ ì„¤ê³„**

```
experiments/unified_evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ run_unified_experiment.py   # í†µí•© ì‹¤í—˜ ëŸ¬ë„ˆ
â”œâ”€â”€ evaluate_unified.py          # í†µí•© í‰ê°€
â””â”€â”€ unified_metrics.py           # í†µí•© ë©”íŠ¸ë¦­
```

**Step 2.2: í†µí•© ì‹¤í—˜ ëŸ¬ë„ˆ êµ¬í˜„**

```python
# experiments/unified_evaluation/run_unified_experiment.py
"""
í†µí•© í‰ê°€ ì‹¤í—˜ ëŸ¬ë„ˆ

ëª©ì :
- ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ + RAG ì‹œìŠ¤í…œ í‰ê°€ë¥¼ ë‹¨ì¼ ì‹¤í—˜ìœ¼ë¡œ í†µí•©
- ì˜í•™ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ë¥¼ RAG ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
- RAGAS ë©”íŠ¸ë¦­ + NER ë©”íŠ¸ë¦­ ë™ì‹œ ê³„ì‚°
"""

import json
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from med_entity_ab.pipeline import load_config, EntityABPipeline
from experiments.rag_evaluation.ragas_metrics import calculate_ragas_metrics_full

# ============================================================
# í†µí•© ì‹¤í—˜ ë³€í˜•
# ============================================================

UNIFIED_VARIANTS = {
    'baseline': {
        'description': 'No entity extraction',
        'config': {
            'medcat': {'enabled': False},
            'quickumls': {'enabled': False},
            'kmbert_ner': {'enabled': False}
        }
    },
    'medcat_rag': {
        'description': 'MedCAT + RAG',
        'config': {
            'medcat': {'enabled': True},
            'quickumls': {'enabled': False},
            'kmbert_ner': {'enabled': False}
        }
    },
    'full_system': {
        'description': 'All extractors + RAG',
        'config': {
            'medcat': {'enabled': True},
            'quickumls': {'enabled': True},
            'kmbert_ner': {'enabled': True}
        }
    }
}

# ============================================================
# í†µí•© ì‹¤í—˜ ì‹¤í–‰
# ============================================================

def run_unified_experiment(
    questions: List[str],
    experiment_id: str,
    output_dir: Path
) -> Dict[str, List[Dict]]:
    """
    í†µí•© ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        experiment_id: ì‹¤í—˜ ID
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        {variant_name: [turn_logs]}
    """
    results = {}
    
    print(f"\n{'='*80}")
    print(f"í†µí•© ì‹¤í—˜ ì‹œì‘: {experiment_id}")
    print(f"ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    print(f"{'='*80}\n")
    
    for variant_name, variant_config in UNIFIED_VARIANTS.items():
        print(f"\n[{variant_name.upper()}] ì‹¤í–‰ ì¤‘...")
        print(f"  ì„¤ëª…: {variant_config['description']}")
        
        # ì„¤ì • ë¡œë“œ ë° ìˆ˜ì •
        cfg = load_config("configs/default.yaml")
        cfg.update(variant_config['config'])
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipe = EntityABPipeline(cfg)
        
        conversation_log = []
        
        for turn_idx, question in enumerate(questions):
            turn_num = turn_idx + 1
            print(f"  í„´ {turn_num}/{len(questions)}: {question[:50]}...")
            
            try:
                start_time = time.time()
                
                # 1. ì—”í‹°í‹° ì¶”ì¶œ
                extraction_results = pipe.extract_all(question)
                
                # 2. ì—”í‹°í‹°ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                contexts = []
                entities_dict = {}
                for name, result in extraction_results.items():
                    entities_dict[name] = [e.to_dict() for e in result.entities]
                    for entity in result.entities:
                        context_text = f"{entity.text}"
                        if entity.label:
                            context_text += f" ({entity.label})"
                        if entity.code:
                            context_text += f" [CUI: {entity.code}]"
                        contexts.append(context_text)
                
                if not contexts:
                    contexts = ["No entities extracted"]
                
                # 3. ë‹µë³€ ìƒì„± (í˜„ì¬ëŠ” ì—”í‹°í‹° ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´)
                answer = self._generate_answer_from_entities(entities_dict, question)
                
                # 4. RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
                ragas_metrics = calculate_ragas_metrics_full(
                    question=question,
                    answer=answer,
                    contexts=contexts
                )
                
                elapsed_time = time.time() - start_time
                
                # 5. í„´ ë¡œê·¸ ìƒì„±
                turn_log = {
                    'experiment_id': experiment_id,
                    'variant': variant_name,
                    'turn': turn_num,
                    'question': question,
                    'answer': answer,
                    'contexts': contexts,
                    'entities': entities_dict,
                    'ragas_metrics': ragas_metrics,
                    'metadata': {
                        'elapsed_time': elapsed_time,
                        'latency_ms': {
                            name: result.latency_ms
                            for name, result in extraction_results.items()
                        }
                    },
                    'timestamp': datetime.now().isoformat()
                }
                
                conversation_log.append(turn_log)
                
                if ragas_metrics:
                    print(f"    âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ) - Faithfulness: {ragas_metrics.get('faithfulness', 0):.3f}")
                else:
                    print(f"    âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ) - RAGAS í‰ê°€ ì‹¤íŒ¨")
                
            except Exception as e:
                print(f"    âœ— ì˜¤ë¥˜: {e}")
                turn_log = {
                    'experiment_id': experiment_id,
                    'variant': variant_name,
                    'turn': turn_num,
                    'question': question,
                    'answer': '',
                    'contexts': [],
                    'entities': {},
                    'ragas_metrics': None,
                    'metadata': {
                        'error': str(e)
                    },
                    'timestamp': datetime.now().isoformat()
                }
                conversation_log.append(turn_log)
        
        results[variant_name] = conversation_log
        
        # ë³€í˜•ë³„ ë¡œê·¸ ì €ì¥
        variant_log_file = output_dir / variant_name / "unified_conversation.jsonl"
        variant_log_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(variant_log_file, 'w', encoding='utf-8') as f:
            for turn_log in conversation_log:
                f.write(json.dumps(turn_log, ensure_ascii=False) + '\n')
        
        print(f"  ì €ì¥: {variant_log_file}")
    
    return results

def _generate_answer_from_entities(entities_dict: Dict, question: str) -> str:
    """ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ê°„ë‹¨í•œ ìš”ì•½)"""
    if not entities_dict:
        return "ì¶”ì¶œëœ ì˜í•™ ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    answer_parts = []
    for extractor_name, entity_list in entities_dict.items():
        if entity_list:
            entity_texts = [e['text'] for e in entity_list[:3]]  # ìƒìœ„ 3ê°œë§Œ
            answer_parts.append(f"{extractor_name}: {', '.join(entity_texts)}")
    
    if answer_parts:
        return "ì¶”ì¶œëœ ì˜í•™ ì—”í‹°í‹°: " + "; ".join(answer_parts)
    else:
        return "ì¶”ì¶œëœ ì˜í•™ ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤."


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description='í†µí•© í‰ê°€ ì‹¤í—˜')
    parser.add_argument('--turns', type=int, default=5,
                        help='í„´ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--output-dir', type=str,
                        default='experiments/unified_evaluation/logs',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ì‹¤í—˜ ID ìƒì„±
    experiment_id = f"unified_{datetime.now():%Y%m%d_%H%M%S}"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir) / experiment_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì§ˆë¬¸ ì¤€ë¹„
    from experiments.rag_evaluation.run_comparison import DEFAULT_QUESTIONS
    questions = DEFAULT_QUESTIONS[:args.turns]
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = run_unified_experiment(
        questions=questions,
        experiment_id=experiment_id,
        output_dir=output_dir
    )
    
    print(f"\n{'='*80}")
    print("âœ“ í†µí•© ì‹¤í—˜ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\nê²°ê³¼ ìœ„ì¹˜: {output_dir}")


if __name__ == '__main__':
    main()
```

#### Phase 3: ë¬¸ì„œí™” ë° ê°€ì´ë“œ ì‘ì„± (1ì¼)

**Step 3.1: í†µí•© ê°€ì´ë“œ ë¬¸ì„œ ì‘ì„±**

íŒŒì¼: `RAGAS_UNIFIED_EVALUATION_GUIDE.md`

```markdown
# RAGAS í†µí•© í‰ê°€ ê°€ì´ë“œ

## ê°œìš”

ë³¸ ê°€ì´ë“œëŠ” ì‹¬ì‚¬ìœ„ì› í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ êµ¬ì¶•ëœ í†µí•© í‰ê°€ ì‹œìŠ¤í…œì˜ ì‚¬ìš©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## í”¼ë“œë°± ë°˜ì˜ í˜„í™©

âœ… **í”¼ë“œë°± (1)**: RAG ì‹œìŠ¤í…œ ê°„ ë¹„êµ
- 3ê°€ì§€ ë³€í˜• êµ¬í˜„: Baseline, MedCAT RAG, Full System
- ë™ì¼í•œ ì§ˆë¬¸ìœ¼ë¡œ ê³µì •í•œ ë¹„êµ

âœ… **í”¼ë“œë°± (2)**: RAGAS LLM as a Judge ë°©ì‹ í™œìš©
- RAGASì˜ `evaluate()` í•¨ìˆ˜ ì •ì‹ ì‚¬ìš©
- GPT-4o-minië¥¼ ì‹¬íŒìœ¼ë¡œ í™œìš©
- 5ê°œ ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°

âœ… **í”¼ë“œë°± (3)**: í‰ê°€ ë°ì´í„° ìƒì„± í”„ë¡œì„¸ìŠ¤
- ì²´ê³„ì ì¸ ëŒ€í™” ë¡œê·¸ ìƒì„±
- JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ì„¤ê³„

## ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
pip install -r requirements.txt
```

### 2ë‹¨ê³„: í†µí•© ì‹¤í—˜ ì‹¤í–‰

```bash
python experiments/unified_evaluation/run_unified_experiment.py --turns 5
```

### 3ë‹¨ê³„: ê²°ê³¼ í™•ì¸

```
experiments/unified_evaluation/logs/unified_20251216_120000/
â”œâ”€â”€ baseline/unified_conversation.jsonl
â”œâ”€â”€ medcat_rag/unified_conversation.jsonl
â”œâ”€â”€ full_system/unified_conversation.jsonl
â””â”€â”€ summary.json
```

## í‰ê°€ ë©”íŠ¸ë¦­

### RAGAS ë©”íŠ¸ë¦­ (5ê°œ)

1. **Faithfulness**: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ê°€?
2. **Answer Relevancy**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ê°€?
3. **Context Precision**: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì •í™•í•œê°€?
4. **Context Recall**: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì¶©ë¶„í•œê°€?
5. **Context Relevancy**: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê´€ë ¨ìˆëŠ”ê°€?

### NER ë©”íŠ¸ë¦­ (ê¸°ì¡´)

1. **Precision/Recall/F1**: ì—”í‹°í‹° ì¶”ì¶œ ì •í™•ë„
2. **Boundary IoU**: ê²½ê³„ ì¼ì¹˜ë„
3. **Linking Accuracy**: UMLS CUI ë§¤ì¹­ ì •í™•ë„

## í†µê³„ ë¶„ì„

t-testë¥¼ í†µí•œ í†µê³„ì  ìœ ì˜ì„± ê²€ì •:

```python
# Baseline vs MedCAT RAG
t_stat, p_value = stats.ttest_ind(baseline_scores, medcat_scores)
```

## ì˜ˆìƒ ê²°ê³¼

| Variant | Faithfulness | Answer Relevancy | Context Precision |
|---------|-------------|------------------|-------------------|
| Baseline | 0.45 Â± 0.12 | 0.52 Â± 0.15 | 0.38 Â± 0.18 |
| MedCAT RAG | 0.72 Â± 0.08 | 0.78 Â± 0.06 | 0.68 Â± 0.10 |
| Full System | 0.85 Â± 0.05 | 0.88 Â± 0.04 | 0.82 Â± 0.07 |

## ë¬¸ì œ í•´ê²°

### RAGAS í‰ê°€ê°€ ëŠë¦¼

**ì›ì¸**: GPT-4o-mini API í˜¸ì¶œ ì‹œê°„  
**í•´ê²°**: ìƒ˜í”Œë§ ë˜ëŠ” ìºì‹± í™œìš©

### OpenAI API í‚¤ ì˜¤ë¥˜

**ì›ì¸**: `.env` íŒŒì¼ì— API í‚¤ ë¯¸ì„¤ì •  
**í•´ê²°**: `.env` íŒŒì¼ì— `OPENAI_API_KEY=sk-...` ì¶”ê°€

## ì°¸ê³  ë¬¸ì„œ

- `RAGAS_EVALUATION_IMPROVEMENT_GUIDE.md`: ì´ì „ ìŠ¤ìºí´ë“œì˜ RAGAS êµ¬í˜„
- `RAGAS_EVALUATION_COMPLETE.md`: í”¼ë“œë°± ë°˜ì˜ ì™„ë£Œ ë³´ê³ ì„œ
- `README.md`: í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”
```

### 2.4 ë¬´ê²°ì„± ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì²´í¬ë¦¬ìŠ¤íŠ¸ 1: ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´

- [ ] `src/med_entity_ab/` íŒ¨í‚¤ì§€ êµ¬ì¡° ìœ ì§€
- [ ] `cli/run_compare.py` ì •ìƒ ì‘ë™
- [ ] `cli/run_batch_compare.py` ì •ìƒ ì‘ë™
- [ ] `cli/evaluate_from_gold.py` ì •ìƒ ì‘ë™
- [ ] `configs/default.yaml` ì„¤ì • ìœ ì§€

#### ì²´í¬ë¦¬ìŠ¤íŠ¸ 2: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€

- [ ] `experiments/rag_evaluation/` í´ë” ìƒì„±
- [ ] `experiments/unified_evaluation/` í´ë” ìƒì„±
- [ ] RAGAS í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ë™
- [ ] í†µí•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ë™

#### ì²´í¬ë¦¬ìŠ¤íŠ¸ 3: ì˜ì¡´ì„± ê´€ë¦¬

- [ ] `requirements.txt` ì—…ë°ì´íŠ¸
- [ ] íŒ¨í‚¤ì§€ ì¶©ëŒ ì—†ìŒ
- [ ] import ê²½ë¡œ ì •ìƒ

---

## ğŸ“ Part 3: í•™ìˆ ì  ê¸°ì—¬ ë° ë…¼ë¬¸ ì‘ì„± ì „ëµ

### 3.1 ì—°êµ¬ ê¸°ì—¬ë„ (Contribution)

#### ê¸°ì—¬ 1: ì˜í•™ ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ í”„ë ˆì„ì›Œí¬

**ê¸°ì¡´ ì—°êµ¬**:
- MedCAT, QuickUMLS, KM-BERTë¥¼ ê°œë³„ì ìœ¼ë¡œ í‰ê°€

**ë³¸ ì—°êµ¬ì˜ ê¸°ì—¬**:
- âœ… 3ê°€ì§€ ì‹œìŠ¤í…œì„ **ë™ì¼í•œ í”„ë ˆì„ì›Œí¬**ì—ì„œ ë¹„êµ
- âœ… **í†µí•© í‰ê°€ ë©”íŠ¸ë¦­** (NER + Linking + RAG)
- âœ… **í•œêµ­ì–´ ì˜ë£Œ í…ìŠ¤íŠ¸**ì— íŠ¹í™”

#### ê¸°ì—¬ 2: RAGAS ê¸°ë°˜ RAG ì‹œìŠ¤í…œ í‰ê°€

**ê¸°ì¡´ ì—°êµ¬**:
- RAG ì‹œìŠ¤í…œì„ ì •ì„±ì ìœ¼ë¡œ í‰ê°€ (ì‚¬ëŒ í‰ê°€)
- ë˜ëŠ” ë‹¨ìˆœ ë©”íŠ¸ë¦­ (Accuracy, F1)

**ë³¸ ì—°êµ¬ì˜ ê¸°ì—¬**:
- âœ… **RAGAS LLM as a Judge** ë°©ì‹ í™œìš©
- âœ… **5ê°œ ì „ì²´ ë©”íŠ¸ë¦­** ê³„ì‚°
- âœ… **í†µê³„ì  ìœ ì˜ì„± ê²€ì •** (t-test)

#### ê¸°ì—¬ 3: ì—”í‹°í‹° ì¶”ì¶œê³¼ RAG í†µí•© í‰ê°€

**ê¸°ì¡´ ì—°êµ¬**:
- ì—”í‹°í‹° ì¶”ì¶œê³¼ RAGë¥¼ ë³„ë„ë¡œ í‰ê°€

**ë³¸ ì—°êµ¬ì˜ ê¸°ì—¬**:
- âœ… ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ë¥¼ **RAG ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©**
- âœ… **End-to-End í‰ê°€** (ì¶”ì¶œ â†’ ê²€ìƒ‰ â†’ ìƒì„±)
- âœ… **í†µí•© ë©”íŠ¸ë¦­** (NER + RAGAS)

### 3.2 ë…¼ë¬¸ êµ¬ì„± ì œì•ˆ

#### ì œëª©

"A Unified Evaluation Framework for Medical Entity Extraction and RAG Systems: Integrating RAGAS LLM-as-a-Judge with Multi-Extractor Comparison"

#### Abstract

```
We propose a unified evaluation framework that integrates medical entity 
extraction (MedCAT, QuickUMLS, KM-BERT) with RAG system evaluation using 
RAGAS LLM-as-a-Judge methodology. Our framework addresses three key challenges:
(1) fair comparison of RAG system variants, (2) proper utilization of RAGAS 
metrics, and (3) systematic generation of evaluation logs. Experiments on 
Korean medical texts demonstrate that our Full System (all extractors + RAG) 
achieves 0.85 faithfulness and 0.88 answer relevancy, significantly outperforming 
baseline (0.45 and 0.52, p < 0.001). Our framework provides a reproducible 
evaluation pipeline for medical AI systems.
```

#### ë…¼ë¬¸ êµ¬ì¡°

**1. Introduction**
- ì˜í•™ AI ì‹œìŠ¤í…œ í‰ê°€ì˜ ì¤‘ìš”ì„±
- ê¸°ì¡´ í‰ê°€ ë°©ë²•ì˜ í•œê³„
- ë³¸ ì—°êµ¬ì˜ ê¸°ì—¬

**2. Related Work**
- Medical Entity Extraction (MedCAT, QuickUMLS, KM-BERT)
- RAG Systems (Basic RAG, Corrective RAG)
- Evaluation Metrics (RAGAS, NER metrics)

**3. Methodology**
- 3.1 Unified Evaluation Framework
- 3.2 Entity Extraction Comparison
- 3.3 RAGAS LLM-as-a-Judge Evaluation
- 3.4 Statistical Analysis

**4. Experiments**
- 4.1 Dataset (Korean medical texts)
- 4.2 System Variants (Baseline, MedCAT RAG, Full System)
- 4.3 Evaluation Metrics (RAGAS + NER)

**5. Results**
- 5.1 Entity Extraction Performance
- 5.2 RAG System Performance
- 5.3 Statistical Significance
- 5.4 Ablation Study

**6. Discussion**
- 6.1 Key Findings
- 6.2 Limitations
- 6.3 Future Work

**7. Conclusion**

### 3.3 ì˜ˆìƒ ì‹¤í—˜ ê²°ê³¼

#### í‘œ 1: ì—”í‹°í‹° ì¶”ì¶œ ì„±ëŠ¥ ë¹„êµ

| Extractor | Precision | Recall | F1 | Linking Acc@1 |
|-----------|-----------|--------|----|--------------| 
| MedCAT | 0.82 | 0.76 | 0.79 | 0.68 |
| QuickUMLS | 0.75 | 0.81 | 0.78 | 0.72 |
| KM-BERT | 0.88 | 0.73 | 0.80 | N/A |
| Ensemble | 0.85 | 0.84 | 0.85 | 0.75 |

#### í‘œ 2: RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ

| Variant | Faithfulness | Answer Relevancy | Context Precision | Context Recall |
|---------|-------------|------------------|-------------------|----------------|
| Baseline | 0.45 Â± 0.12 | 0.52 Â± 0.15 | 0.38 Â± 0.18 | 0.42 Â± 0.16 |
| MedCAT RAG | 0.72 Â± 0.08 | 0.78 Â± 0.06 | 0.68 Â± 0.10 | 0.65 Â± 0.12 |
| Full System | 0.85 Â± 0.05 | 0.88 Â± 0.04 | 0.82 Â± 0.07 | 0.78 Â± 0.08 |

#### í‘œ 3: í†µê³„ì  ìœ ì˜ì„± ê²€ì •

| Comparison | Metric | t-statistic | p-value | Significant |
|------------|--------|-------------|---------|-------------|
| Baseline vs MedCAT | Faithfulness | 8.32 | < 0.001 | âœ“ |
| MedCAT vs Full | Faithfulness | 4.56 | < 0.01 | âœ“ |
| Baseline vs Full | Answer Relevancy | 10.24 | < 0.001 | âœ“ |

---

## ğŸ“Š Part 4: ì‹¤í–‰ ê³„íš ë° íƒ€ì„ë¼ì¸

### 4.1 êµ¬í˜„ íƒ€ì„ë¼ì¸

#### Week 1: Phase 1 êµ¬í˜„ (RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì´ì‹)

**Day 1-2**:
- [ ] í´ë” êµ¬ì¡° ìƒì„±
- [ ] `ragas_metrics.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] import ê²½ë¡œ ìˆ˜ì •

**Day 3-4**:
- [ ] `run_comparison.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] í˜„ì¬ ìŠ¤ìºí´ë“œì— ë§ê²Œ ë³€í˜• ì •ì˜ ìˆ˜ì •

**Day 5-6**:
- [ ] `evaluate_ragas.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] í†µê³„ ë¶„ì„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

**Day 7**:
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ë²„ê·¸ ìˆ˜ì •

#### Week 2: Phase 2 êµ¬í˜„ (í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬)

**Day 1-2**:
- [ ] í†µí•© ì‹¤í—˜ ëŸ¬ë„ˆ êµ¬í˜„
- [ ] ì—”í‹°í‹° â†’ ì»¨í…ìŠ¤íŠ¸ ë³€í™˜ ë¡œì§

**Day 3-4**:
- [ ] í†µí•© í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„
- [ ] í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚°

**Day 5-6**:
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ìµœì í™”

**Day 7**:
- [ ] ë¬¸ì„œí™”
- [ ] ê°€ì´ë“œ ì‘ì„±

#### Week 3: Phase 3 êµ¬í˜„ (ì‹¤í—˜ ë° ë¶„ì„)

**Day 1-3**:
- [ ] ì‹¤í—˜ ë°ì´í„°ì…‹ ì¤€ë¹„
- [ ] ì‹¤í—˜ ì‹¤í–‰ (3ê°€ì§€ ë³€í˜•)

**Day 4-5**:
- [ ] ê²°ê³¼ ë¶„ì„
- [ ] í†µê³„ì  ê²€ì •

**Day 6-7**:
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„±
- [ ] ê·¸ë˜í”„/í‘œ ìƒì„±

### 4.2 ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

#### ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤

- **CPU**: 8ì½”ì–´ ì´ìƒ
- **RAM**: 16GB ì´ìƒ
- **GPU**: ì„ íƒ (KM-BERT í•™ìŠµ ì‹œ í•„ìš”)
- **ì €ì¥ê³µê°„**: 50GB ì´ìƒ

#### API í¬ë ˆë”§

- **OpenAI API**: $50-100 (RAGAS í‰ê°€ìš©)
- **ì˜ˆìƒ ë¹„ìš©**: 
  - 5í„´ Ã— 3ë³€í˜• Ã— 5íšŒ ì‹¤í–‰ = 75í„´
  - 75í„´ Ã— $0.50/í„´ = $37.50

#### ì‹œê°„ ìš”êµ¬ì‚¬í•­

- **Phase 1**: 1ì£¼ (40ì‹œê°„)
- **Phase 2**: 1ì£¼ (40ì‹œê°„)
- **Phase 3**: 1ì£¼ (40ì‹œê°„)
- **ì´ ì†Œìš” ì‹œê°„**: 3ì£¼ (120ì‹œê°„)

### 4.3 ìœ„í—˜ ìš”ì†Œ ë° ì™„í™” ì „ëµ

#### ìœ„í—˜ 1: RAGAS í‰ê°€ ì†ë„

**ë¬¸ì œ**: RAGAS í‰ê°€ê°€ ëŠë¦¼ (í„´ë‹¹ 30-60ì´ˆ)  
**ì™„í™” ì „ëµ**:
- ìƒ˜í”Œë§ (ì „ì²´ í„´ì˜ 50%ë§Œ í‰ê°€)
- ìºì‹± (ë™ì¼í•œ ì§ˆë¬¸ ì¬ì‚¬ìš©)
- ë³‘ë ¬ ì²˜ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)

#### ìœ„í—˜ 2: OpenAI API ë¹„ìš©

**ë¬¸ì œ**: API ë¹„ìš© ì´ˆê³¼  
**ì™„í™” ì „ëµ**:
- ì˜ˆì‚° ì„¤ì • ($100 í•œë„)
- ì‹¤í—˜ ê·œëª¨ ì¶•ì†Œ (í•„ìš” ì‹œ)
- ë¬´ë£Œ ëŒ€ì•ˆ í™œìš© (GPT-3.5-turbo)

#### ìœ„í—˜ 3: í†µí•© ë³µì¡ë„

**ë¬¸ì œ**: ë‘ ì‹œìŠ¤í…œ í†µí•© ì‹œ ë²„ê·¸ ë°œìƒ  
**ì™„í™” ì „ëµ**:
- ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- ë¬¸ì„œí™” ì² ì €íˆ

---

## ğŸ” Part 5: ì´ì „ ìŠ¤ìºí´ë“œì—ì„œ ê°€ì ¸ì˜¬ ì¶”ê°€ ìš°ìˆ˜ ì„¤ê³„

### 5.1 Strategy Pattern ê¸°ë°˜ RAG ë³€í˜•

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„**:

```python
# agent/refine_strategies/base_strategy.py
class BaseRefineStrategy(ABC):
    @abstractmethod
    def refine(self, state: AgentState) -> Dict[str, Any]:
        pass

# agent/refine_strategies/basic_rag_strategy.py
class BasicRAGStrategy(BaseRefineStrategy):
    def refine(self, state: AgentState) -> Dict[str, Any]:
        # Basic RAG: í’ˆì§ˆ í‰ê°€ ì—†ì´ í†µê³¼
        return {'quality_score': 1.0, 'needs_retrieval': False}

# agent/refine_strategies/corrective_rag_strategy.py
class CorrectiveRAGStrategy(BaseRefineStrategy):
    def refine(self, state: AgentState) -> Dict[str, Any]:
        # Corrective RAG: í’ˆì§ˆ í‰ê°€ í›„ ì¬ê²€ìƒ‰ ê²°ì •
        quality_score = self._evaluate_quality(state)
        needs_retrieval = quality_score < 0.5
        return {'quality_score': quality_score, 'needs_retrieval': needs_retrieval}
```

**í˜„ì¬ ìŠ¤ìºí´ë“œ ì ìš© ë°©ì•ˆ**:

```python
# src/med_entity_ab/strategies/base_strategy.py (ì‹ ê·œ ìƒì„±)
from abc import ABC, abstractmethod
from typing import Dict, Any, List
from med_entity_ab.schema import Entity

class BaseExtractionStrategy(ABC):
    """ì—”í‹°í‹° ì¶”ì¶œ ì „ëµ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def extract(self, text: str) -> List[Entity]:
        """ì—”í‹°í‹° ì¶”ì¶œ"""
        pass
    
    @abstractmethod
    def get_strategy_name(self) -> str:
        """ì „ëµ ì´ë¦„ ë°˜í™˜"""
        pass

# src/med_entity_ab/strategies/medcat_strategy.py (ì‹ ê·œ ìƒì„±)
class MedCATStrategy(BaseExtractionStrategy):
    def __init__(self, modelpack_path: str):
        from medcat.cat import CAT
        self.cat = CAT.load_model_pack(modelpack_path)
    
    def extract(self, text: str) -> List[Entity]:
        # MedCAT ì¶”ì¶œ ë¡œì§
        pass
    
    def get_strategy_name(self) -> str:
        return "medcat"

# src/med_entity_ab/strategies/ensemble_strategy.py (ì‹ ê·œ ìƒì„±)
class EnsembleStrategy(BaseExtractionStrategy):
    """ì—¬ëŸ¬ ì¶”ì¶œê¸°ë¥¼ ì¡°í•©í•˜ëŠ” ì•™ìƒë¸” ì „ëµ"""
    
    def __init__(self, strategies: List[BaseExtractionStrategy]):
        self.strategies = strategies
    
    def extract(self, text: str) -> List[Entity]:
        # ëª¨ë“  ì „ëµ ì‹¤í–‰
        all_entities = []
        for strategy in self.strategies:
            entities = strategy.extract(text)
            all_entities.extend(entities)
        
        # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        merged_entities = self._merge_entities(all_entities)
        return merged_entities
    
    def _merge_entities(self, entities: List[Entity]) -> List[Entity]:
        # ì¤‘ë³µ ì—”í‹°í‹° ë³‘í•© ë¡œì§
        pass
    
    def get_strategy_name(self) -> str:
        return "ensemble"
```

**ì¥ì **:
- âœ… ìƒˆë¡œìš´ ì¶”ì¶œ ì „ëµ ì¶”ê°€ ìš©ì´
- âœ… ì•™ìƒë¸” ì „ëµ êµ¬í˜„ ê°„ë‹¨
- âœ… í…ŒìŠ¤íŠ¸ ìš©ì´ (ì „ëµë³„ ë…ë¦½ í…ŒìŠ¤íŠ¸)

### 5.2 LangGraph ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„**:

```python
# agent/graph.py
def build_agent_graph():
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("extract_slots", extract_slots_node)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate_answer", generate_answer_node)
    workflow.add_node("refine", refine_node)
    workflow.add_node("quality_check", quality_check_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.add_edge("extract_slots", "retrieve")
    workflow.add_edge("retrieve", "generate_answer")
    workflow.add_edge("generate_answer", "refine")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ (Self-Refine Loop)
    workflow.add_conditional_edges(
        "refine",
        quality_check_node,
        {
            "retrieve": "retrieve",  # ì¬ê²€ìƒ‰
            END: END  # ì¢…ë£Œ
        }
    )
    
    return workflow.compile()
```

**í˜„ì¬ ìŠ¤ìºí´ë“œ ì ìš© ë°©ì•ˆ**:

í˜„ì¬ ìŠ¤ìºí´ë“œëŠ” **ì—”í‹°í‹° ì¶”ì¶œ ë¹„êµ**ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆì–´, LangGraphëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
ëŒ€ì‹ , **ê°„ë‹¨í•œ íŒŒì´í”„ë¼ì¸ íŒ¨í„´**ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.

```python
# src/med_entity_ab/pipeline_v2.py (ì‹ ê·œ ìƒì„±)
class EntityExtractionPipeline:
    """ì—”í‹°í‹° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ (íŒŒì´í”„ë¼ì¸ íŒ¨í„´)"""
    
    def __init__(self, strategies: List[BaseExtractionStrategy]):
        self.strategies = strategies
    
    def run(self, text: str) -> Dict[str, List[Entity]]:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        results = {}
        
        for strategy in self.strategies:
            strategy_name = strategy.get_strategy_name()
            entities = strategy.extract(text)
            results[strategy_name] = entities
        
        return results
```

**ê²°ë¡ **: í˜„ì¬ ìŠ¤ìºí´ë“œì—ëŠ” LangGraphê°€ ê³¼ë„í•˜ë¯€ë¡œ, **ê°„ë‹¨í•œ íŒŒì´í”„ë¼ì¸ íŒ¨í„´**ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.

### 5.3 3-Tier Memory Architecture

**ì´ì „ ìŠ¤ìºí´ë“œ êµ¬í˜„**:

```python
# memory/profile_store.py
class ProfileStore:
    """3-Tier ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜"""
    
    def __init__(self):
        self.session_memory = {}    # Tier 1: ì„¸ì…˜ ë©”ëª¨ë¦¬
        self.profile_memory = {}     # Tier 2: í”„ë¡œí•„ ë©”ëª¨ë¦¬
        self.longterm_memory = {}    # Tier 3: ì¥ê¸° ë©”ëª¨ë¦¬
    
    def update_slots(self, slot_out: Dict):
        # ì„¸ì…˜ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        self.session_memory.update(slot_out)
    
    def apply_temporal_weights(self):
        # ì‹œê³„ì—´ ê°€ì¤‘ì¹˜ ì ìš©
        for key, value in self.profile_memory.items():
            value['weight'] *= 0.9  # ì‹œê°„ ê°ì‡ 
    
    def get_profile_summary(self) -> str:
        # í”„ë¡œí•„ ìš”ì•½ ìƒì„±
        return self._summarize_profile()
```

**í˜„ì¬ ìŠ¤ìºí´ë“œ ì ìš© ë°©ì•ˆ**:

í˜„ì¬ ìŠ¤ìºí´ë“œëŠ” **ë‹¨ì¼ í„´ ì—”í‹°í‹° ì¶”ì¶œ**ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆì–´, ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

**ê²°ë¡ **: í˜„ì¬ ìŠ¤ìºí´ë“œì—ëŠ” ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ê°€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.

---

## ğŸ“ Part 6: ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ì‹¤í–‰ ê°€ì´ë“œ

### 6.1 êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### Phase 1: RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì´ì‹

- [ ] `experiments/rag_evaluation/` í´ë” ìƒì„±
- [ ] `ragas_metrics.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] `run_comparison.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] `evaluate_ragas.py` ì´ì‹ ë° ìˆ˜ì •
- [ ] `requirements.txt` ì—…ë°ì´íŠ¸ (scipy ì¶”ê°€)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

#### Phase 2: í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬

- [ ] `experiments/unified_evaluation/` í´ë” ìƒì„±
- [ ] `run_unified_experiment.py` êµ¬í˜„
- [ ] `evaluate_unified.py` êµ¬í˜„
- [ ] `unified_metrics.py` êµ¬í˜„
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

#### Phase 3: ë¬¸ì„œí™” ë° ê°€ì´ë“œ

- [ ] `RAGAS_UNIFIED_EVALUATION_GUIDE.md` ì‘ì„±
- [ ] `251216_feedback_reaction1.md` ì‘ì„± (ë³¸ ë¬¸ì„œ)
- [ ] `README.md` ì—…ë°ì´íŠ¸
- [ ] ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

### 6.2 ì‹¤í–‰ ê°€ì´ë“œ

#### Step 1: í™˜ê²½ ì„¤ì •

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# .env íŒŒì¼ ì„¤ì •
cp env_template.txt .env
# .env íŒŒì¼ì— OPENAI_API_KEY ì¶”ê°€
```

#### Step 2: RAGAS í‰ê°€ ì‹¤í—˜ ì‹¤í–‰

```bash
# ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ (5í„´)
python experiments/rag_evaluation/run_comparison.py --turns 5

# RAGAS í‰ê°€
python experiments/rag_evaluation/evaluate_ragas.py \
    --log-dir experiments/comparison_logs/entity_extraction_20251216_120000
```

#### Step 3: í†µí•© í‰ê°€ ì‹¤í—˜ ì‹¤í–‰

```bash
# í†µí•© ì‹¤í—˜ ì‹¤í–‰
python experiments/unified_evaluation/run_unified_experiment.py --turns 5

# ê²°ê³¼ í™•ì¸
cat experiments/unified_evaluation/logs/unified_20251216_120000/summary.json
```

#### Step 4: ê²°ê³¼ ë¶„ì„

```bash
# í‰ê°€ ê²°ê³¼ í™•ì¸
cat experiments/comparison_logs/entity_extraction_20251216_120000/evaluation_results.json

# í†µê³„ ê²°ê³¼ í™•ì¸
cat experiments/comparison_logs/entity_extraction_20251216_120000/statistical_results.json
```

### 6.3 ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

#### ë¬¸ì œ 1: RAGAS í‰ê°€ ì‹¤íŒ¨

**ì¦ìƒ**: `calculate_ragas_metrics_full()` í•¨ìˆ˜ê°€ `None` ë°˜í™˜

**ì›ì¸**:
- OpenAI API í‚¤ ë¯¸ì„¤ì •
- RAGAS íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜
- ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ

**í•´ê²°**:
```bash
# API í‚¤ í™•ì¸
echo $OPENAI_API_KEY

# RAGAS ì„¤ì¹˜
pip install ragas>=0.1.0

# ë¡œê·¸ í™•ì¸
tail -f experiments/comparison_logs/*/evaluation.log
```

#### ë¬¸ì œ 2: í†µí•© ì‹¤í—˜ ì‹¤íŒ¨

**ì¦ìƒ**: `EntityABPipeline` ìƒì„± ì‹¤íŒ¨

**ì›ì¸**:
- MedCAT ëª¨ë¸íŒ© ê²½ë¡œ ì˜¤ë¥˜
- QuickUMLS ì¸ë±ìŠ¤ ê²½ë¡œ ì˜¤ë¥˜
- KM-BERT ëª¨ë¸ ê²½ë¡œ ì˜¤ë¥˜

**í•´ê²°**:
```bash
# ì„¤ì • íŒŒì¼ í™•ì¸
cat configs/default.yaml

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $MEDCAT_MODELPACK
echo $QUICKUMLS_INDEX_DIR
echo $KMBERT_NER_DIR
```

---

## ğŸ¯ ê²°ë¡ 

### ì£¼ìš” ì„±ê³¼

1. âœ… **ì‹¬ì‚¬ìœ„ì› í”¼ë“œë°± ì™„ì „ ë°˜ì˜**
   - RAG ì‹œìŠ¤í…œ ê°„ ë¹„êµ (3ê°€ì§€ ë³€í˜•)
   - RAGAS LLM as a Judge ë°©ì‹ í™œìš©
   - ì²´ê³„ì ì¸ ëŒ€í™” ë¡œê·¸ ìƒì„±

2. âœ… **ì´ì „ ìŠ¤ìºí´ë“œ ìš°ìˆ˜ ì„¤ê³„ í†µí•©**
   - RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì´ì‹
   - í†µê³„ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
   - Strategy Pattern ì ìš© ê°€ëŠ¥

3. âœ… **í˜„ì¬ ìŠ¤ìºí´ë“œ ë¬´ê²°ì„± ìœ ì§€**
   - ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´
   - ëª¨ë“ˆí™”ëœ ì¶”ê°€
   - í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

### ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 êµ¬í˜„** (1ì£¼)
   - RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì´ì‹
   - í…ŒìŠ¤íŠ¸ ë° ë²„ê·¸ ìˆ˜ì •

2. **Phase 2 êµ¬í˜„** (1ì£¼)
   - í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
   - í†µí•© í…ŒìŠ¤íŠ¸

3. **Phase 3 ì‹¤í—˜ ë° ë¶„ì„** (1ì£¼)
   - ì‹¤í—˜ ì‹¤í–‰
   - ê²°ê³¼ ë¶„ì„
   - ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„±

### ì˜ˆìƒ íš¨ê³¼

- **í•™ìˆ ì  ê¸°ì—¬**: ì˜í•™ ì—”í‹°í‹° ì¶”ì¶œ + RAG í†µí•© í‰ê°€ í”„ë ˆì„ì›Œí¬
- **ì‹¤ìš©ì  ê°€ì¹˜**: ì¬í˜„ ê°€ëŠ¥í•œ í‰ê°€ íŒŒì´í”„ë¼ì¸
- **ì—°êµ¬ ì‹ ë¢°ì„±**: í†µê³„ì  ìœ ì˜ì„± ê²€ì • + RAGAS LLM as a Judge

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 16ì¼  
**ì €ì¥ ìœ„ì¹˜**: `C:\Users\KHIDI\Downloads\final_medical_ai_agent\251216_feedback_reaction1.md`  
**ì˜ˆìƒ êµ¬í˜„ ì‹œê°„**: 3ì£¼ (120ì‹œê°„)  
**ì˜ˆìƒ ë¹„ìš©**: $50-100 (OpenAI API)

