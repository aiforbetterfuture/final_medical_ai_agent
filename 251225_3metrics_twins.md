# 251225 3-Metrics í‰ê°€ ì‹œìŠ¤í…œ í†µí•© ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 25ì¼
**ëª©ì **: faithfulness, answer_relevance, perplexity 3ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ LLM-as-a-Judge í‰ê°€ ì‹œìŠ¤í…œì˜ ì•ˆì •í™” ë° ìŠ¤í‚¤ë§ˆ ê³ ì •

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ 3-ë©”íŠ¸ë¦­ ì •ì˜](#í•µì‹¬-3-ë©”íŠ¸ë¦­-ì •ì˜)
3. [íŒŒì¼ êµ¬ì¡° ë° ì—­í• ](#íŒŒì¼-êµ¬ì¡°-ë°-ì—­í• )
4. [ì£¼ìš” ê°œì„  ì‚¬í•­](#ì£¼ìš”-ê°œì„ -ì‚¬í•­)
5. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
6. [ìì£¼ í„°ì§€ëŠ” ì§€ì  ë° í•´ê²°ì±…](#ìì£¼-í„°ì§€ëŠ”-ì§€ì -ë°-í•´ê²°ì±…)
7. [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)
8. [ë¬¸ì œ í•´ê²° ê°€ì´ë“œ](#ë¬¸ì œ-í•´ê²°-ê°€ì´ë“œ)

---

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ì˜ë£Œ AI ì—ì´ì „íŠ¸ì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ **3ê°œì˜ í•µì‹¬ ë©”íŠ¸ë¦­**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

1. **faithfulness** (ì‹ ë¢°ì„±): ë‹µë³€ì´ ê²€ìƒ‰ëœ TS ê·¼ê±°ë¡œ ë’·ë°›ì¹¨ë˜ëŠ”ê°€?
2. **answer_relevance** (ê´€ë ¨ì„±): ì§ˆë¬¸ì— ì‹¤ì œë¡œ ë‹µí•˜ëŠ”ê°€?
3. **perplexity** (ë³µì¡ë„): ë‹µë³€ì˜ ì–¸ì–´ì  ìì—°ìŠ¤ëŸ¬ì›€ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

ì´ ë¬¸ì„œëŠ” í‰ê°€ ì‹œìŠ¤í…œì˜ **ìŠ¤í‚¤ë§ˆ ì•ˆì •ì„±**ì„ í™•ë³´í•˜ê³ , ChatGPTê°€ ì§€ì í•œ "ìì£¼ í„°ì§€ëŠ” ì§€ì "ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•œ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## í•µì‹¬ 3-ë©”íŠ¸ë¦­ ì •ì˜

### 1. Faithfulness (ì‹ ë¢°ì„±)
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì˜ë¯¸**: ë‹µë³€ì´ ê²€ìƒ‰ëœ TS(Two-Stage) ê·¼ê±°ë¡œ ë’·ë°›ì¹¨ë˜ëŠ”ê°€?
- **í‰ê°€ ê¸°ì¤€**:
  - âœ… TS ê·¼ê±°ì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©
  - âŒ í™˜ê°(hallucination), ì¶”ì¸¡ â†’ í¬ê²Œ ê°ì 
  - âŒ TL íŒíŠ¸ë¥¼ ì‚¬ì‹¤ì²˜ëŸ¼ ë‹¨ì • â†’ ê°ì 
- **ì„ê³„ê°’**: ì¼ë°˜ì ìœ¼ë¡œ 0.75 ì´ìƒì´ë©´ PASS

### 2. Answer Relevance (ê´€ë ¨ì„±)
- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì˜ë¯¸**: ì‚¬ìš©ì ì§ˆë¬¸ì— ì‹¤ì œë¡œ ë‹µí•˜ëŠ”ê°€?
- **í‰ê°€ ê¸°ì¤€**:
  - âœ… ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì§ì ‘ ë‹µë³€
  - âŒ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ì¼ë°˜ë¡ ë§Œ ë‚˜ì—´
  - âŒ ì§ˆë¬¸ì˜ ì¼ë¶€ë§Œ ë‹µí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë¬´ì‹œ
- **ì„ê³„ê°’**: 0.75 ì´ìƒ

### 3. Perplexity (ë³µì¡ë„)
- **ë²”ìœ„**: float (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ì¼ë°˜ì ìœ¼ë¡œ 10~100)
- **ì˜ë¯¸**: ë‹µë³€ì˜ ì–¸ì–´ì  ìì—°ìŠ¤ëŸ¬ì›€ / ì˜ˆì¸¡ ê°€ëŠ¥ì„±
- **ê³„ì‚° ë°©ë²•**:
  - `transformers` + `torch` ìˆìœ¼ë©´: ë¡œì»¬ì—ì„œ causal LMìœ¼ë¡œ ê³„ì‚° (ê¸°ë³¸: `distilgpt2`)
  - ì—†ìœ¼ë©´: `-1.0`ìœ¼ë¡œ ê¸°ë¡í•˜ê³  ì´ìœ  ë‚¨ê¹€
- **ì‚¬ìš© ëª©ì **: ë¬¸ë²•/ì–´ìƒ‰í•¨ ê°ì§€, ê³¼ë„í•œ ë°˜ë³µ ë°©ì§€

### ì¶”ê°€ ë©”íŠ¸ë¦­ (ì„ íƒ)
- **context_use** (ë§¥ë½ í™œìš©): í™˜ì ì •ë³´(ë‚˜ì´/ì„±ë³„/ë³‘ë ¥/ë³µì•½ ë“±)ë¥¼ ì ì ˆíˆ í™œìš©í•˜ëŠ”ê°€? (0.0~1.0)

---

## íŒŒì¼ êµ¬ì¡° ë° ì—­í• 

### 1. `configs/eval_rubric.yaml`
**ì—­í• **: í‰ê°€ ì„¤ì •ì˜ **ë‹¨ì¼ ì§„ì‹¤ì›(SSOT, Single Source of Truth)**

```yaml
llm_judge:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0
  max_tokens: 800
  timeout_s: 60
  threshold: 0.75

  system_prompt: |
    ë‹¹ì‹ ì€ ì˜ë£Œ QA í’ˆì§ˆ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
    ê·¼ê±°(TS evidence)ë§Œì„ ì‹ ë¢°í•˜ë©°, TL íŒíŠ¸ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë´…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    - faithfulness: TS ê·¼ê±°ë¡œ ë’·ë°›ì¹¨ë˜ëŠ”ê°€?
    - answer_relevance: ì§ˆë¬¸ì— ì‹¤ì œë¡œ ë‹µí•˜ëŠ”ê°€?
    - context_use: í™˜ì ë§¥ë½ì„ í™œìš©í•˜ëŠ”ê°€?

  scoring_criteria:
    - key: faithfulness
      desc: "TS ê·¼ê±°ì™€ ì¼ì¹˜/ì§€ì§€ ì—¬ë¶€"
    - key: answer_relevance
      desc: "ì§ˆë¬¸-ë‹µë³€ ì í•©ì„±"
    - key: context_use
      desc: "í™˜ì/ëŒ€í™” ë§¥ë½ í™œìš©"

perplexity:
  enabled: true
  model: distilgpt2
  env_var: HF_PERPLEXITY_MODEL
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- âœ… ëª¨ë“  ì„¤ì •ì´ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ë¨
- âœ… YAML êµ¬ì¡°ê°€ ëª…í™• (í•œ ì¤„ ì••ì¶• ê¸ˆì§€)
- âœ… ë©”íŠ¸ë¦­ í‚¤ê°€ ì½”ë“œì™€ ì •í™•íˆ ì¼ì¹˜

### 2. `configs/question_templates.yaml`
**ì—­í• **: í‰ê°€ì…‹ ì¼€ì´ìŠ¤ ì •ì˜ ë° í•œì˜ ì–‘ì–¸ì–´ ì§€ì›

```yaml
bilingual:
  enabled: true
  glossary:
    - canonical: HbA1c
      ko: [HbA1c, ë‹¹í™”í˜ˆìƒ‰ì†Œ, í—¤ëª¨ê¸€ë¡œë¹ˆ A1c]
      en: [HbA1c, hemoglobin A1c]
    - canonical: diabetes mellitus
      ko: [ë‹¹ë‡¨, ë‹¹ë‡¨ë³‘, ë‹¹ë‡¨ë³‘ì„±]
      en: [diabetes, diabetes mellitus]

cases:
  - case_id: DM_A1C_TARGET_basic
    domain_id: 3
    q_type: 2
    difficulty: easy
    turns:
      - turn_id: 1
        utterance: ë‹¹ë‡¨ í™˜ì HbA1c ëª©í‘œ ë²”ìœ„ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
        utterance_en: What is the target HbA1c range for diabetes patients?
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- âœ… ì–‘ì–¸ì–´(í•œ/ì˜) glossaryë¡œ ìŠ¬ë¡¯ ë§¤ì¹­ ê°•í™”
- âœ… Case êµ¬ì¡°ê°€ ì¼ê´€ë¨ (turn_id, utterance, expected_slot_updates ë“±)
- âœ… í‰ê°€ ì‹œ ì–¸ì–´ ì°¨ì´ë¡œ ì¸í•œ ì˜¤ë¥˜ ë°©ì§€

### 3. `tools/llm_as_judge.py`
**ì—­í• **: LLM ê¸°ë°˜ í‰ê°€ ì‹¤í–‰ + Perplexity ê³„ì‚°

**ì£¼ìš” ê°œì„  ì‚¬í•­**:
```python
@dataclass
class LLMJudgeConfig:
    """eval_rubric.yamlì—ì„œ ì„¤ì • ë¡œë“œ ê°€ëŠ¥"""

    @classmethod
    def from_rubric(cls, rubric_path: str) -> "LLMJudgeConfig":
        """rubric YAML ìë™ ë¡œë“œ"""
        # configs/eval_rubric.yamlì˜ llm_judge ì„¹ì…˜ ì½ê¸°
        ...

def judge_one(
    *,
    question: str,
    answer: str,
    evidence: str,
    rubric_path: Optional[str] = None,  # ğŸ†• ì¶”ê°€ë¨
    cfg: Optional[LLMJudgeConfig] = None,
) -> Dict[str, Any]:
    """
    Returns:
        {
          "scores": {"faithfulness": float, "answer_relevance": float, "context_use": float},
          "perplexity": float,
          "perplexity_ok": bool,
          "verdict": "pass|fail|skip",
          ...
        }
    """
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- âœ… `rubric_path` íŒŒë¼ë¯¸í„°ë¡œ YAML ìë™ ë¡œë“œ
- âœ… í•˜ìœ„ í˜¸í™˜ alias ë§¤í•‘ (factuality â†’ faithfulness, relevance â†’ answer_relevance ë“±)
- âœ… Non-JSON ì¶œë ¥ ë°©ì–´ (```json ... ``` íœìŠ¤ ë¸”ë¡ íŒŒì‹±)
- âœ… Perplexity ì˜ì¡´ì„± ë¶€ì¬ ì‹œì—ë„ ê³„ì† ì§„í–‰ (perplexity=-1.0 ê¸°ë¡)

---

## ì£¼ìš” ê°œì„  ì‚¬í•­

### 1. ìŠ¤í‚¤ë§ˆ ë“œë¦¬í”„íŠ¸ ë°©ì§€ (Schema Drift Prevention)

**ë¬¸ì œ**: LLMì´ ê°€ë” ë‹¤ë¥¸ í‚¤ë¡œ ì‘ë‹µ (ì˜ˆ: `factuality` ëŒ€ì‹  `faithfulness`)

**í•´ê²°**:
```python
alias_map = {
    "factuality": "faithfulness",      # ë ˆê±°ì‹œ í‚¤
    "relevance": "answer_relevance",   # ë ˆê±°ì‹œ í‚¤
    "completeness": "answer_relevance", # ì¼ë¶€ ëª¨ë¸
    "faithfulness": "faithfulness",    # í‘œì¤€ í‚¤
    "answer_relevance": "answer_relevance",
    "context_use": "context_use",
}

# scores dict + top-level fields ì–‘ìª½ì—ì„œ ì¶”ì¶œ
for k, v in scores.items():
    dst = alias_map.get(k)
    if dst:
        norm_scores[dst] = _clamp01(v)
```

### 2. Non-JSON ì¶œë ¥ ë°©ì–´

**ë¬¸ì œ**: LLMì´ ë§ˆí¬ë‹¤ìš´, ì„¤ëª…ë¬¸, ì½”ë“œë¸”ë¡ ë“±ê³¼ í•¨ê»˜ JSON ë°˜í™˜

**í•´ê²°**:
```python
def _extract_first_json_object(text: str) -> Optional[dict]:
    """
    1) ì§ì ‘ json.loads
    2) ```json ... ``` íœìŠ¤ ë¸”ë¡ ì°¾ê¸°
    3) ì²« {...} ë¸”ë¡ ì°¾ê¸°
    """
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ JSON ì¶”ì¶œ
    m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.DOTALL)
    ...
```

### 3. Perplexity ì˜ì¡´ì„± ìš°ì•„í•œ ì²˜ë¦¬

**ë¬¸ì œ**: `transformers`/`torch` ë¯¸ì„¤ì¹˜ ì‹œ perplexity ê³„ì‚° ì‹¤íŒ¨ â†’ ì „ì²´ í‰ê°€ ì¤‘ë‹¨

**í•´ê²°**:
```python
def compute_perplexity(text: str, model_name: Optional[str] = None) -> Dict[str, Any]:
    try:
        ppl, src = _try_compute_perplexity_hf(text, model_name)
        return {"perplexity": ppl, "perplexity_ok": True, ...}
    except Exception as e:
        return {
            "perplexity": -1.0,
            "perplexity_source": f"unavailable:{type(e).__name__}",
            "perplexity_ok": False,
        }
```

### 4. YAML í•œ ì¤„ ì••ì¶• ê¸ˆì§€

**ë¬¸ì œ (Before)**:
```yaml
llm_judge: enabled: true model: gpt-4o-mini temperature: 0.0 ...
```
â†’ íŒŒì‹±ì€ ë˜ì§€ë§Œ diff/ë¦¬ë·°/ìœ ì§€ë³´ìˆ˜ ì•…ëª½

**í•´ê²° (After)**:
```yaml
llm_judge:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0
  max_tokens: 800
  timeout_s: 60
```

### 5. Rubric ê¸°ë°˜ ì„¤ì • ë¡œë“œ

**Before**: ì½”ë“œì— í•˜ë“œì½”ë”©
```python
cfg = LLMJudgeConfig(model="gpt-4o-mini", temperature=0.0)
```

**After**: YAMLì—ì„œ ìë™ ë¡œë“œ
```python
cfg = LLMJudgeConfig.from_rubric("configs/eval_rubric.yaml")
# ë˜ëŠ”
result = judge_one(
    question=q,
    answer=a,
    evidence=ev,
    rubric_path="configs/eval_rubric.yaml"  # ìë™ ë¡œë“œ
)
```

---

## ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (Rubric ìë™ ë¡œë“œ)

```python
from tools.llm_as_judge import judge_one

result = judge_one(
    question="ë‹¹ë‡¨ í™˜ì HbA1c ëª©í‘œ ë²”ìœ„ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    answer="ì¼ë°˜ì ìœ¼ë¡œ 7% ë¯¸ë§Œì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.",
    evidence="[TS evidence] HbA1c ëª©í‘œëŠ” 7% ë¯¸ë§Œ... (ê·¼ê±° í…ìŠ¤íŠ¸)",
    rubric_path="configs/eval_rubric.yaml"
)

print(result)
# {
#   "scores": {
#     "faithfulness": 0.85,
#     "answer_relevance": 0.90,
#     "context_use": 0.60
#   },
#   "perplexity": 15.3,
#   "perplexity_ok": True,
#   "perplexity_source": "hf:distilgpt2@cpu",
#   "verdict": "pass",
#   "rationale": "TS ê·¼ê±°ì™€ ì˜ ì¼ì¹˜í•¨",
#   "raw_text": "{...}"
# }
```

### 2. Grade Run íŒŒì´í”„ë¼ì¸ í†µí•©

```bash
python tools/grade_run.py \
  --pipeline \
  --evalset "experiments/retrieval_tuning/eval_tl.jsonl" \
  --rubric "configs/eval_rubric.yaml" \
  --run "experiments/eval_runs/run.jsonl" \
  --out "experiments/eval_runs/grades.jsonl"
```

ë‚´ë¶€ì—ì„œ `llm_as_judge.judge_one(..., rubric_path=args.rubric)`ë¡œ í˜¸ì¶œ

### 3. Perplexity ì„¤ì •

#### Option 1: í™˜ê²½ ë³€ìˆ˜
```bash
export HF_PERPLEXITY_MODEL=gpt2  # ë˜ëŠ” distilgpt2, gpt2-medium ë“±
python tools/grade_run.py ...
```

#### Option 2: YAML ì„¤ì •
```yaml
perplexity:
  enabled: true
  model: gpt2  # distilgpt2ë³´ë‹¤ í¬ì§€ë§Œ ì •í™•
```

#### Option 3: ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install transformers torch

# GPU ì‚¬ìš© (ì„ íƒ)
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

---

## ìì£¼ í„°ì§€ëŠ” ì§€ì  ë° í•´ê²°ì±…

### 1. ìŠ¤í‚¤ë§ˆ ë“œë¦¬í”„íŠ¸ (Schema Drift)

**ì¦ìƒ**:
```
KeyError: 'faithfulness'  # LLMì´ 'factuality'ë¡œ ë°˜í™˜
```

**ì›ì¸**: LLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë¬´ì‹œí•˜ê³  ë‹¤ë¥¸ í‚¤ ì´ë¦„ ì‚¬ìš©

**í•´ê²°ì±…**:
- âœ… `alias_map`ìœ¼ë¡œ ì—¬ëŸ¬ ë³€í˜• ìˆ˜ìš©
- âœ… `system_prompt`ì—ì„œ "ì •í™•í•œ ìŠ¤í‚¤ë§ˆ" ê°•ì¡°
- âœ… Temperature=0.0ìœ¼ë¡œ ì¼ê´€ì„± í–¥ìƒ

### 2. Non-JSON ì¶œë ¥

**ì¦ìƒ**:
```
json.JSONDecodeError: Expecting value: line 1 column 1
```

**ì›ì¸**: LLMì´ ì„¤ëª…ë¬¸ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ê³¼ í•¨ê»˜ JSON ë°˜í™˜
```
Here's the evaluation:
```json
{"scores": ...}
```
```

**í•´ê²°ì±…**:
- âœ… `_extract_first_json_object()`: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ JSON ì¶”ì¶œ
- âœ… System promptì— "ONLY valid JSON. No markdown." ëª…ì‹œ

### 3. Perplexity ì˜ì¡´ì„± ë¯¸ì„¤ì¹˜

**ì¦ìƒ**:
```
ModuleNotFoundError: No module named 'transformers'
```

**ì›ì¸**: `transformers`/`torch` ë¯¸ì„¤ì¹˜

**í•´ê²°ì±…**:
- âœ… ì˜ˆì™¸ ì²˜ë¦¬ë¡œ `perplexity=-1.0` ê¸°ë¡ í›„ ê³„ì† ì§„í–‰
- âœ… `perplexity_ok: false` í”Œë˜ê·¸ë¡œ ìœ íš¨ì„± í‘œì‹œ
- âœ… ì‚¬ìš©ìê°€ ë‚˜ì¤‘ì— ì„¤ì¹˜ ê°€ëŠ¥ (ì„ íƒì )

### 4. YAML íŒŒì‹± ì˜¤ë¥˜

**ì¦ìƒ**:
```
yaml.scanner.ScannerError: mapping values are not allowed here
```

**ì›ì¸**: í•œ ì¤„ì— ì—¬ëŸ¬ í‚¤ ì••ì¶•
```yaml
llm_judge: enabled: true model: gpt-4o-mini  # âŒ ì˜ëª»ë¨
```

**í•´ê²°ì±…**:
```yaml
llm_judge:
  enabled: true
  model: gpt-4o-mini  # âœ… ì˜¬ë°”ë¦„
```

### 5. í•œì˜ ë§¤ì¹­ ì‹¤íŒ¨

**ì¦ìƒ**: "HbA1c"ì™€ "ë‹¹í™”í˜ˆìƒ‰ì†Œ"ë¥¼ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì¸ì‹

**ì›ì¸**: ì–‘ì–¸ì–´ alias ë¶€ì¬

**í•´ê²°ì±…**:
```yaml
bilingual:
  glossary:
    - canonical: HbA1c
      ko: [HbA1c, ë‹¹í™”í˜ˆìƒ‰ì†Œ, í—¤ëª¨ê¸€ë¡œë¹ˆ A1c]
      en: [HbA1c, hemoglobin A1c]
```

---

## í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 1. Import í…ŒìŠ¤íŠ¸

```bash
python -c "from tools.llm_as_judge import judge_one, LLMJudgeConfig; print('OK')"
```

### 2. Rubric ë¡œë“œ í…ŒìŠ¤íŠ¸

```python
from tools.llm_as_judge import LLMJudgeConfig

cfg = LLMJudgeConfig.from_rubric("configs/eval_rubric.yaml")
print(f"Model: {cfg.model}")
print(f"Threshold: {cfg.threshold}")
print(f"Enabled: {cfg.enabled}")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
Model: gpt-4o-mini
Threshold: 0.75
Enabled: True
```

### 3. Perplexity ê³„ì‚° í…ŒìŠ¤íŠ¸

```python
from tools.llm_as_judge import compute_perplexity

result = compute_perplexity("ë‹¹ë‡¨ í™˜ìëŠ” í˜ˆë‹¹ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.")
print(result)
```

**ì˜ˆìƒ ì¶œë ¥** (transformers ì„¤ì¹˜ ì‹œ):
```python
{
  'perplexity': 23.45,
  'perplexity_source': 'hf:distilgpt2@cpu',
  'perplexity_ok': True
}
```

**ì˜ˆìƒ ì¶œë ¥** (transformers ë¯¸ì„¤ì¹˜ ì‹œ):
```python
{
  'perplexity': -1.0,
  'perplexity_source': 'unavailable:ModuleNotFoundError',
  'perplexity_ok': False
}
```

### 4. End-to-End í‰ê°€ í…ŒìŠ¤íŠ¸

```python
from tools.llm_as_judge import judge_one

result = judge_one(
    question="ë©”íŠ¸í¬ë¥´ë¯¼ ë¶€ì‘ìš©ì´ ë­”ê°€ìš”?",
    answer="ë©”íŠ¸í¬ë¥´ë¯¼ì˜ ì£¼ìš” ë¶€ì‘ìš©ì€ ìœ„ì¥ ì¥ì• ì…ë‹ˆë‹¤.",
    evidence="[TS] ë©”íŠ¸í¬ë¥´ë¯¼ì€ ìœ„ì¥ê´€ ë¶€ì‘ìš©(ì„¤ì‚¬, ë³µí†µ)ì´ í”í•©ë‹ˆë‹¤.",
    rubric_path="configs/eval_rubric.yaml"
)

assert result['verdict'] in ['pass', 'fail', 'skip']
assert 0.0 <= result['scores']['faithfulness'] <= 1.0
assert 'perplexity' in result
print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼")
```

---

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### Q1: "LLMì´ ê³„ì† factuality í‚¤ë¥¼ ë°˜í™˜í•´ìš”"

**A**: ì •ìƒì…ë‹ˆë‹¤. `alias_map`ì´ ìë™ìœ¼ë¡œ `faithfulness`ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.

í™•ì¸:
```python
result = judge_one(...)
print(result['scores'])  # {'faithfulness': 0.8, ...}  â† ì •ìƒ ë³€í™˜ë¨
```

### Q2: "Perplexityê°€ í•­ìƒ -1.0ì´ì—ìš”"

**A**: `transformers`/`torch` ë¯¸ì„¤ì¹˜ì…ë‹ˆë‹¤. ì„ íƒì‚¬í•­ì´ë¯€ë¡œ ê³„ì† ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì„¤ì¹˜ (ì„ íƒ):
```bash
pip install transformers torch
```

### Q3: "Rubric ë¡œë“œ ì‹¤íŒ¨ ê²½ê³ ê°€ ë– ìš”"

**A**: YAML ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.

```python
import os
rubric_path = "configs/eval_rubric.yaml"
print(f"Exists: {os.path.exists(rubric_path)}")  # Trueì—¬ì•¼ í•¨
```

### Q4: "í•œêµ­ì–´ ì§ˆë¬¸ì— ì˜ì–´ë¡œ ë‹µí•´ë„ ì ìˆ˜ê°€ ë‚®ì•„ìš”"

**A**: `system_prompt`ì— ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤:
```
â€» í•œêµ­ì–´/ì˜ì–´ê°€ ì„ì—¬ë„ ì–¸ì–´ ìì²´ë¡œ ê°ì í•˜ì§€ ë§ê³ , ì˜ë¯¸/ì •í™•ì„±ìœ¼ë¡œë§Œ í‰ê°€í•˜ì„¸ìš”.
```

LLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²½ìš° temperatureë¥¼ ë” ë‚®ì¶”ê±°ë‚˜ (0.0), few-shot ì˜ˆì œ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.

### Q5: "Grade runì´ ì‹¤íŒ¨í•´ìš”"

**A**: ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

1. **í•„ìˆ˜ íŒŒì¼ ì¡´ì¬**:
   ```bash
   ls configs/eval_rubric.yaml
   ls configs/question_templates.yaml
   ```

2. **Agent import ë¬¸ì œ** (ì´ì „ ì´ìŠˆ):
   ```bash
   python -c "from agent.entrypoint import run_agent; import agent.graph; print('OK')"
   ```

3. **LLM API í‚¤**:
   ```bash
   echo $OPENAI_API_KEY  # ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
   ```

---

## ë‹¤ìŒ ë‹¨ê³„

### 1. í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```bash
# tests/test_llm_judge.sh
#!/bin/bash
set -e

echo "1. Import í…ŒìŠ¤íŠ¸..."
python -c "from tools.llm_as_judge import judge_one; print('âœ… Import OK')"

echo "2. Rubric ë¡œë“œ í…ŒìŠ¤íŠ¸..."
python -c "
from tools.llm_as_judge import LLMJudgeConfig
cfg = LLMJudgeConfig.from_rubric('configs/eval_rubric.yaml')
print(f'âœ… Loaded: {cfg.model}')
"

echo "3. Perplexity í…ŒìŠ¤íŠ¸..."
python -c "
from tools.llm_as_judge import compute_perplexity
result = compute_perplexity('í…ŒìŠ¤íŠ¸ ë¬¸ì¥')
print(f'âœ… Perplexity: {result}')
"

echo "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!"
```

### 2. CI/CD í†µí•©

```yaml
# .github/workflows/test.yml
name: LLM Judge Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - run: pip install -r requirements.txt
      - run: bash tests/test_llm_judge.sh
```

### 3. ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
# tools/analyze_grades.py
import json
import pandas as pd

with open("experiments/eval_runs/grades.jsonl") as f:
    grades = [json.loads(line) for line in f]

df = pd.DataFrame([
    {
        'question_id': g['question_id'],
        'faithfulness': g['scores']['faithfulness'],
        'answer_relevance': g['scores']['answer_relevance'],
        'perplexity': g['perplexity'],
        'verdict': g['verdict']
    }
    for g in grades
])

print(df.describe())
print(f"\nPass rate: {(df['verdict']=='pass').mean():.1%}")
```

---

## ê²°ë¡ 

ì´ ë¬¸ì„œëŠ” **faithfulness, answer_relevance, perplexity** 3ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ í‰ê°€ ì‹œìŠ¤í…œì˜ ì•ˆì •í™” ë°©ë²•ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼
- âœ… ìŠ¤í‚¤ë§ˆ ë“œë¦¬í”„íŠ¸ ë°©ì§€ (alias ë§¤í•‘)
- âœ… Non-JSON ì¶œë ¥ ë°©ì–´ (ì •ê·œí‘œí˜„ì‹ ì¶”ì¶œ)
- âœ… Perplexity ì˜ì¡´ì„± ìš°ì•„í•œ ì²˜ë¦¬
- âœ… YAML ê¸°ë°˜ ì„¤ì • ìë™ ë¡œë“œ
- âœ… í•œì˜ ì–‘ì–¸ì–´ ì§€ì› ê°•í™”

### ìœ ì§€ë³´ìˆ˜ ì›ì¹™
1. **SSOT**: `configs/eval_rubric.yaml`ì´ ëª¨ë“  ì„¤ì •ì˜ ë‹¨ì¼ ì§„ì‹¤ì›
2. **í•˜ìœ„ í˜¸í™˜**: ë ˆê±°ì‹œ í‚¤ë„ aliasë¡œ ìˆ˜ìš©
3. **ë°©ì–´ì  í”„ë¡œê·¸ë˜ë°**: ì™¸ë¶€ ì…ë ¥(LLM ì¶œë ¥, YAML) í•­ìƒ ê²€ì¦
4. **ëª…ì‹œì  ì—ëŸ¬**: ì‹¤íŒ¨ ì‹œ ì´ìœ ë¥¼ ëª…í™•íˆ ê¸°ë¡

---

**ë¬¸ì˜ ë° í”¼ë“œë°±**: ì´ìŠˆê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ ì²¨ë¶€í•˜ì—¬ ë³´ê³ í•´ì£¼ì„¸ìš”:
- `configs/eval_rubric.yaml` ë‚´ìš©
- `llm_as_judge` í˜¸ì¶œ ì½”ë“œ
- ì „ì²´ ì—ëŸ¬ ë©”ì‹œì§€ ë° ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
- `perplexity_ok` ê°’ (True/False)
