# Modular RAG ì „ëµ ë° ì•„í‚¤í…ì²˜ ì¢…í•© ë¶„ì„

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 15ì¼  
**ëª©ì **: Basic RAG, Modular RAG, Corrective RAG ëª¨ë“ˆì‹ ì „í™˜ ë° LangGraph ì•„í‚¤í…ì²˜ í‰ê°€

---

## ğŸ“‹ ëª©ì°¨

1. [Modular RAG ê°œìš” ë° ì „ëµ](#1-modular-rag-ê°œìš”-ë°-ì „ëµ)
2. [ì„ í–‰ ì‘ì—… ì™„ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸](#2-ì„ í–‰-ì‘ì—…-ì™„ì „-ì²´í¬ë¦¬ìŠ¤íŠ¸)
3. [RAG ë³€í˜•ë³„ êµ¬í˜„ ìš”êµ¬ì‚¬í•­](#3-rag-ë³€í˜•ë³„-êµ¬í˜„-ìš”êµ¬ì‚¬í•­)
4. [LangGraph vs ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ì‹¬ì¸µ ë¶„ì„](#4-langgraph-vs-ëŒ€ì•ˆ-ì•„í‚¤í…ì²˜-ì‹¬ì¸µ-ë¶„ì„)
5. [ìµœì¢… ê¶Œì¥ì‚¬í•­ ë° ë¡œë“œë§µ](#5-ìµœì¢…-ê¶Œì¥ì‚¬í•­-ë°-ë¡œë“œë§µ)

---

## 1. Modular RAG ê°œìš” ë° ì „ëµ

### 1.1 RAG ì§„í™” ë‹¨ê³„

```
Generation 1: Basic RAG (2020-2021)
  â””â”€ Query â†’ Retrieve â†’ Generate
     ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸, ê³ ì •ëœ íë¦„

Generation 2: Advanced RAG (2022-2023)
  â””â”€ Pre-retrieval + Retrieval + Post-retrieval
     ì¿¼ë¦¬ ì¬ì‘ì„±, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ë¦¬ë­í‚¹

Generation 3: Modular RAG (2023-2024) â­
  â””â”€ Pluggable Modules + Orchestration
     ëª¨ë“ˆ ì¡°í•© ììœ , ë™ì  ë¼ìš°íŒ…, ì ì‘í˜• ì²˜ë¦¬

Generation 4: Agentic RAG (2024-) ğŸš€
  â””â”€ Self-Reflection + Tool Use + Planning
     ììœ¨ ì—ì´ì „íŠ¸, ë„êµ¬ í™œìš©, ë©€í‹° ìŠ¤í… ì¶”ë¡ 
```

### 1.2 Modular RAGì˜ í•µì‹¬ ê°œë…

**ì •ì˜**: RAG ì‹œìŠ¤í…œì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„í•´í•˜ì—¬ ì¡°í•© ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ì„¤ê³„ íŒ¨ëŸ¬ë‹¤ì„

**í•µì‹¬ ì›ì¹™**:
1. **Module Independence**: ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ êµì²´ ê°€ëŠ¥
2. **Interface Standardization**: í‘œì¤€í™”ëœ ì…ì¶œë ¥ ì¸í„°í˜ì´ìŠ¤
3. **Dynamic Composition**: ëŸ°íƒ€ì„ì— ëª¨ë“ˆ ì¡°í•© ë³€ê²½ ê°€ëŠ¥
4. **Pluggability**: ìƒˆë¡œìš´ ëª¨ë“ˆ ì¶”ê°€ê°€ ìš©ì´

**ëª¨ë“ˆ ë¶„ë¥˜**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Modular RAG Architecture              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Pre-Retrieval Modules                  â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Query Rewriter                       â”‚   â”‚
â”‚  â”‚  - Query Decomposer                     â”‚   â”‚
â”‚  â”‚  - Query Router                         â”‚   â”‚
â”‚  â”‚  - HyDE (Hypothetical Document)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Retrieval Modules                      â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Dense Retriever (FAISS)              â”‚   â”‚
â”‚  â”‚  - Sparse Retriever (BM25)              â”‚   â”‚
â”‚  â”‚  - Hybrid Retriever (RRF)               â”‚   â”‚
â”‚  â”‚  - Graph Retriever (Neo4j)              â”‚   â”‚
â”‚  â”‚  - SQL Retriever (Structured Data)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Post-Retrieval Modules                 â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Reranker (Cross-encoder)             â”‚   â”‚
â”‚  â”‚  - Compressor (LLMLingua)               â”‚   â”‚
â”‚  â”‚  - Filter (Relevance Threshold)         â”‚   â”‚
â”‚  â”‚  - Deduplicator                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Generation Modules                     â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Generator (LLM)                      â”‚   â”‚
â”‚  â”‚  - Summarizer                           â”‚   â”‚
â”‚  â”‚  - Fact Checker                         â”‚   â”‚
â”‚  â”‚  - Citation Adder                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â†“                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Evaluation & Correction Modules        â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Quality Evaluator                    â”‚   â”‚
â”‚  â”‚  - Hallucination Detector               â”‚   â”‚
â”‚  â”‚  - Self-Refine Loop                     â”‚   â”‚
â”‚  â”‚  - Feedback Collector                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Basic RAG vs Modular RAG vs Corrective RAG ë¹„êµ

| íŠ¹ì„± | Basic RAG | Modular RAG | Corrective RAG (CRAG) |
|-----|-----------|-------------|----------------------|
| **êµ¬ì¡°** | ê³ ì • íŒŒì´í”„ë¼ì¸ | ëª¨ë“ˆ ì¡°í•© | í‰ê°€ + êµì • ë£¨í”„ |
| **ë³µì¡ë„** | ë‚®ìŒ â­ | ì¤‘ê°„ â­â­ | ë†’ìŒ â­â­â­ |
| **ìœ ì—°ì„±** | ì—†ìŒ | ë†’ìŒ | ì¤‘ê°„ |
| **ì„±ëŠ¥** | ê¸°ë³¸ | í–¥ìƒ (+10-20%) | ìµœê³  (+20-40%) |
| **êµ¬í˜„ ë‚œì´ë„** | ì‰¬ì›€ | ì¤‘ê°„ | ì–´ë ¤ì›€ |
| **ìœ ì§€ë³´ìˆ˜** | ì–´ë ¤ì›€ | ì‰¬ì›€ | ì¤‘ê°„ |
| **Ablation ìš©ì´ì„±** | ë‚®ìŒ | ë§¤ìš° ë†’ìŒ â­â­â­ | ì¤‘ê°„ |

**Basic RAG**:
```python
def basic_rag(query):
    docs = retrieve(query, k=5)
    answer = generate(query, docs)
    return answer
```

**Modular RAG**:
```python
def modular_rag(query, modules):
    # ë™ì  ëª¨ë“ˆ ì¡°í•©
    query = modules['pre_retrieval'](query)
    docs = modules['retrieval'](query)
    docs = modules['post_retrieval'](docs)
    answer = modules['generation'](query, docs)
    answer = modules['evaluation'](answer)
    return answer
```

**Corrective RAG (CRAG)**:
```python
def corrective_rag(query, max_iterations=3):
    for i in range(max_iterations):
        docs = retrieve(query, k=10)
        
        # ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
        relevance_scores = evaluate_relevance(query, docs)
        
        if all(score > threshold for score in relevance_scores):
            # ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ì„± ë†’ìŒ
            answer = generate(query, docs)
            break
        elif any(score > threshold for score in relevance_scores):
            # ì¼ë¶€ ë¬¸ì„œë§Œ ê´€ë ¨ì„± ë†’ìŒ
            filtered_docs = filter_by_score(docs, relevance_scores)
            answer = generate(query, filtered_docs)
            break
        else:
            # ëª¨ë“  ë¬¸ì„œ ê´€ë ¨ì„± ë‚®ìŒ â†’ ì›¹ ê²€ìƒ‰ ë˜ëŠ” ì¿¼ë¦¬ ì¬ì‘ì„±
            query = rewrite_query(query, docs)
            if i == max_iterations - 1:
                answer = generate_with_web_search(query)
    
    return answer
```

---

## 2. ì„ í–‰ ì‘ì—… ì™„ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 2.1 Phase 0: ì•„í‚¤í…ì²˜ ì„¤ê³„ (1-2ì¼)

#### âœ… ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ì •ì˜

```python
# core/module_interface.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List
from dataclasses import dataclass

@dataclass
class RAGContext:
    """ëª¨ë“  ëª¨ë“ˆ ê°„ ê³µìœ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸"""
    query: str
    original_query: str
    retrieved_docs: List[Dict[str, Any]]
    generated_answer: str
    metadata: Dict[str, Any]
    iteration: int

class RAGModule(ABC):
    """ëª¨ë“  RAG ëª¨ë“ˆì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = self.__class__.__name__
    
    @abstractmethod
    def execute(self, context: RAGContext) -> RAGContext:
        """
        ëª¨ë“ˆ ì‹¤í–‰ ë©”ì„œë“œ
        
        Args:
            context: í˜„ì¬ RAG ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ RAG ì»¨í…ìŠ¤íŠ¸
        """
        pass
    
    def validate_input(self, context: RAGContext) -> bool:
        """ì…ë ¥ ê²€ì¦"""
        return True
    
    def validate_output(self, context: RAGContext) -> bool:
        """ì¶œë ¥ ê²€ì¦"""
        return True
```

#### âœ… ëª¨ë“ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¶•

```python
# core/module_registry.py
from typing import Dict, Type, List
from core.module_interface import RAGModule

class ModuleRegistry:
    """ëª¨ë“ˆ ë“±ë¡ ë° ê´€ë¦¬"""
    
    def __init__(self):
        self._modules: Dict[str, Type[RAGModule]] = {}
        self._instances: Dict[str, RAGModule] = {}
    
    def register(self, name: str, module_class: Type[RAGModule]):
        """ëª¨ë“ˆ í´ë˜ìŠ¤ ë“±ë¡"""
        self._modules[name] = module_class
        print(f"[Registry] ëª¨ë“ˆ ë“±ë¡: {name}")
    
    def create(self, name: str, config: Dict) -> RAGModule:
        """ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if name not in self._modules:
            raise ValueError(f"Unknown module: {name}")
        
        instance = self._modules[name](config)
        self._instances[name] = instance
        return instance
    
    def get(self, name: str) -> RAGModule:
        """ë“±ë¡ëœ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if name not in self._instances:
            raise ValueError(f"Module not instantiated: {name}")
        return self._instances[name]
    
    def list_modules(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë“ˆ ëª©ë¡"""
        return list(self._modules.keys())

# ê¸€ë¡œë²Œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
registry = ModuleRegistry()
```

#### âœ… íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ê³„

```python
# core/pipeline.py
from typing import List, Dict, Any, Callable
from core.module_interface import RAGModule, RAGContext
from core.module_registry import registry

class RAGPipeline:
    """ëª¨ë“ˆì‹ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, name: str):
        self.name = name
        self.modules: List[RAGModule] = []
        self.conditional_branches: Dict[str, Callable] = {}
    
    def add_module(self, module_name: str, config: Dict = None):
        """íŒŒì´í”„ë¼ì¸ì— ëª¨ë“ˆ ì¶”ê°€"""
        config = config or {}
        module = registry.create(module_name, config)
        self.modules.append(module)
        return self
    
    def add_conditional(self, condition_fn: Callable, 
                       true_modules: List[str], 
                       false_modules: List[str]):
        """ì¡°ê±´ë¶€ ë¶„ê¸° ì¶”ê°€"""
        self.conditional_branches[condition_fn.__name__] = {
            'condition': condition_fn,
            'true': true_modules,
            'false': false_modules
        }
        return self
    
    def execute(self, query: str, **kwargs) -> RAGContext:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        context = RAGContext(
            query=query,
            original_query=query,
            retrieved_docs=[],
            generated_answer='',
            metadata=kwargs,
            iteration=0
        )
        
        for module in self.modules:
            print(f"[Pipeline] ì‹¤í–‰: {module.name}")
            
            # ì…ë ¥ ê²€ì¦
            if not module.validate_input(context):
                raise ValueError(f"Invalid input for {module.name}")
            
            # ëª¨ë“ˆ ì‹¤í–‰
            context = module.execute(context)
            
            # ì¶œë ¥ ê²€ì¦
            if not module.validate_output(context):
                raise ValueError(f"Invalid output from {module.name}")
        
        return context
```

### 2.2 Phase 1: ë°ì´í„° ë ˆì´ì–´ ì¤€ë¹„ (ì´ë¯¸ ì„¤ê³„ë¨, ì¬í™•ì¸ í•„ìš”)

#### âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ì²­í‚¹ ì „ëµ êµ¬í˜„ ì™„ë£Œ**
  - [ ] TypeAwareChunker êµ¬í˜„ (180-400 tokens)
  - [ ] ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ê¸° (drug/guideline/case/general)
  - [ ] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (source, confidence, section)

- [ ] **ì„ë² ë”© ìƒì„± ì™„ë£Œ**
  - [ ] text-embedding-3-large ì‚¬ìš©
  - [ ] ë°°ì¹˜ ì„ë² ë”© (batch_size=128)
  - [ ] L2 ì •ê·œí™” ì ìš©

- [ ] **ë“€ì–¼ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ**
  - [ ] Fine-grained index (< 300 tokens)
  - [ ] Coarse-grained index (â‰¥ 300 tokens)
  - [ ] BM25 ì¸ë±ìŠ¤ (ì „ì²´ ì²­í¬)

**ì„ í–‰ ì‘ì—… í™•ì¸**:
```bash
# 1. ì²­í¬ í’ˆì§ˆ ê²€ì¦
python scripts/validate_chunks.py \
  --input data/corpus_v2/train_source/chunks.jsonl \
  --check_size --check_metadata

# 2. ì¸ë±ìŠ¤ ë¬´ê²°ì„± ê²€ì‚¬
python scripts/validate_index.py \
  --fine_index data/index_v2/train_source/fine.index.faiss \
  --coarse_index data/index_v2/train_source/coarse.index.faiss

# 3. ê²€ìƒ‰ ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸
python scripts/measure_retrieval_baseline.py
```

### 2.3 Phase 2: Modular RAG í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„ (1-2ì£¼)

#### âœ… Pre-Retrieval ëª¨ë“ˆ

**1. Query Rewriter Module**

```python
# modules/pre_retrieval/query_rewriter.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient

class QueryRewriterModule(RAGModule):
    """ì¿¼ë¦¬ ì¬ì‘ì„± ëª¨ë“ˆ"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
        self.strategy = config.get('strategy', 'expansion')  # expansion/simplification/medical_focus
    
    def execute(self, context: RAGContext) -> RAGContext:
        if self.strategy == 'expansion':
            rewritten = self._expand_query(context.query)
        elif self.strategy == 'simplification':
            rewritten = self._simplify_query(context.query)
        elif self.strategy == 'medical_focus':
            rewritten = self._add_medical_context(context.query, context.metadata)
        
        context.query = rewritten
        context.metadata['rewritten_queries'] = context.metadata.get('rewritten_queries', [])
        context.metadata['rewritten_queries'].append(rewritten)
        
        return context
    
    def _expand_query(self, query: str) -> str:
        """ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´ ì¶”ê°€)"""
        prompt = f"""
        Expand this medical query by adding synonyms and related terms.
        Keep it concise and focused.
        
        Original query: {query}
        
        Expanded query:
        """
        return self.llm.generate(prompt, temperature=0.3, max_tokens=100).strip()
    
    def _simplify_query(self, query: str) -> str:
        """ë³µì¡í•œ ì¿¼ë¦¬ ë‹¨ìˆœí™”"""
        prompt = f"""
        Simplify this medical query to its core question.
        
        Original query: {query}
        
        Simplified query:
        """
        return self.llm.generate(prompt, temperature=0.2, max_tokens=50).strip()
    
    def _add_medical_context(self, query: str, metadata: Dict) -> str:
        """í™˜ì í”„ë¡œí•„ ë°˜ì˜"""
        profile = metadata.get('patient_profile', {})
        if not profile:
            return query
        
        context_parts = []
        if profile.get('age'):
            context_parts.append(f"Patient age: {profile['age']}")
        if profile.get('conditions'):
            context_parts.append(f"Conditions: {', '.join(profile['conditions'])}")
        if profile.get('medications'):
            context_parts.append(f"Current medications: {', '.join(profile['medications'])}")
        
        if context_parts:
            context_str = ". ".join(context_parts)
            return f"{query}. Context: {context_str}"
        
        return query
```

**2. Query Decomposer Module**

```python
# modules/pre_retrieval/query_decomposer.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient
from typing import List

class QueryDecomposerModule(RAGModule):
    """ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
        self.max_subqueries = config.get('max_subqueries', 3)
    
    def execute(self, context: RAGContext) -> RAGContext:
        # ë³µì¡ë„ í‰ê°€
        if not self._is_complex(context.query):
            return context
        
        # í•˜ìœ„ ì§ˆë¬¸ ìƒì„±
        subqueries = self._decompose(context.query)
        
        context.metadata['subqueries'] = subqueries
        context.metadata['is_decomposed'] = True
        
        return context
    
    def _is_complex(self, query: str) -> bool:
        """ì¿¼ë¦¬ ë³µì¡ë„ íŒë‹¨"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë‹¨ì–´ ìˆ˜, 'and' ê°œìˆ˜ ë“±
        words = query.split()
        has_conjunction = any(word.lower() in ['and', 'or', 'also'] for word in words)
        return len(words) > 15 or has_conjunction
    
    def _decompose(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ ë¶„í•´"""
        prompt = f"""
        Break down this complex medical question into {self.max_subqueries} simpler sub-questions.
        Each sub-question should be independently answerable.
        
        Complex question: {query}
        
        Sub-questions (one per line):
        """
        
        response = self.llm.generate(prompt, temperature=0.3, max_tokens=200)
        subqueries = [line.strip() for line in response.split('\n') if line.strip()]
        return subqueries[:self.max_subqueries]
```

**3. HyDE (Hypothetical Document Embeddings) Module**

```python
# modules/pre_retrieval/hyde.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient

class HyDEModule(RAGModule):
    """ê°€ìƒ ë¬¸ì„œ ìƒì„± í›„ ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
    
    def execute(self, context: RAGContext) -> RAGContext:
        # ê°€ìƒ ë‹µë³€ ìƒì„±
        hypothetical_doc = self._generate_hypothetical_answer(context.query)
        
        # ì›ë³¸ ì¿¼ë¦¬ ëŒ€ì‹  ê°€ìƒ ë¬¸ì„œë¡œ ê²€ìƒ‰
        context.metadata['hypothetical_doc'] = hypothetical_doc
        context.metadata['use_hyde'] = True
        
        return context
    
    def _generate_hypothetical_answer(self, query: str) -> str:
        """ê°€ìƒì˜ ì´ìƒì ì¸ ë‹µë³€ ìƒì„±"""
        prompt = f"""
        Generate a hypothetical, ideal answer to this medical question.
        Write as if you're citing from a medical textbook.
        
        Question: {query}
        
        Hypothetical answer:
        """
        return self.llm.generate(prompt, temperature=0.5, max_tokens=300).strip()
```

#### âœ… Retrieval ëª¨ë“ˆ (ì´ë¯¸ êµ¬í˜„ë¨, ëª¨ë“ˆí™” í•„ìš”)

```python
# modules/retrieval/hybrid_retrieval.py
from core.module_interface import RAGModule, RAGContext
from retrieval.dual_retriever import DualIndexRetriever

class HybridRetrievalModule(RAGModule):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“ˆ"""
    
    def __init__(self, config):
        super().__init__(config)
        self.retriever = DualIndexRetriever(
            index_dir=config['index_dir'],
            embedding_model=config.get('embedding_model', 'text-embedding-3-large')
        )
        self.k_fine = config.get('k_fine', 12)
        self.k_coarse = config.get('k_coarse', 5)
    
    def execute(self, context: RAGContext) -> RAGContext:
        # HyDE ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if context.metadata.get('use_hyde'):
            query = context.metadata['hypothetical_doc']
        else:
            query = context.query
        
        # ê²€ìƒ‰ ì‹¤í–‰
        docs = self.retriever.search(
            query=query,
            k_fine=self.k_fine,
            k_coarse=self.k_coarse,
            route='both'
        )
        
        context.retrieved_docs = docs
        context.metadata['num_retrieved'] = len(docs)
        
        return context
```

#### âœ… Post-Retrieval ëª¨ë“ˆ

**1. Reranker Module**

```python
# modules/post_retrieval/reranker.py
from core.module_interface import RAGModule, RAGContext
from sentence_transformers import CrossEncoder

class RerankerModule(RAGModule):
    """êµì°¨ ì¸ì½”ë” ê¸°ë°˜ ë¦¬ë­í‚¹"""
    
    def __init__(self, config):
        super().__init__(config)
        model_name = config.get('model', 'cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.reranker = CrossEncoder(model_name)
        self.top_k = config.get('top_k', 5)
    
    def execute(self, context: RAGContext) -> RAGContext:
        docs = context.retrieved_docs
        
        if len(docs) <= self.top_k:
            return context
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = [(context.query, doc['text']) for doc in docs]
        
        # ë¦¬ë­í‚¹ ì ìˆ˜ ê³„ì‚°
        scores = self.reranker.predict(pairs)
        
        # ì ìˆ˜ë¡œ ì •ë ¬
        for doc, score in zip(docs, scores):
            doc['rerank_score'] = float(score)
        
        docs.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        # ìƒìœ„ kê°œë§Œ ìœ ì§€
        context.retrieved_docs = docs[:self.top_k]
        context.metadata['reranked'] = True
        
        return context
```

**2. Relevance Filter Module**

```python
# modules/post_retrieval/relevance_filter.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient

class RelevanceFilterModule(RAGModule):
    """ê´€ë ¨ì„± ë‚®ì€ ë¬¸ì„œ í•„í„°ë§ (Corrective RAG í•µì‹¬)"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
        self.threshold = config.get('threshold', 0.5)
        self.use_llm = config.get('use_llm', True)
    
    def execute(self, context: RAGContext) -> RAGContext:
        docs = context.retrieved_docs
        
        if self.use_llm:
            relevance_scores = self._evaluate_with_llm(context.query, docs)
        else:
            relevance_scores = self._evaluate_heuristic(context.query, docs)
        
        # ì ìˆ˜ ì¶”ê°€
        for doc, score in zip(docs, relevance_scores):
            doc['relevance_score'] = score
        
        # í•„í„°ë§
        filtered_docs = [doc for doc in docs if doc['relevance_score'] >= self.threshold]
        
        context.retrieved_docs = filtered_docs
        context.metadata['filtered_count'] = len(docs) - len(filtered_docs)
        context.metadata['all_irrelevant'] = len(filtered_docs) == 0
        
        return context
    
    def _evaluate_with_llm(self, query: str, docs: List[Dict]) -> List[float]:
        """LLM ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€"""
        scores = []
        
        for doc in docs:
            prompt = f"""
            Rate the relevance of this document to the query on a scale of 0-1.
            Only return a number.
            
            Query: {query}
            Document: {doc['text'][:500]}
            
            Relevance score:
            """
            
            try:
                score_str = self.llm.generate(prompt, temperature=0.0, max_tokens=10).strip()
                score = float(score_str)
                scores.append(max(0.0, min(1.0, score)))
            except:
                scores.append(0.5)  # ê¸°ë³¸ê°’
        
        return scores
    
    def _evaluate_heuristic(self, query: str, docs: List[Dict]) -> List[float]:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        query_keywords = set(query.lower().split())
        scores = []
        
        for doc in docs:
            doc_keywords = set(doc['text'].lower().split())
            overlap = len(query_keywords & doc_keywords)
            score = overlap / len(query_keywords) if query_keywords else 0.0
            scores.append(min(1.0, score))
        
        return scores
```

#### âœ… Generation ëª¨ë“ˆ

```python
# modules/generation/generator.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient

class GeneratorModule(RAGModule):
    """LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
        self.model = config.get('model', 'gpt-4o-mini')
        self.temperature = config.get('temperature', 0.2)
        self.max_tokens = config.get('max_tokens', 800)
    
    def execute(self, context: RAGContext) -> RAGContext:
        # ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
        context_text = self._assemble_context(context.retrieved_docs)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(context.query, context_text, context.metadata)
        
        # ë‹µë³€ ìƒì„±
        answer = self.llm.generate(
            prompt,
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        context.generated_answer = answer
        
        return context
    
    def _assemble_context(self, docs: List[Dict]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¡°ë¦½"""
        context_parts = []
        for i, doc in enumerate(docs, 1):
            context_parts.append(f"[Document {i}]\n{doc['text']}\n")
        return "\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str, metadata: Dict) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""You are a medical AI assistant. Answer the question based on the provided context.

Context:
{context}

Question: {query}

Instructions:
- Provide accurate, evidence-based information
- Cite document numbers when referencing information
- If information is insufficient, state clearly
- Use professional medical terminology

Answer:"""
        
        return prompt
```

#### âœ… Evaluation & Correction ëª¨ë“ˆ

```python
# modules/evaluation/quality_evaluator.py
from core.module_interface import RAGModule, RAGContext
from core.llm_client import LLMClient

class QualityEvaluatorModule(RAGModule):
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ (Self-Refineìš©)"""
    
    def __init__(self, config):
        super().__init__(config)
        self.llm = LLMClient()
        self.threshold = config.get('threshold', 0.5)
    
    def execute(self, context: RAGContext) -> RAGContext:
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._evaluate_quality(
            context.query,
            context.generated_answer,
            context.retrieved_docs
        )
        
        context.metadata['quality_score'] = quality_score
        context.metadata['needs_refinement'] = quality_score < self.threshold
        
        return context
    
    def _evaluate_quality(self, query: str, answer: str, docs: List[Dict]) -> float:
        """LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        prompt = f"""
        Evaluate the quality of this answer on a scale of 0-1.
        Consider: relevance, accuracy, completeness, clarity.
        
        Question: {query}
        Answer: {answer}
        
        Available context: {len(docs)} documents
        
        Quality score (0-1):
        """
        
        try:
            score_str = self.llm.generate(prompt, temperature=0.0, max_tokens=10).strip()
            return float(score_str)
        except:
            return 0.5
```

### 2.4 Phase 3: RAG ë³€í˜• íŒŒì´í”„ë¼ì¸ êµ¬ì„± (3-5ì¼)

#### âœ… Basic RAG Pipeline

```python
# pipelines/basic_rag.py
from core.pipeline import RAGPipeline
from core.module_registry import registry

def build_basic_rag_pipeline():
    """ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸"""
    pipeline = RAGPipeline('basic_rag')
    
    # ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸: Retrieve â†’ Generate
    pipeline.add_module('hybrid_retrieval', {
        'index_dir': 'data/index_v2/train_source',
        'k_fine': 8,
        'k_coarse': 3
    })
    
    pipeline.add_module('generator', {
        'model': 'gpt-4o-mini',
        'temperature': 0.2
    })
    
    return pipeline
```

#### âœ… Modular RAG Pipeline

```python
# pipelines/modular_rag.py
from core.pipeline import RAGPipeline

def build_modular_rag_pipeline(config):
    """ëª¨ë“ˆí˜• RAG íŒŒì´í”„ë¼ì¸ (ë™ì  êµ¬ì„±)"""
    pipeline = RAGPipeline('modular_rag')
    
    # Pre-retrieval
    if config.get('use_query_rewriter'):
        pipeline.add_module('query_rewriter', {
            'strategy': config.get('rewrite_strategy', 'medical_focus')
        })
    
    if config.get('use_query_decomposer'):
        pipeline.add_module('query_decomposer', {
            'max_subqueries': 3
        })
    
    if config.get('use_hyde'):
        pipeline.add_module('hyde', {})
    
    # Retrieval
    pipeline.add_module('hybrid_retrieval', {
        'index_dir': config['index_dir'],
        'k_fine': config.get('k_fine', 12),
        'k_coarse': config.get('k_coarse', 5)
    })
    
    # Post-retrieval
    if config.get('use_reranker'):
        pipeline.add_module('reranker', {
            'model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
            'top_k': config.get('rerank_top_k', 5)
        })
    
    if config.get('use_relevance_filter'):
        pipeline.add_module('relevance_filter', {
            'threshold': config.get('relevance_threshold', 0.5),
            'use_llm': config.get('llm_filter', True)
        })
    
    # Generation
    pipeline.add_module('generator', {
        'model': config.get('llm_model', 'gpt-4o-mini'),
        'temperature': config.get('temperature', 0.2)
    })
    
    # Evaluation
    if config.get('use_quality_evaluator'):
        pipeline.add_module('quality_evaluator', {
            'threshold': config.get('quality_threshold', 0.5)
        })
    
    return pipeline
```

#### âœ… Corrective RAG (CRAG) Pipeline

```python
# pipelines/corrective_rag.py
from core.pipeline import RAGPipeline

def build_corrective_rag_pipeline():
    """êµì • RAG íŒŒì´í”„ë¼ì¸ (Self-Refine ë£¨í”„ í¬í•¨)"""
    pipeline = RAGPipeline('corrective_rag')
    
    # 1. Initial Retrieval
    pipeline.add_module('hybrid_retrieval', {
        'index_dir': 'data/index_v2/train_source',
        'k_fine': 15,  # ë” ë§ì´ ê²€ìƒ‰
        'k_coarse': 8
    })
    
    # 2. Relevance Filtering (í•µì‹¬!)
    pipeline.add_module('relevance_filter', {
        'threshold': 0.6,  # ë†’ì€ ì„ê³„ê°’
        'use_llm': True
    })
    
    # 3. Conditional: ëª¨ë“  ë¬¸ì„œ ê´€ë ¨ì„± ë‚®ìœ¼ë©´ ì›¹ ê²€ìƒ‰ ë˜ëŠ” ì¿¼ë¦¬ ì¬ì‘ì„±
    def check_relevance(context):
        return not context.metadata.get('all_irrelevant', False)
    
    pipeline.add_conditional(
        condition_fn=check_relevance,
        true_modules=['generator'],  # ê´€ë ¨ ë¬¸ì„œ ìˆìŒ â†’ ìƒì„±
        false_modules=['query_rewriter', 'hybrid_retrieval', 'generator']  # ì¬ê²€ìƒ‰
    )
    
    # 4. Quality Evaluation
    pipeline.add_module('quality_evaluator', {
        'threshold': 0.6
    })
    
    # 5. Self-Refine Loop (ìµœëŒ€ 2íšŒ)
    # (LangGraphì—ì„œ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ë” ì í•©)
    
    return pipeline
```

### 2.5 Phase 4: Ablation ì‹¤í—˜ ì„¤ê³„ (2-3ì¼)

#### âœ… RAG ë³€í˜• ë¹„êµ ì‹¤í—˜

```python
# experiments/rag_variants_ablation.py
"""RAG ë³€í˜• ë¹„êµ ì‹¤í—˜"""

EXPERIMENTS = {
    'E1_basic_rag': {
        'pipeline': 'basic_rag',
        'config': {
            'k_fine': 8,
            'k_coarse': 3
        }
    },
    
    'E2_modular_rag_minimal': {
        'pipeline': 'modular_rag',
        'config': {
            'use_query_rewriter': True,
            'rewrite_strategy': 'medical_focus',
            'use_reranker': False,
            'use_relevance_filter': False
        }
    },
    
    'E3_modular_rag_full': {
        'pipeline': 'modular_rag',
        'config': {
            'use_query_rewriter': True,
            'use_query_decomposer': False,
            'use_hyde': False,
            'use_reranker': True,
            'use_relevance_filter': True,
            'use_quality_evaluator': True
        }
    },
    
    'E4_corrective_rag': {
        'pipeline': 'corrective_rag',
        'config': {
            'max_refine_iterations': 2,
            'relevance_threshold': 0.6,
            'quality_threshold': 0.6
        }
    },
    
    'E5_modular_with_hyde': {
        'pipeline': 'modular_rag',
        'config': {
            'use_hyde': True,
            'use_reranker': True,
            'rerank_top_k': 5
        }
    },
}

def run_ablation_experiment(exp_id: str, test_cases: List[Dict]):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
    exp_config = EXPERIMENTS[exp_id]
    
    # íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    if exp_config['pipeline'] == 'basic_rag':
        pipeline = build_basic_rag_pipeline()
    elif exp_config['pipeline'] == 'modular_rag':
        pipeline = build_modular_rag_pipeline(exp_config['config'])
    elif exp_config['pipeline'] == 'corrective_rag':
        pipeline = build_corrective_rag_pipeline()
    
    results = []
    
    for case in test_cases:
        # ì‹¤í–‰
        context = pipeline.execute(
            query=case['query'],
            patient_profile=case.get('profile', {})
        )
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = {
            'query': case['query'],
            'answer': context.generated_answer,
            'num_docs': len(context.retrieved_docs),
            'quality_score': context.metadata.get('quality_score', 0.0),
            'iteration_count': context.iteration,
            # ... ì¶”ê°€ ë©”íŠ¸ë¦­
        }
        
        results.append(metrics)
    
    return results
```

---

## 3. RAG ë³€í˜•ë³„ êµ¬í˜„ ìš”êµ¬ì‚¬í•­

### 3.1 Basic RAG ìš”êµ¬ì‚¬í•­

**í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸**:
- [x] Retriever (BM25 ë˜ëŠ” FAISS)
- [x] Generator (LLM)

**ì„ íƒ ì»´í¬ë„ŒíŠ¸**:
- [ ] Query preprocessing (ì„ íƒ)
- [ ] Context assembly (ì„ íƒ)

**êµ¬í˜„ ë‚œì´ë„**: â­ (ì‰¬ì›€)

**ì˜ˆìƒ ì„±ëŠ¥**:
- Recall@5: 0.60-0.70
- Judge Score: 6.5-7.5/10

### 3.2 Modular RAG ìš”êµ¬ì‚¬í•­

**í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸**:
- [x] Module Interface (RAGModule ì¶”ìƒ í´ë˜ìŠ¤)
- [x] Module Registry (ëª¨ë“ˆ ë“±ë¡ ì‹œìŠ¤í…œ)
- [x] Pipeline Orchestrator (ëª¨ë“ˆ ì¡°í•© ê´€ë¦¬)
- [x] ìµœì†Œ 3ê°œ ëª¨ë“ˆ (Pre-retrieval, Retrieval, Generation)

**ê¶Œì¥ ì»´í¬ë„ŒíŠ¸**:
- [ ] Query Rewriter (ì¿¼ë¦¬ ê°œì„ )
- [ ] Reranker (ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬)
- [ ] Quality Evaluator (í’ˆì§ˆ í‰ê°€)

**êµ¬í˜„ ë‚œì´ë„**: â­â­ (ì¤‘ê°„)

**ì˜ˆìƒ ì„±ëŠ¥**:
- Recall@5: 0.70-0.80 (+10-15%p vs Basic)
- Judge Score: 7.5-8.5/10

**ì„ í–‰ ì‘ì—…**:
1. âœ… ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (Phase 0)
2. âœ… ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¶• (Phase 0)
3. âœ… íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (Phase 0)
4. âœ… ìµœì†Œ 5-7ê°œ ëª¨ë“ˆ êµ¬í˜„ (Phase 2)

### 3.3 Corrective RAG (CRAG) ìš”êµ¬ì‚¬í•­

**í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸**:
- [x] Relevance Evaluator (ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€) â­ í•µì‹¬!
- [x] Conditional Branching (ì¡°ê±´ë¶€ ë¶„ê¸°)
- [x] Query Rewriter (ì¬ì‘ì„±)
- [x] Web Search Fallback (ì„ íƒì )
- [x] Self-Refine Loop (ë°˜ë³µ ê°œì„ )

**êµ¬í˜„ ë‚œì´ë„**: â­â­â­ (ì–´ë ¤ì›€)

**ì˜ˆìƒ ì„±ëŠ¥**:
- Recall@5: 0.75-0.85 (+15-25%p vs Basic)
- Judge Score: 8.0-9.0/10
- Hallucination Rate: -30-50% vs Basic

**ì„ í–‰ ì‘ì—…**:
1. âœ… Relevance Filter Module êµ¬í˜„ (LLM ê¸°ë°˜) â­ ìµœìš°ì„ !
2. âœ… Conditional Pipeline ì§€ì› (Phase 0)
3. âœ… Query Rewriter Module (Phase 2)
4. âœ… Quality Evaluator Module (Phase 2)
5. âœ… Self-Refine Loop (LangGraphì—ì„œ êµ¬í˜„)

**CRAG íŠ¹í™” ìš”êµ¬ì‚¬í•­**:

```python
# CRAG í•µì‹¬: 3ë‹¨ê³„ ê´€ë ¨ì„± í‰ê°€
def evaluate_document_relevance(query, docs):
    """
    ë¬¸ì„œ ê´€ë ¨ì„±ì„ 3ë‹¨ê³„ë¡œ ë¶„ë¥˜:
    - Correct: ë†’ì€ ê´€ë ¨ì„± (score > 0.7)
    - Ambiguous: ì¤‘ê°„ ê´€ë ¨ì„± (0.4 < score <= 0.7)
    - Incorrect: ë‚®ì€ ê´€ë ¨ì„± (score <= 0.4)
    """
    scores = []
    for doc in docs:
        score = llm_evaluate_relevance(query, doc)
        if score > 0.7:
            label = 'correct'
        elif score > 0.4:
            label = 'ambiguous'
        else:
            label = 'incorrect'
        scores.append({'doc': doc, 'score': score, 'label': label})
    
    return scores

def corrective_action(relevance_results):
    """
    ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ êµì • ì•¡ì…˜:
    - All Correct â†’ Generate directly
    - Some Correct â†’ Filter + Generate
    - All Incorrect â†’ Web search OR Query rewrite
    """
    correct_docs = [r for r in relevance_results if r['label'] == 'correct']
    
    if len(correct_docs) == len(relevance_results):
        return 'generate', relevance_results
    elif len(correct_docs) > 0:
        return 'filter_and_generate', correct_docs
    else:
        return 'web_search_or_rewrite', []
```

---

## 4. LangGraph vs ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ì‹¬ì¸µ ë¶„ì„

### 4.1 LangGraph ìƒì„¸ ë¶„ì„

#### **í•µì‹¬ íŠ¹ì§•**

1. **State Graph ê¸°ë°˜**
```python
# LangGraphì˜ í•µì‹¬: StateGraph
from langgraph.graph import StateGraph, END

graph = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("retrieve", retrieve_node)
graph.add_node("generate", generate_node)
graph.add_node("evaluate", evaluate_node)

# ì¡°ê±´ë¶€ ì—£ì§€ (Self-Refine ë£¨í”„)
def should_refine(state):
    return state['quality_score'] < 0.5

graph.add_conditional_edges(
    "evaluate",
    should_refine,
    {
        True: "retrieve",  # ì¬ê²€ìƒ‰
        False: END         # ì¢…ë£Œ
    }
)
```

2. **ìˆœí™˜ ê·¸ë˜í”„ ì§€ì›**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangGraph Cycle Support         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Retrieve â†’ Generate â†’ Evaluate         â”‚
â”‚      â†‘                      â†“           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€ (if low) â”€â”€â”€â”€â”˜           â”‚
â”‚                                         â”‚
â”‚  ìµœëŒ€ NíšŒ ë°˜ë³µ ê°€ëŠ¥ (max_iterations)    â”‚
â”‚  ë¬´í•œ ë£¨í”„ ë°©ì§€ ë‚´ì¥                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

3. **ì¤‘ì•™ì§‘ì¤‘ì‹ ìƒíƒœ ê´€ë¦¬**
```python
@dataclass
class AgentState:
    """ëª¨ë“  ë…¸ë“œê°€ ê³µìœ í•˜ëŠ” ìƒíƒœ"""
    query: str
    retrieved_docs: List[Dict]
    answer: str
    quality_score: float
    iteration_count: int
    # ... ëª¨ë“  ì¤‘ê°„ ê²°ê³¼ ì €ì¥
```

#### **ì¥ì  (LangGraphë¥¼ ìœ ì§€í•´ì•¼ í•˜ëŠ” ì´ìœ )**

| ì¥ì  | ì„¤ëª… | ì¤‘ìš”ë„ |
|-----|------|--------|
| **ìˆœí™˜ ë¡œì§ ë„¤ì´í‹°ë¸Œ ì§€ì›** | Self-Refine, CRAG ê°™ì€ ë°˜ë³µ íŒ¨í„´ì„ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ | â­â­â­â­â­ |
| **ìƒíƒœ ê´€ë¦¬ ìë™í™”** | ëª¨ë“  ë…¸ë“œ ê°„ ìƒíƒœ ìë™ ì „ë‹¬, ìˆ˜ë™ ê´€ë¦¬ ë¶ˆí•„ìš” | â­â­â­â­â­ |
| **ì‹œê°í™” ìš©ì´** | ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ Mermaid ë“±ìœ¼ë¡œ ìë™ ì‹œê°í™” ê°€ëŠ¥ | â­â­â­â­ |
| **ë””ë²„ê¹… í¸ì˜ì„±** | ê° ë…¸ë“œì˜ ì…ì¶œë ¥ ì¶”ì  ìš©ì´ | â­â­â­â­ |
| **LangChain ìƒíƒœê³„** | LangChainì˜ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì¬ì‚¬ìš© ê°€ëŠ¥ | â­â­â­â­ |
| **Checkpointing** | ì¤‘ê°„ ìƒíƒœ ì €ì¥ ë° ë³µêµ¬ ì§€ì› | â­â­â­ |
| **Human-in-the-loop** | ì‚¬ëŒ ê°œì… ì§€ì  ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥ | â­â­â­ |

#### **ë‹¨ì **

| ë‹¨ì  | ì„¤ëª… | ì˜í–¥ë„ |
|-----|------|--------|
| **í•™ìŠµ ê³¡ì„ ** | StateGraph ê°œë… ì´í•´ í•„ìš” | â­â­ |
| **ì˜¤ë²„í—¤ë“œ** | ë‹¨ìˆœ íŒŒì´í”„ë¼ì¸ì—ëŠ” ê³¼ë„í•œ ì¶”ìƒí™” | â­â­ |
| **ì„±ëŠ¥** | ìƒíƒœ ë³µì‚¬ ì˜¤ë²„í—¤ë“œ (ëŒ€ë¶€ë¶„ ë¬´ì‹œ ê°€ëŠ¥) | â­ |

#### **LangGraphê°€ íŠ¹íˆ ìš°ìˆ˜í•œ ì‹œë‚˜ë¦¬ì˜¤**

1. **Self-Refine Loop**
```python
# LangGraph: ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
graph.add_conditional_edges(
    "evaluate",
    lambda state: state['quality_score'] < 0.5,
    {True: "rewrite_query", False: END}
)

# ëŒ€ì•ˆ (ìˆ˜ë™ êµ¬í˜„): ë³µì¡í•˜ê³  ì˜¤ë¥˜ ê°€ëŠ¥ì„± ë†’ìŒ
def manual_self_refine(query, max_iter=3):
    for i in range(max_iter):
        docs = retrieve(query)
        answer = generate(query, docs)
        quality = evaluate(answer)
        if quality >= 0.5:
            break
        query = rewrite(query, answer)
    return answer
```

2. **Corrective RAGì˜ ì¡°ê±´ë¶€ ë¶„ê¸°**
```python
# LangGraph: ëª…í™•í•œ ë¶„ê¸° í‘œí˜„
def route_by_relevance(state):
    if state['all_docs_irrelevant']:
        return "web_search"
    elif state['some_docs_relevant']:
        return "filter_and_generate"
    else:
        return "generate"

graph.add_conditional_edges("evaluate_relevance", route_by_relevance)
```

3. **ë©€í‹°í„´ ëŒ€í™” ìƒíƒœ ê´€ë¦¬**
```python
# LangGraph: ìë™ ìƒíƒœ ìœ ì§€
@dataclass
class ConversationState:
    history: List[Dict]  # ìë™ìœ¼ë¡œ ëˆ„ì 
    current_query: str
    context: Dict
    # ... ëª¨ë“  í„´ì˜ ì •ë³´ ìœ ì§€
```

### 4.2 ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ë¹„êµ

#### **Option 1: LlamaIndex**

**íŠ¹ì§•**:
- ë°ì´í„° ì¤‘ì‹¬ í”„ë ˆì„ì›Œí¬
- ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (Vector, Tree, Keyword)
- Query Engine ì¶”ìƒí™”

**ì¥ì **:
- ì¸ë±ì‹± ë° ê²€ìƒ‰ì— íŠ¹í™”
- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ì»¤ë„¥í„°
- ê°„ë‹¨í•œ API

**ë‹¨ì **:
- ìˆœí™˜ ë¡œì§ ì§€ì› ì•½í•¨
- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° í‘œí˜„ ì–´ë ¤ì›€
- ìƒíƒœ ê´€ë¦¬ ìˆ˜ë™

**ë¹„êµ**:
```python
# LlamaIndex: ë‹¨ìˆœ RAGì— ì í•©
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("What is X?")

# LangGraph: ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ì— ì í•©
graph = StateGraph(AgentState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("evaluate", evaluate_node)
graph.add_conditional_edges("evaluate", should_refine)
```

**ê²°ë¡ **: LlamaIndexëŠ” Basic RAGì— ì í•©, Modular/Corrective RAGì—ëŠ” LangGraphê°€ ìš°ìˆ˜

#### **Option 2: Haystack**

**íŠ¹ì§•**:
- íŒŒì´í”„ë¼ì¸ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬
- ë…¸ë“œì™€ íŒŒì´í”„ë¼ì¸ ê°œë…
- í”„ë¡œë•ì…˜ ë°°í¬ì— ê°•ì 

**ì¥ì **:
- ëª…í™•í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
- REST API ìë™ ìƒì„±
- í”„ë¡œë•ì…˜ ë„êµ¬ í’ë¶€

**ë‹¨ì **:
- ìˆœí™˜ íŒŒì´í”„ë¼ì¸ ì§€ì› ì œí•œì 
- ìƒíƒœ ê´€ë¦¬ ë³µì¡
- LangChain ìƒíƒœê³„ ë¯¸ì§€ì›

**ë¹„êµ**:
```python
# Haystack: ì„ í˜• íŒŒì´í”„ë¼ì¸
pipeline = Pipeline()
pipeline.add_node("retriever", retriever, inputs=["Query"])
pipeline.add_node("reader", reader, inputs=["retriever"])

# LangGraph: ìˆœí™˜ ê°€ëŠ¥
graph.add_conditional_edges("reader", should_refine, {
    True: "retriever",  # ìˆœí™˜!
    False: END
})
```

**ê²°ë¡ **: Haystackì€ í”„ë¡œë•ì…˜ ë°°í¬ì— ê°•ì , ì—°êµ¬/ì‹¤í—˜ì—ëŠ” LangGraphê°€ ìœ ì—°

#### **Option 3: Custom Implementation (ìˆœìˆ˜ Python)**

**ì¥ì **:
- ì™„ì „í•œ ì œì–´
- ì˜ì¡´ì„± ìµœì†Œí™”
- ìµœì í™” ê°€ëŠ¥

**ë‹¨ì **:
- ê°œë°œ ì‹œê°„ ì¦ê°€
- ë²„ê·¸ ê°€ëŠ¥ì„±
- ìœ ì§€ë³´ìˆ˜ ë¶€ë‹´

**ë¹„êµ**:
```python
# Custom: ëª¨ë“  ê²ƒì„ ì§ì ‘ êµ¬í˜„
class CustomRAG:
    def __init__(self):
        self.state = {}
    
    def run(self, query):
        for i in range(self.max_iter):
            docs = self.retrieve(query)
            answer = self.generate(query, docs)
            quality = self.evaluate(answer)
            if quality >= self.threshold:
                break
            query = self.rewrite(query)
        return answer
    # ... ìˆ˜ë°± ì¤„ì˜ ìƒíƒœ ê´€ë¦¬ ì½”ë“œ

# LangGraph: ì„ ì–¸ì  ì •ì˜
graph = StateGraph(AgentState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("generate", generate_node)
graph.add_node("evaluate", evaluate_node)
graph.add_conditional_edges("evaluate", should_refine)
# ë!
```

**ê²°ë¡ **: íŠ¹ìˆ˜í•œ ìš”êµ¬ì‚¬í•­ì´ ì—†ë‹¤ë©´ LangGraphê°€ íš¨ìœ¨ì 

### 4.3 ì•„í‚¤í…ì²˜ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤

| ìš”êµ¬ì‚¬í•­ | LangGraph | LlamaIndex | Haystack | Custom |
|---------|-----------|------------|----------|--------|
| **Basic RAG** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| **Modular RAG** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |
| **Corrective RAG** | â­â­â­â­â­ | â­â­ | â­â­ | â­â­â­â­ |
| **Self-Refine** | â­â­â­â­â­ | â­ | â­â­ | â­â­â­ |
| **ìˆœí™˜ ë¡œì§** | â­â­â­â­â­ | â­ | â­â­ | â­â­â­â­ |
| **ìƒíƒœ ê´€ë¦¬** | â­â­â­â­â­ | â­â­ | â­â­â­ | â­â­ |
| **í•™ìŠµ ê³¡ì„ ** | â­â­â­ | â­â­â­â­ | â­â­â­ | â­ |
| **í”„ë¡œë•ì…˜** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **Ablation ìš©ì´ì„±** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |

### 4.4 ìµœì¢… íŒë‹¨: LangGraph ìœ ì§€ ê¶Œì¥

**ê²°ë¡ **: **LangGraphë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.**

**ì´ìœ **:

1. **í˜„ì¬ í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ê³¼ ì™„ë²½íˆ ì¼ì¹˜**
   - âœ… Corrective RAG êµ¬í˜„ í•„ìš” â†’ LangGraphì˜ ì¡°ê±´ë¶€ ë¶„ê¸° í•„ìˆ˜
   - âœ… Self-Refine Loop â†’ LangGraphì˜ ìˆœí™˜ ê·¸ë˜í”„ ìµœì 
   - âœ… Ablation ì—°êµ¬ â†’ LangGraphì˜ ëª¨ë“ˆì„±ì´ ì´ìƒì 

2. **ê¸°ì¡´ ì½”ë“œë² ì´ìŠ¤ì™€ì˜ í˜¸í™˜ì„±**
   - í˜„ì¬ ì‹œìŠ¤í…œì´ ì´ë¯¸ LangGraph ê¸°ë°˜
   - ë§ˆì´ê·¸ë ˆì´ì…˜ ë¹„ìš© > ìœ ì§€ ë¹„ìš©

3. **ì—°êµ¬ ìœ ì—°ì„±**
   - ë‹¤ì–‘í•œ RAG ë³€í˜• ì‹¤í—˜ì— ìµœì 
   - ë…¸ë“œ ì¶”ê°€/ì œê±°ê°€ ë§¤ìš° ì‰¬ì›€

4. **ì»¤ë®¤ë‹ˆí‹° ë° ìƒíƒœê³„**
   - LangChain ìƒíƒœê³„ í™œìš© ê°€ëŠ¥
   - í™œë°œí•œ ê°œë°œ ë° ì—…ë°ì´íŠ¸

**ì•„í‚¤í…ì²˜ ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš°**:
- âŒ í”„ë¡œë•ì…˜ ë°°í¬ê°€ ìµœìš°ì„  â†’ Haystack ê³ ë ¤
- âŒ ë‹¨ìˆœ RAGë§Œ í•„ìš” â†’ LlamaIndex ê³ ë ¤
- âŒ ê·¹í•œì˜ ì„±ëŠ¥ ìµœì í™” í•„ìš” â†’ Custom ê³ ë ¤

**í˜„ì¬ í”„ë¡œì íŠ¸ì—ëŠ” í•´ë‹¹ ì—†ìŒ!**

---

## 5. ìµœì¢… ê¶Œì¥ì‚¬í•­ ë° ë¡œë“œë§µ

### 5.1 ì¢…í•© ê¶Œì¥ì‚¬í•­

#### **1. ì•„í‚¤í…ì²˜: LangGraph ìœ ì§€ âœ…**

**ì´ìœ **:
- Corrective RAGì˜ ì¡°ê±´ë¶€ ë¶„ê¸° êµ¬í˜„ì— ìµœì 
- Self-Refine Loopë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„
- í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ í˜¸í™˜ì„± 100%
- Ablation ì—°êµ¬ì— ì´ìƒì 

**ê¸°ëŒ€ íš¨ê³¼**:
- ê°œë°œ ì‹œê°„ ì ˆì•½: 50-70% (vs ìƒˆ ì•„í‚¤í…ì²˜)
- ì•ˆì •ì„± ìœ ì§€: ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©
- ìœ ì—°ì„± í™•ë³´: ì‹¤í—˜ ë³€í˜• ìš©ì´

#### **2. Modular RAG ì ‘ê·¼ ì±„íƒ âœ…**

**ì´ìœ **:
- Basic, Modular, Corrective RAG ëª¨ë‘ ì§€ì›
- ëª¨ë“ˆ ë‹¨ìœ„ Ablation ê°€ëŠ¥
- ì ì§„ì  ê°œì„  ê°€ëŠ¥

**êµ¬í˜„ ì „ëµ**:
```
Week 1: ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ + ë ˆì§€ìŠ¤íŠ¸ë¦¬
Week 2: í•µì‹¬ ëª¨ë“ˆ 5-7ê°œ êµ¬í˜„
Week 3: íŒŒì´í”„ë¼ì¸ 3ì¢… êµ¬ì¶•
Week 4: Ablation ì‹¤í—˜ ë° ë¶„ì„
```

#### **3. ì„ í–‰ ì‘ì—… ìš°ì„ ìˆœìœ„**

**P0 (í•„ìˆ˜, 1ì£¼)**:
- [ ] ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (RAGModule, RAGContext)
- [ ] ëª¨ë“ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¶•
- [ ] íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

**P1 (í•µì‹¬, 2ì£¼)**:
- [ ] Query Rewriter Module
- [ ] Relevance Filter Module (CRAG í•µì‹¬!)
- [ ] Reranker Module
- [ ] Quality Evaluator Module

**P2 (ê³ ê¸‰, 1ì£¼)**:
- [ ] Query Decomposer Module
- [ ] HyDE Module
- [ ] Context Compressor Module

### 5.2 4ì£¼ êµ¬í˜„ ë¡œë“œë§µ

#### **Week 1: Foundation (ê¸°ë°˜ êµ¬ì¶•)**

**Day 1-2: ì•„í‚¤í…ì²˜ ì„¤ê³„**
- [ ] ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- [ ] ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬í˜„
- [ ] íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê¸°ë³¸ êµ¬í˜„

**Day 3-4: LangGraph í†µí•©**
- [ ] LangGraphì™€ ëª¨ë“ˆ ì‹œìŠ¤í…œ í†µí•©
- [ ] StateGraphì—ì„œ ëª¨ë“ˆ í˜¸ì¶œ ë°©ì‹ ì„¤ê³„
- [ ] ìˆœí™˜ ë¡œì§ í…ŒìŠ¤íŠ¸

**Day 5-7: ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸**
- [ ] Basic RAG Pipeline êµ¬í˜„
- [ ] í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- [ ] ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •

**Deliverable**: 
- ë™ì‘í•˜ëŠ” Basic RAG Pipeline
- ë² ì´ìŠ¤ë¼ì¸ ë©”íŠ¸ë¦­ (Recall@5, Judge Score)

#### **Week 2: Core Modules (í•µì‹¬ ëª¨ë“ˆ)**

**Day 8-10: Pre/Post-Retrieval**
- [ ] Query Rewriter Module
- [ ] Reranker Module
- [ ] Relevance Filter Module (CRAGìš©)

**Day 11-12: Generation & Evaluation**
- [ ] Generator Module (ê¸°ì¡´ ì½”ë“œ ëª¨ë“ˆí™”)
- [ ] Quality Evaluator Module

**Day 13-14: Modular RAG Pipeline**
- [ ] Modular RAG Pipeline êµ¬ì¶•
- [ ] ëª¨ë“ˆ ì¡°í•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ì¸¡ì •

**Deliverable**:
- 5-7ê°œ í•µì‹¬ ëª¨ë“ˆ
- Modular RAG Pipeline
- ì„±ëŠ¥ ê°œì„  í™•ì¸ (+10-15%p)

#### **Week 3: Corrective RAG (êµì • RAG)**

**Day 15-17: CRAG êµ¬í˜„**
- [ ] Relevance Evaluator ê³ ë„í™”
- [ ] Conditional Branching êµ¬í˜„
- [ ] Self-Refine Loop í†µí•©

**Day 18-19: Web Search Fallback**
- [ ] ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ (ì„ íƒì )
- [ ] Query Rewrite ì „ëµ ê°œì„ 

**Day 20-21: CRAG Pipeline ì™„ì„±**
- [ ] Corrective RAG Pipeline êµ¬ì¶•
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ì¸¡ì •

**Deliverable**:
- Corrective RAG Pipeline
- ì„±ëŠ¥ ê°œì„  í™•ì¸ (+20-30%p vs Basic)
- Hallucination Rate ê°ì†Œ (-30-50%)

#### **Week 4: Ablation & Analysis (ì‹¤í—˜ ë° ë¶„ì„)**

**Day 22-24: Ablation ì‹¤í—˜**
- [ ] E1-E5 ì‹¤í—˜ ì‹¤í–‰
- [ ] ë©”íŠ¸ë¦­ ìë™ ìˆ˜ì§‘
- [ ] ê²°ê³¼ ë°ì´í„° ì •ë¦¬

**Day 25-26: ë¶„ì„ ë° ì‹œê°í™”**
- [ ] ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
- [ ] ê·¸ë˜í”„ ìƒì„±
- [ ] í†µê³„ ë¶„ì„

**Day 27-28: ë¬¸ì„œí™”**
- [ ] ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸
- [ ] ë…¼ë¬¸ìš© í‘œ/ê·¸ë˜í”„
- [ ] ì½”ë“œ ë¬¸ì„œí™”

**Deliverable**:
- ì „ì²´ Ablation ê²°ê³¼
- ë…¼ë¬¸ ì´ˆì•ˆ (Method + Results)
- ì¬í˜„ ê°€ëŠ¥í•œ ì½”ë“œ

### 5.3 ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Performance Improvement Roadmap               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Baseline (Current)                                     â”‚
â”‚  â”œâ”€ Recall@5: 0.65                                     â”‚
â”‚  â”œâ”€ Judge Score: 7.2/10                                â”‚
â”‚  â””â”€ Hallucination: 35%                                 â”‚
â”‚                                                         â”‚
â”‚  â†“ Week 1: Basic RAG (í™•ì¸)                            â”‚
â”‚                                                         â”‚
â”‚  Week 2: Modular RAG                                   â”‚
â”‚  â”œâ”€ Recall@5: 0.75 (+15%p) â­                         â”‚
â”‚  â”œâ”€ Judge Score: 7.8/10 (+0.6)                        â”‚
â”‚  â””â”€ Hallucination: 25% (-29%)                         â”‚
â”‚                                                         â”‚
â”‚  â†“ +Query Rewriter, Reranker                          â”‚
â”‚                                                         â”‚
â”‚  Week 3: Corrective RAG                                â”‚
â”‚  â”œâ”€ Recall@5: 0.82 (+26%p) â­â­                       â”‚
â”‚  â”œâ”€ Judge Score: 8.5/10 (+1.3) â­                     â”‚
â”‚  â””â”€ Hallucination: 12% (-66%) â­â­                    â”‚
â”‚                                                         â”‚
â”‚  â†“ +Relevance Filter, Self-Refine                     â”‚
â”‚                                                         â”‚
â”‚  Week 4: Optimized System                              â”‚
â”‚  â”œâ”€ Recall@5: 0.85 (+31%p) â­â­â­                     â”‚
â”‚  â”œâ”€ Judge Score: 8.8/10 (+1.6) â­â­                   â”‚
â”‚  â””â”€ Hallucination: 8% (-77%) â­â­â­                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 ë¹„ìš© ë° ë¦¬ì†ŒìŠ¤ ì¶”ì •

**ê°œë°œ ì‹œê°„**: 4ì£¼ (í•˜ë£¨ 6-8ì‹œê°„ ì‘ì—… ê¸°ì¤€)

**API ë¹„ìš©** (1,000 ì¿¼ë¦¬ ì‹¤í—˜ ê¸°ì¤€):
- Embedding (ì¬ìƒì„± ë¶ˆí•„ìš”): $0
- LLM í˜¸ì¶œ (ì‹¤í—˜):
  - Basic RAG: ~$5-10
  - Modular RAG: ~$10-20 (Reranker, Evaluator ì¶”ê°€)
  - Corrective RAG: ~$20-30 (ë°˜ë³µ í˜¸ì¶œ)
- ì´ ì˜ˆìƒ: $50-100

**ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤**:
- CPU: ì¼ë°˜ ë…¸íŠ¸ë¶ ì¶©ë¶„
- RAM: 16GB ê¶Œì¥ (FAISS ì¸ë±ìŠ¤ ë¡œë“œ)
- GPU: ë¶ˆí•„ìš” (RerankerëŠ” CPUë¡œ ì¶©ë¶„)

### 5.5 ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

#### **ì‹œì‘ ì „ í™•ì¸**
- [ ] ê¸°ì¡´ ë°ì´í„° ë ˆì´ì–´ ì™„ì„± (ì²­í‚¹, ë“€ì–¼ ì¸ë±ìŠ¤)
- [ ] LangGraph ê¸°ë³¸ ì´í•´
- [ ] Python 3.9+ í™˜ê²½
- [ ] API í‚¤ ì„¤ì • (OpenAI)

#### **Week 1 ì™„ë£Œ ê¸°ì¤€**
- [ ] ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ë™ì‘
- [ ] Basic RAG Pipeline ì‹¤í–‰ ì„±ê³µ
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë©”íŠ¸ë¦­ ì¸¡ì • ì™„ë£Œ

#### **Week 2 ì™„ë£Œ ê¸°ì¤€**
- [ ] 5ê°œ ì´ìƒ ëª¨ë“ˆ êµ¬í˜„
- [ ] Modular RAG Pipeline ë™ì‘
- [ ] ì„±ëŠ¥ ê°œì„  í™•ì¸ (+10%p)

#### **Week 3 ì™„ë£Œ ê¸°ì¤€**
- [ ] Corrective RAG Pipeline ë™ì‘
- [ ] Self-Refine Loop ì •ìƒ ì‘ë™
- [ ] ì„±ëŠ¥ ê°œì„  í™•ì¸ (+20%p)

#### **Week 4 ì™„ë£Œ ê¸°ì¤€**
- [ ] 5ê°œ ì´ìƒ Ablation ì‹¤í—˜ ì™„ë£Œ
- [ ] ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„±

---

## 6. ê²°ë¡ 

### 6.1 í•µì‹¬ ë©”ì‹œì§€

```
1. LangGraph ìœ ì§€ âœ…
   â†’ Corrective RAG êµ¬í˜„ì— ìµœì 
   â†’ ìˆœí™˜ ë¡œì§ ë„¤ì´í‹°ë¸Œ ì§€ì›
   â†’ í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ 100% í˜¸í™˜

2. Modular RAG ì ‘ê·¼ ì±„íƒ âœ…
   â†’ Basic, Modular, Corrective ëª¨ë‘ ì§€ì›
   â†’ ëª¨ë“ˆ ë‹¨ìœ„ Ablation ê°€ëŠ¥
   â†’ ì ì§„ì  ê°œì„  ì „ëµ

3. ì„ í–‰ ì‘ì—… ëª…í™• âœ…
   â†’ Phase 0: ì•„í‚¤í…ì²˜ (1-2ì¼)
   â†’ Phase 1: ë°ì´í„° (ì´ë¯¸ ì™„ë£Œ)
   â†’ Phase 2: ëª¨ë“ˆ (1-2ì£¼)
   â†’ Phase 3: íŒŒì´í”„ë¼ì¸ (3-5ì¼)
   â†’ Phase 4: Ablation (2-3ì¼)

4. ì˜ˆìƒ ì„±ëŠ¥ âœ…
   â†’ Recall@5: +26-31%p
   â†’ Judge Score: +1.3-1.6ì 
   â†’ Hallucination: -66-77%
```

### 6.2 ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ (ì˜¤ëŠ˜)**:
1. ì´ ë¬¸ì„œ ì •ë… (1-2ì‹œê°„)
2. ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„ ì‹œì‘
3. ê¸°ì¡´ ë°ì´í„° ë ˆì´ì–´ ê²€ì¦

**Week 1**:
1. ëª¨ë“ˆ ì‹œìŠ¤í…œ êµ¬ì¶•
2. Basic RAG Pipeline êµ¬í˜„
3. ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •

**Week 2-4**:
1. í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„
2. 3ê°€ì§€ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
3. Ablation ì‹¤í—˜ ë° ë¶„ì„

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •**: 2025ë…„ 12ì›” 15ì¼  
**ì‘ì„±ì**: Medical AI Agent Research Team  
**ì €ì¥ ìœ„ì¹˜**: `C:\Users\KHIDI\Downloads\final_medical_ai_agent\`

**ê´€ë ¨ ë¬¸ì„œ**:
- `ì¬ì„¤ê³„_ì „ëµ_í•µì‹¬ìš”ì•½_KO.md`
- `ZERO_TO_ONE_REDESIGN_STRATEGY.md`
- `IMPLEMENTATION_EXAMPLES.md`
- `REDESIGN_QUICK_START.md`

---

**END OF DOCUMENT**

